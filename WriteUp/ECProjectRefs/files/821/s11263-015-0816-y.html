<!DOCTYPE html>
<html class="js" lang="en-gb">
<head>
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type">


<meta content="0.2" name="wpd_version">
<meta content="https://link-springer-com.proxy.library.cornell.edu/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst" name="wpd_baseurl">
<meta content="https://link-springer-com.proxy.library.cornell.edu/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst" name="wpd_url">
<meta content="2017-09-04T19:01Z" name="wpd_date">

        <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=2.5,user-scalable=yes">
    <meta name="citation_publisher" content="Springer US">
    <meta name="citation_title" content="ImageNet Large Scale Visual Recognition Challenge">
    <meta name="citation_doi" content="10.1007/s11263-015-0816-y">
    <meta name="citation_language" content="en">
    <meta name="citation_abstract_html_url" content="https://link-springer-com.proxy.library.cornell.edu/article/10.1007/s11263-015-0816-y">
    <meta name="citation_fulltext_html_url" content="https://link-springer-com.proxy.library.cornell.edu/article/10.1007/s11263-015-0816-y">
    <meta name="citation_pdf_url" content="https://link-springer-com.proxy.library.cornell.edu/content/pdf/10.1007%2Fs11263-015-0816-y.pdf">
    <meta name="citation_springer_api_url" content="http://api.springer.com.proxy.library.cornell.edu/metadata/pam?q=doi:10.1007/s11263-015-0816-y&amp;api_key=">
    <meta name="citation_firstpage" content="211">
    <meta name="citation_lastpage" content="252">
    <meta name="citation_author" content="Olga Russakovsky">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="citation_author_email" content="olga@cs.stanford.edu">
    <meta name="citation_author" content="Jia Deng">
    <meta name="citation_author_institution" content="University of Michigan">
    <meta name="citation_author" content="Hao Su">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="citation_author" content="Jonathan Krause">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="citation_author" content="Sanjeev Satheesh">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="citation_author" content="Sean Ma">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="citation_author" content="Zhiheng Huang">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="citation_author" content="Andrej Karpathy">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="citation_author" content="Aditya Khosla">
    <meta name="citation_author_institution" content="Massachusetts Institute of Technology">
    <meta name="citation_author" content="Michael Bernstein">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="citation_author" content="Alexander C. Berg">
    <meta name="citation_author_institution" content="UNC Chapel Hill">
    <meta name="citation_author" content="Li Fei-Fei">
    <meta name="citation_author_institution" content="Stanford University">
    <meta name="dc.identifier" content="10.1007/s11263-015-0816-y">
    <meta name="format-detection" content="telephone=no">
    <meta name="meta:description" content="The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="ImageNet Large Scale Visual Recognition Challenge">
    <meta name="twitter:description" content="The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run">
    <meta name="twitter:image" content="https://static-content-springer-com.proxy.library.cornell.edu/cover/journal/11263/115/3.jpg">
    <meta name="twitter:image:alt" content="Content cover image">
    <meta name="twitter:site" content="SpringerLink">
    <meta name="citation_journal_title" content="International Journal of Computer Vision">
    <meta name="citation_journal_abbrev" content="Int J Comput Vis">
    <meta name="citation_volume" content="115">
    <meta name="citation_issue" content="3">
    <meta name="citation_issn" content="0920-5691">
    <meta name="citation_issn" content="1573-1405">
    <meta name="citation_online_date" content="2015/04/11">
    <meta name="citation_cover_date" content="2015/12/01">
    <meta name="citation_article_type" content="Article">
    <meta property="og:title" content="ImageNet Large Scale Visual Recognition Challenge">
    <meta property="og:description" content="The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run">
    <meta property="og:type" content="Article">
    <meta property="og:url" content="https://link-springer-com.proxy.library.cornell.edu/article/10.1007/s11263-015-0816-y">
    <meta property="og:image" content="https://static-content-springer-com.proxy.library.cornell.edu/cover/journal/11263/115/3.jpg">
    <meta property="og:site_name" content="SpringerLink">

        <title>ImageNet Large Scale Visual Recognition Challenge | SpringerLink</title>
        <link rel="canonical" href="https://link-springer-com.proxy.library.cornell.edu/article/10.1007/s11263-015-0816-y">
        <link rel="shortcut icon" href="favicon.ico">
<link rel="icon" sizes="16x16 32x32 48x48" href="favicon.ico">
<link rel="icon" sizes="16x16" type="image/png" href="favicon-16x16.png">
<link rel="icon" sizes="32x32" type="image/png" href="favicon-32x32.png">
<link rel="icon" sizes="48x48" type="image/png" href="favicon-48x48.png">
<link rel="apple-touch-icon" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/images/favicon/app-icon-iphone@3x.png">
<link rel="apple-touch-icon" sizes="72x72" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/images/favicon/ic_launcher_hdpi.png">
<link rel="apple-touch-icon" sizes="76x76" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/images/favicon/app-icon-ipad.png">
<link rel="apple-touch-icon" sizes="114x114" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/images/favicon/app-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/images/favicon/app-icon-iphone@2x.png">
<link rel="apple-touch-icon" sizes="144x144" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/images/favicon/ic_launcher_xxhdpi.png">
<link rel="apple-touch-icon" sizes="152x152" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/images/favicon/app-icon-ipad@2x.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/images/favicon/app-icon-iphone@3x.png">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/springerlink-static/1756782716/images/favicon/ic_launcher_xxhdpi.png">
        <link rel="dns-prefetch" href="https://fonts.gstatic.com/">
<link rel="dns-prefetch" href="https://fonts.googleapis.com/">
<link rel="dns-prefetch" href="https://google-analytics.com/">
<link rel="dns-prefetch" href="https://www-google-analytics-com.proxy.library.cornell.edu/">
<link rel="dns-prefetch" href="https://www-googletagservices-com.proxy.library.cornell.edu/">
<link rel="dns-prefetch" href="https://www.googletagmanager.com/">
<link rel="dns-prefetch" href="https://static-content-springer-com.proxy.library.cornell.edu/">
        




            <script type="text/javascript" async="" src="analytics.js"></script><script async="" src="controltag.js.73f4c3c5d949eb6203a73b137d7242c4"></script><script async="" type="text/javascript" src="gpt.js"></script><script type="text/javascript" async="" src="KDqyaFZ_.js"></script><script async="" src="mathJax.js"></script><script async="" src="gtm.js"></script><script id="webtrekk-properties">

    var webtrekkProperties = {
          trackDomain : "springergmbh01.webtrekk.net",
          trackId : "935649882378213",
            bpJson: {1:"8200595982;8200828607",2:"CORNELL UNIVERSITY Ithaca, NY 14850;Center for Research LIbraries c/o NERL"}
      };
    </script>
    <script type="text/javascript" async="" src="webtrekk_v4.min.js"></script>

            <script>

        var dataLayer = [{
                'GA Key':'UA-26408784-1',
                'Event Category':'Article',
                'Open Access':'N',
                'Labs':'Y',
                'DOI':'10.1007\/s11263-015-0816-y',
                'VG Wort Identifier':'pw-vgzm.415900-10.1007-s11263-015-0816-y',
                'HasAccess':'Y',
                'Full HTML':'Y',
                'Has Body':'Y',
                'Static Hash':'1756782716',
                'Has Preview':'N',
                'user':{'license': {'businessPartnerID': ['8200595982', '8200828607'], 'businessPartnerIDString': '8200595982|8200828607'}},
                'content':{'type': 'article', 'category': {'pmc': {'primarySubject': 'Computer Science', 'primarySubjectCode': 'I', 'secondarySubjects': {'1': 'Computer Imaging, Vision, Pattern Recognition and Graphics', '2': 'Artificial Intelligence (incl. Robotics)', '3': 'Image Processing and Computer Vision', '4': 'Pattern Recognition'}, 'secondarySubjectCodes': {'1': 'I22005', '2': 'I21017', '3': 'I22021', '4': 'I2203X'}}}},
                'Access Type':'subscription',
                'Page':'article',
                'Bpids':'8200595982, 8200828607',
                'Bpnames':'CORNELL UNIVERSITY Ithaca, NY 14850, Center for Research LIbraries c\/o NERL',
                'SubjectCodes':'SCI, SCI22005, SCI21017, SCI22021, SCI2203X',
                'Keywords':'Dataset, Large-scale, Benchmark, Object recognition, Object detection',
                'Webtrekk parent prefix':'journal',
                'Webtrekk leaf prefix':'article',
                'Webtrekk toc prefix':'journal',
                'Country':'US',
                'Journal Id':'11263',
                'Journal Title':'International Journal of Computer Vision',
        }];
    </script>

    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
            'https://www.googletagmanager.com/gtm.js?cache-busting=' + new Date().getTime() + '&id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WCF9Z9');</script>

    <script src="pubads_impl_153.js" async=""></script><link rel="prefetch" href="https://securepubads.g.doubleclick.net/static/3p_cookie.html"></head>
<body>
        <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
                      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <nav class="skip-to">
    <a class="skip-to__link skip-to__link--article" href="#main-content">Skip to main content</a>
        <a class="skip-to__link skip-to__link--contents" href="#article-contents">Skip to sections</a>
</nav>
        <div class="page-wrapper">
            <noscript>

    <div class="nojs-banner u-interface">
        <p>This service is more advanced with JavaScript available, learn more at <a
                href="http://activatejavascript.org" target="_blank" rel="noopener noreferrer">http://activatejavascript.org</a>
        </p>
    </div>
</noscript>
                    <div id="leaderboard" class="leaderboard u-hide" data-component="SpringerLink.GoogleAds" data-namespace="leaderboard"></div>

                <header id="header" class="header u-interface" role="banner">
        <div class="header__content">
            <div class="header__menu-container">
                    <a id="logo" class="site-logo" href="https://link-springer-com.proxy.library.cornell.edu/" title="Go to homepage">
            <span class="u-screenreader-only">SpringerLink</span>
            <svg class="site-logo__springer" width="148" height="30" role="img" aria-label="SpringerLink Logo">
                <image width="148" height="30" alt="SpringerLink Logo" src="/springerlink-static/1756782716/images/png/springerlink.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/1756782716/images/svg/springerlink.svg"></image>
            </svg>
    </a>


                    <nav id="search-container" class="u-inline-block">
                        <div class="search">
                            <div class="search__content">
                                <form class="u-form-single-input" action="https://link-springer-com.proxy.library.cornell.edu/search" method="get" role="search">
    <input aria-label="Search" name="query" autocomplete="off" value="" placeholder="Search" type="text">
    <input class="u-hide-text" value="Submit" title="Submit" type="submit">
    <svg class="u-vertical-align-absolute" width="13" height="13" viewBox="222 151 13 13" version="1.1" xmlns="http://www.w3.org/2000/svg">
        <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"></path>
    </svg>
</form>
                            </div>
                        </div>
                    </nav>

                    <nav class="nav-container u-interface">
    <div class="global-nav__wrapper">
        <div class="search-button">
            <a class="search-button__label" href="#search-container">
                <span class="search-button__title">Search</span><svg width="12" height="12" viewBox="222 151 12 12" version="1.1" xmlns="http://www.w3.org/2000/svg">
                    <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"></path>
                </svg>
            </a>
        </div>

        <ul class="global-nav" data-component="SpringerLink.Menu" data-title="Navigation menu" data-text="Menu">
            <li>
                <a href="https://link-springer-com.proxy.library.cornell.edu/">
                    <span class="u-overflow-ellipsis">Home</span>
                </a>
            </li>
            <li>
                <a href="https://link-springer-com.proxy.library.cornell.edu/contactus">
                    <span class="u-overflow-ellipsis">Contact Us</span>
                </a>
            </li>

                <li class="global-nav__logged-out">
                    <a class="test-login-link" href="https://link-springer-com.proxy.library.cornell.edu/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y%3Fsa_campaign%3Demail%2Fevent%2FarticleAuthor%2FonlineFirst">
                        <span class="u-overflow-ellipsis">Log in</span>
                    </a>
                </li>

        </ul>
    </div> 
</nav> 
            </div>

        </div>
    </header>

            
            

            <main id="main-content">
                <article class="main-wrapper">
                    <div class="main-container uptodate-recommendations-off">
                        <aside class="main-sidebar-left">
                            <div class="main-sidebar-left__content">
                                <div class="test-cover cover-image" itemscope="">
        <a class="test-cover-link" href="https://link-springer-com.proxy.library.cornell.edu/journal/11263" title="International Journal of Computer Vision">
            <img class="test-cover-image" src="3.jpg" alt="International Journal of Computer Vision" itemprop="image">
        </a>

</div>
                            </div>
                        </aside>

                        <div class="main-body" data-role="NavigationContainer">
                            <div class="main-body__content">
                                        <div class="cta-button-container">
                    <a href="https://link-springer-com.proxy.library.cornell.edu/content/pdf/10.1007%2Fs11263-015-0816-y.pdf" target="_top" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener noreferrer">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"></path></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

        </div>



                                <div xmlns="http://www.w3.org/1999/xhtml" class="FulltextWrapper"><div class="ArticleHeader main-context"><div id="enumeration" class="enumeration"><p><a href="https://link-springer-com.proxy.library.cornell.edu/journal/11263" title="International Journal of Computer Vision"><span class="JournalTitle">International Journal of Computer Vision</span></a></p><p class="icon--meta-keyline-before"><span class="ArticleCitation_Year"><time datetime="2015-12">December 2015</time>, </span><span class="ArticleCitation_Volume">Volume 115, </span><a class="ArticleCitation_Issue" href="https://link-springer-com.proxy.library.cornell.edu/journal/11263/115/3/page/1">Issue&nbsp;3</a>,
                       <span class="ArticleCitation_Pages"> pp 211–252</span><span class="u-inline-block u-ml-4"> | <a class="gtm-cite-link" href="#citeas">Cite as</a></span></p></div><div class="MainTitleSection"><h1 class="ArticleTitle" lang="en">ImageNet Large Scale Visual Recognition Challenge</h1></div><div class="authors u-clearfix" data-component="SpringerLink.Authors"><ul class="u-interface u-inline-list authors__title" data-role="AuthorsNavigation"><li><span>Authors</span></li><li><a href="#authorsandaffiliations" class="gtm-tab-authorsandaffiliations">Authors and affiliations</a></li></ul><div class="authors__list" data-role="AuthorsList"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Olga&nbsp;Russakovsky</span><span class="author-information"><span class="authors__contact"><a href="mailto:olga@cs.stanford.edu" title="olga@cs.stanford.edu" itemprop="email" class="gtm-email-author">Email author</a></span></span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Jia&nbsp;Deng</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Hao&nbsp;Su</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Jonathan&nbsp;Krause</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Sanjeev&nbsp;Satheesh</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Sean&nbsp;Ma</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Zhiheng&nbsp;Huang</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Andrej&nbsp;Karpathy</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Aditya&nbsp;Khosla</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Michael&nbsp;Bernstein</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Alexander&nbsp;C.&nbsp;Berg</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Li&nbsp;Fei-Fei</span></li></ul></div></div><div class="main-context__container" data-component="SpringerLink.ArticleMetrics"><div class="main-context__column"><span><span class="test-render-category">Article</span></span><div class="article-dates" data-component="SpringerLink.ArticleDates"><div class="article-dates__entry"><span class="article-dates__label">First Online: </span><span class="article-dates__first-online"><time datetime="2015-04-11">11 April 2015</time></span></div><div class="article-dates__history"><div class="article-dates__entry"><span class="article-dates__label">Received: </span><span><time datetime="2014-08-31">31 August 2014</time></span></div><div class="article-dates__entry"><span class="article-dates__label">Accepted: </span><span><time datetime="2015-03-12">12 March 2015</time></span></div></div></div></div>    <div class="main-context__column">
        <ul id="book-metrics" class="article-metrics">
                <li class="article-metrics__item">
                        <a class="article-metrics__link gtm-citations-count" href="http://citations.springer.com.proxy.library.cornell.edu/item?doi=10.1007/s11263-015-0816-y" target="_top" rel="noopener noreferrer" title="Visit Springer Citations for full citation details" id="citations-link">
                           <span id="citations-count-number" class="test-metric-count c-button-circle gtm-citations-count">973</span>
                           <span class="test-metric-name article-metrics__label gtm-citations-count">Citations</span>
                        </a>
                </li>
                <li class="article-metrics__item">
                        <a class="article-metrics__link gtm-socialmediamentions-count" href="http://www.altmetric.com/details.php?citation_id=3898208&amp;domain=link.springer.com" target="_top" rel="noopener noreferrer" title="Visit Altmetric for full social mention details" id="socialmediamentions-link">
                           <span id="socialmediamentions-count-number" class="test-metric-count c-button-circle gtm-socialmediamentions-count">6</span>
                           <span class="test-metric-name article-metrics__label gtm-socialmediamentions-count">Shares</span>
                        </a>
                </li>
        </ul>
    </div>
</div></div><section class="Abstract" id="Abs1" tabindex="-1" lang="en"><h2 class="Heading">Abstract</h2><p id="Par1" class="Para">The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5&nbsp;years of the challenge, and propose future directions and improvements.</p></section><div class="KeywordGroup" lang="en"><h3 class="Heading">Keywords</h3><span class="Keyword">Dataset&nbsp;</span><span class="Keyword">Large-scale&nbsp;</span><span class="Keyword">Benchmark&nbsp;</span><span class="Keyword">Object recognition&nbsp;</span><span class="Keyword">Object detection&nbsp;</span></div><div class="HeaderArticleNotes"><aside class="ArticleNote ArticleNoteCommunicatedBy"><p class="SimplePara">Communicated by M. Hebert.</p></aside><aside class="ArticleNote ArticleNoteMisc"><p class="SimplePara">Olga Russakovsky and Jia Deng authors contributed equally.</p></aside></div><div class="note test-pdf-link" id="cobranding-and-download-availability-text"><div>Access to this content is enabled by <strong>CORNELL UNIVERSITY Ithaca, NY 14850</strong></div><div>    <a class="gtm-pdf-link" href="https://link-springer-com.proxy.library.cornell.edu/content/pdf/10.1007%2Fs11263-015-0816-y.pdf" target="_top" rel="noreferrer noopener">Download</a>
 fulltext PDF</div></div><div class="article-actions--inline" id="article-actions--inline" data-component="article-actions--inline"></div><div id="body"><section id="Sec1" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">1 </span>Introduction</h2><div class="content"><p id="Par2" class="Para"><em class="EmphasisTypeItalic ">Overview</em> The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has been running annually for 5&nbsp;years (since 2010) and has become the standard benchmark for large-scale object recognition.<sup><a href="#Fn1" id="Fn1_source">1</a></sup> ILSVRC follows in the footsteps of the PASCAL VOC challenge (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR18">2012</a></span>), established in 2005, which set the precedent for standardized evaluation of recognition algorithms in the form of yearly competitions. As in PASCAL VOC, ILSVRC consists of two components: (1) a publically available <em class="EmphasisTypeItalic ">dataset</em>, and (2) an annual <em class="EmphasisTypeItalic ">competition</em> and corresponding workshop. The dataset allows for the development and comparison of categorical object recognition algorithms, and the competition and workshop provide a way to track the progress and discuss the lessons learned from the most successful and innovative entries each year.</p><p id="Par4" class="Para">The publically released dataset contains a set of manually annotated <em class="EmphasisTypeItalic ">training</em> images. A set of <em class="EmphasisTypeItalic ">test</em> images is also released, with the manual annotations withheld.<sup><a href="#Fn2" id="Fn2_source">2</a></sup> Participants train their algorithms using the training images and then automatically annotate the test images. These predicted annotations are submitted to the <em class="EmphasisTypeItalic ">evaluation server</em>. Results of the evaluation are revealed at the end of the competition period and authors are invited to share insights at the workshop held at the International Conference on Computer Vision (ICCV) or European Conference on Computer Vision (ECCV) in alternate years.</p><p id="Par6" class="Para">ILSVRC annotations fall into one of two categories: (1) <em class="EmphasisTypeItalic ">image-level annotation</em> of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) <em class="EmphasisTypeItalic ">object-level annotation</em> of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”.</p><p id="Par7" class="Para"><em class="EmphasisTypeItalic ">Large-Scale Challenges and Innovations</em> In creating the dataset, several challenges had to be addressed. Scaling up from 19,737 images in PASCAL VOC 2010 to 1,461,406 in ILSVRC 2010 and from 20 object classes to 1000 object classes brings with it several challenges. It is no longer feasible for a small group of annotators to annotate the data as is done for other datasets (Fei-Fei et&nbsp;al. <span class="CitationRef"><a href="#CR22">2004</a></span>; Criminisi <span class="CitationRef"><a href="#CR12">2004</a></span>; Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR18">2012</a></span>; Xiao et&nbsp;al. <span class="CitationRef"><a href="#CR96">2010</a></span>). Instead we turn to designing novel crowdsourcing approaches for collecting large-scale annotations (Su et&nbsp;al. <span class="CitationRef"><a href="#CR76">2012</a></span>; Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>, <span class="CitationRef"><a href="#CR15">2014</a></span>).</p><p id="Par8" class="Para">Some of the 1000 object classes may not be as easy to annotate as the 20 categories of PASCAL VOC: e.g., bananas which appear in bunches may not be as easy to delineate as the basic-level categories of aeroplanes or cars. Having more than a million images makes it infeasible to annotate the locations of all objects (much less with object segmentations, human body parts, and other detailed annotations that subsets of PASCAL VOC contain). New evaluation criteria have to be defined to take into account the facts that obtaining perfect manual annotations in this setting may be infeasible.</p><p id="Par9" class="Para">Once the challenge dataset was collected, its scale allowed for unprecedented opportunities both in evaluation of object recognition algorithms and in developing new techniques. Novel algorithmic innovations emerge with the availability of large-scale training data. The broad spectrum of object categories motivated the need for algorithms that are even able to distinguish classes which are visually very similar. We highlight the most successful of these algorithms in this paper, and compare their performance with human-level accuracy.</p><p id="Par10" class="Para">Finally, the large variety of object classes in ILSVRC allows us to perform an analysis of statistical properties of objects and their impact on recognition algorithms. This type of analysis allows for a deeper understanding of object recognition, and for designing the next generation of general object recognition algorithms.</p><div id="Par11" class="Para"><em class="EmphasisTypeItalic ">Goals</em> This paper has three key goals:<div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(1)</span><div class="ItemContent"><p id="Par12" class="Para">To discuss the challenges of creating this large-scale object recognition benchmark dataset,</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(2)</span><div class="ItemContent"><p id="Par13" class="Para">To highlight the developments in object classification and detection that have resulted from this effort, and</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(3)</span><div class="ItemContent"><p id="Par14" class="Para">To take a closer look at the current state of the field of categorical object recognition.</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div>The paper may be of interest to researchers working on creating large-scale datasets, as well as to anybody interested in better understanding the history and the current state of large-scale object recognition.</div><p id="Par15" class="Para">The collected dataset and additional information about ILSVRC can be found at:</p><p id="Par16" class="Para"><span class="ExternalRef"><a target="_top" rel="noopener noreferrer" href="http://image-net.org/challenges/LSVRC/"><span class="RefSource">http://image-net.org/challenges/LSVRC/</span></a></span>.</p><section id="Sec2" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">1.1 </span>Related Work</h3><p id="Par17" class="Para">We briefly discuss some prior work in constructing benchmark image datasets.</p><p id="Par18" class="Para"><em class="EmphasisTypeItalic ">Image Classification Datasets</em> Caltech 101 (Fei-Fei et&nbsp;al. <span class="CitationRef"><a href="#CR22">2004</a></span>) was among the first standardized datasets for multi-category image classification, with 101 object classes and commonly 15–30 training images per class. Caltech 256 (Griffin et&nbsp;al. <span class="CitationRef"><a href="#CR30">2007</a></span>) increased the number of object classes to 256 and added images with greater scale and background variability. The TinyImages dataset (Torralba et&nbsp;al. <span class="CitationRef"><a href="#CR81">2008</a></span>) contains 80 million <span class="InlineEquation" id="IEq1">\(32\times 32\)</span> low resolution images collected from the internet using synsets in WordNet (Miller <span class="CitationRef"><a href="#CR55">1995</a></span>) as queries. However, since this data has not been manually verified, there are many errors, making it less suitable for algorithm evaluation. Datasets such as 15 Scenes (Oliva and Torralba <span class="CitationRef"><a href="#CR56">2001</a></span>; Fei-Fei and Perona <span class="CitationRef"><a href="#CR21">2005</a></span>; Lazebnik et&nbsp;al. <span class="CitationRef"><a href="#CR45">2006</a></span>) or recent Places (Zhou et&nbsp;al. <span class="CitationRef"><a href="#CR101">2014</a></span>) provide a single scene category label (as opposed to an object category).</p><p id="Par19" class="Para">The ImageNet dataset (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>) is the backbone of ILSVRC. ImageNet is an image dataset organized according to the WordNet hierarchy (Miller <span class="CitationRef"><a href="#CR55">1995</a></span>). Each concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images. As a result, ImageNet contains 14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014). ImageNet is larger in scale and diversity than the other image classification datasets. ILSVRC uses a subset of ImageNet images for training the algorithms and some of ImageNet’s image collection protocols for annotating additional images for testing the algorithms.</p><p id="Par20" class="Para"><em class="EmphasisTypeItalic ">Image Parsing Datasets</em> Many datasets aim to provide richer image annotations beyond image-category labels. LabelMe (Russell et&nbsp;al. <span class="CitationRef"><a href="#CR66">2007</a></span>) contains general photographs with multiple objects per image. It has bounding polygon annotations around objects, but the object names are not standardized: annotators are free to choose which objects to label and what to name each object. The SUN2012 (Xiao et&nbsp;al. <span class="CitationRef"><a href="#CR96">2010</a></span>) dataset contains 16,873 manually cleaned up and fully annotated images more suitable for standard object detection training and evaluation. SIFT Flow (Liu et&nbsp;al. <span class="CitationRef"><a href="#CR49">2011</a></span>) contains 2,688 images labeled using the LabelMe system. The LotusHill dataset (Yao et&nbsp;al. <span class="CitationRef"><a href="#CR98">2007</a></span>) contains very detailed annotations of objects in 636,748 images and video frames, but it is not available for free. Several datasets provide pixel-level segmentations: for example, MSRC dataset (Criminisi <span class="CitationRef"><a href="#CR12">2004</a></span>) with 591 images and 23 object classes, Stanford Background Dataset (Gould et&nbsp;al. <span class="CitationRef"><a href="#CR28">2009</a></span>) with 715 images and 8 classes, and the Berkeley Segmentation dataset (Arbelaez et&nbsp;al. <span class="CitationRef"><a href="#CR5">2011</a></span>) with 500 images annotated with object boundaries. OpenSurfaces segments surfaces from consumer photographs and annotates them with surface properties, including material, texture, and contextual information (Bell et&nbsp;al. <span class="CitationRef"><a href="#CR7">2013</a></span>).</p><p id="Par21" class="Para">The closest to ILSVRC is the PASCAL VOC dataset (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR19">2010</a></span>, <span class="CitationRef"><a href="#CR20">2014</a></span>), which provides a standardized test bed for object detection, image classification, object segmentation, person layout, and action classification. Much of the design choices in ILSVRC have been inspired by PASCAL VOC and the similarities and differences between the datasets are discussed at length throughout the paper. ILSVRC scales up PASCAL VOC’s goal of standardized training and evaluation of recognition algorithms by more than an order of magnitude in number of object classes and images: PASCAL VOC 2012 has 20 object classes and 21,738 images compared to ILSVRC2012 with 1000 object classes and 1,431,167 annotated images.</p><p id="Par22" class="Para">The recently released COCO dataset (Lin et&nbsp;al. <span class="CitationRef"><a href="#CR48">2014b</a></span>) contains more than 328,000 images with 2.5 million object instances manually segmented. It has fewer object categories than ILSVRC (91 in COCO versus 200 in ILSVRC object detection) but more instances per category (27K on average compared to about 1K in ILSVRC object detection). Further, it contains object segmentation annotations which are not currently available in ILSVRC. COCO is likely to become another important large-scale benchmark.</p><p id="Par23" class="Para"><em class="EmphasisTypeItalic ">Large-Scale Annotation</em> ILSVRC makes extensive use of Amazon Mechanical Turk to obtain accurate annotations (Sorokin and Forsyth <span class="CitationRef"><a href="#CR75">2008</a></span>). Works such as (Welinder et&nbsp;al. <span class="CitationRef"><a href="#CR95">2010</a></span>; Sheng et&nbsp;al. <span class="CitationRef"><a href="#CR72">2008</a></span>; Vittayakorn and Hays <span class="CitationRef"><a href="#CR88">2011</a></span>) describe quality control mechanisms for this marketplace. Vondrick et&nbsp;al. (<span class="CitationRef"><a href="#CR90">2012</a></span>) provides a detailed overview of crowdsourcing video annotation. A related line of work is to obtain annotations through well-designed games, e.g. (von Ahn and Dabbish <span class="CitationRef"><a href="#CR89">2005</a></span>). Our novel approaches to crowdsourcing accurate image annotations are in Sects.&nbsp;<span class="InternalRef"><a href="#Sec12">3.1.3</a></span>,&nbsp;<span class="InternalRef"><a href="#Sec15">3.2.1</a></span> and&nbsp;<span class="InternalRef"><a href="#Sec20">3.3.3</a></span>.</p><p id="Par24" class="Para"><em class="EmphasisTypeItalic ">Standardized Challenges</em> There are several datasets with standardized online evaluation similar to ILSVRC: the aforementioned PASCAL VOC (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR18">2012</a></span>), Labeled Faces in the Wild (Huang et&nbsp;al. <span class="CitationRef"><a href="#CR37">2007</a></span>) for unconstrained face recognition, Reconstruction meets Recognition (Urtasun et&nbsp;al. <span class="CitationRef"><a href="#CR83">2014</a></span>) for 3D reconstruction and KITTI (Geiger et&nbsp;al. <span class="CitationRef"><a href="#CR25">2013</a></span>) for computer vision in autonomous driving. These datasets along with ILSVRC help benchmark progress in different areas of computer vision. Works such as (Torralba and Efros <span class="CitationRef"><a href="#CR80">2011</a></span>) emphasize the importance of examining the bias inherent in any standardized dataset.</p></section><section id="Sec3" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">1.2 </span>Paper Layout</h3><p id="Par25" class="Para">We begin with a brief overview of ILSVRC challenge tasks in Sect.&nbsp;<span class="InternalRef"><a href="#Sec4">2</a></span>. Dataset collection and annotation are described at length in Sect.&nbsp;<span class="InternalRef"><a href="#Sec8">3</a></span>. Section&nbsp;<span class="InternalRef"><a href="#Sec22">4</a></span> discusses the evaluation criteria of algorithms in the large-scale recognition setting. Section&nbsp;<span class="InternalRef"><a href="#Sec26">5</a></span> provides an overview of the methods developed by ILSVRC participants.</p><p id="Par26" class="Para">Section&nbsp;<span class="InternalRef"><a href="#Sec29">6</a></span> contains an in-depth analysis of ILSVRC results: Sect.&nbsp;<span class="InternalRef"><a href="#Sec30">6.1</a></span> documents the progress of large-scale recognition over the years, Sect.&nbsp;<span class="InternalRef"><a href="#Sec33">6.2</a></span> concludes that ILSVRC results are statistically significant, Sect.&nbsp;<span class="InternalRef"><a href="#Sec34">6.3</a></span> thoroughly analyzes the current state of the field of object recognition, and Sect.&nbsp;<span class="InternalRef"><a href="#Sec39">6.4</a></span> compares state-of-the-art computer vision accuracy with human accuracy. We conclude and discuss lessons learned from ILSVRC in Sect.&nbsp;<span class="InternalRef"><a href="#Sec43">7</a></span>.</p></section></div></section><section id="Sec4" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">2 </span>Challenge Tasks</h2><div class="content"><p id="Par27" class="Para">The goal of ILSVRC is to estimate the content of photographs for the purpose of retrieval and automatic annotation. Test images are presented with no initial annotation, and algorithms have to produce labelings specifying what objects are present in the images. New test images are collected and labeled especially for this competition and are not part of the previously published ImageNet dataset (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>).</p><div id="Par28" class="Para">ILSVRC over the years has consisted of one or more of the following tasks (years in parentheses):<sup><a href="#Fn3" id="Fn3_source">3</a></sup><div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(1)</span><div class="ItemContent"><p id="Par30" class="Para"><em class="EmphasisTypeItalic ">Image classification</em> (2010–2014): Algorithms produce a list of object categories present in the image.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(2)</span><div class="ItemContent"><p id="Par31" class="Para"><em class="EmphasisTypeItalic ">Single-object localization</em> (2011–2014): Algorithms produce a list of object categories present in the image, along with an axis-aligned bounding box indicating the position and scale of <em class="EmphasisTypeItalic ">one</em> instance of each object category.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(3)</span><div class="ItemContent"><p id="Par32" class="Para"><em class="EmphasisTypeItalic ">Object detection</em> (2013–2014): Algorithms produce a list of object categories present in the image along with an axis-aligned bounding box indicating the position and scale of <em class="EmphasisTypeItalic ">every</em> instance of each object category.</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div>This section provides an overview and history of each of the three tasks. Table&nbsp;<span class="InternalRef"><a href="#Tab1">1</a></span> shows summary statistics.<div id="Tab1" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 1</span><p class="SimplePara">Overview of the provided annotations for each of the tasks in ILSVRC</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"><col class="tcol5 align-left"></colgroup><thead><tr><th colspan="2"><p class="SimplePara">Task</p></th><th><p class="SimplePara">Image classification</p></th><th><p class="SimplePara">Single-object localization</p></th><th><p class="SimplePara">Object detection</p></th></tr></thead><tbody><tr><td rowspan="2"><p class="SimplePara">Manual labeling on training set</p></td><td><p class="SimplePara">Number of object classes annotated per image</p></td><td><p class="SimplePara">1</p></td><td><p class="SimplePara">1</p></td><td><p class="SimplePara">1 or More</p></td></tr><tr><td><p class="SimplePara">Locations of annotated classes</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">All instances on some images</p></td><td><p class="SimplePara">All instances on all images</p></td></tr><tr><td rowspan="2"><p class="SimplePara">Manual labeling on validation and test sets</p></td><td><p class="SimplePara">Number of object classes annotated per image</p></td><td><p class="SimplePara">1</p></td><td><p class="SimplePara">1</p></td><td><p class="SimplePara">All target classes</p></td></tr><tr><td><p class="SimplePara">Locations of annotated classes</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">All instances on all images</p></td><td><p class="SimplePara">All instances on all images</p></td></tr></tbody></table></div></div></div><section id="Sec5" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.1 </span>Image Classification Task</h3><p id="Par33" class="Para">Data for the image classification task consists of photographs collected from Flickr<sup><a href="#Fn4" id="Fn4_source">4</a></sup> and other search engines, manually labeled with the presence of one of 1000 object categories. Each image contains one ground truth label.</p><p id="Par35" class="Para">For each image, algorithms produce a list of object categories present in the image. The quality of a labeling is evaluated based on the label that best matches the ground truth label for the image (see Sect.&nbsp;<span class="InternalRef"><a href="#Sec23">4.1</a></span>).</p><p id="Par36" class="Para">Constructing ImageNet was an effort to scale up an image classification dataset to cover most nouns in English using tens of millions of manually verified photographs (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>). The image classification task of ILSVRC came as a direct extension of this effort. A subset of categories and images was chosen and fixed to provide a standardized benchmark while the rest of ImageNet continued to grow.</p></section><section id="Sec6" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.2 </span>Single-Object Localization Task</h3><p id="Par37" class="Para">The single-object localization task, introduced in 2011, built off of the image classification task to evaluate the ability of algorithms to learn the appearance of the target object itself rather than its image context.</p><p id="Par38" class="Para">Data for the single-object localization task consists of the same photographs collected for the image classification task, hand labeled with the presence of one of 1000 object categories. Each image contains one ground truth label. Additionally, every instance of this category is annotated with an axis-aligned bounding box.</p><p id="Par39" class="Para">For each image, algorithms produce a list of object categories present in the image, along with a bounding box indicating the position and scale of one instance of each object category. The quality of a labeling is evaluated based on the object category label that best matches the ground truth label, with the additional requirement that the location of the predicted instance is also accurate (see Sect.&nbsp;<span class="InternalRef"><a href="#Sec24">4.2</a></span>).</p></section><section id="Sec7" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.3 </span>Object Detection Task</h3><p id="Par40" class="Para">The object detection task went a step beyond single-object localization and tackled the problem of localizing multiple object categories in the image. This task has been a part of the PASCAL VOC for many years on the scale of 20 object categories and tens of thousands of images, but scaling it up by an order of magnitude in object categories and in images proved to be very challenging from a dataset collection and annotation point of view (see Sect.&nbsp;<span class="InternalRef"><a href="#Sec17">3.3</a></span>).</p><p id="Par41" class="Para">Data for the detection tasks consists of new photographs collected from Flickr using scene-level queries. The images are annotated with axis-aligned bounding boxes indicating the position and scale of every instance of each target object category. The training set is additionally supplemented with (a) data from the single-object localization task, which contains annotations for all instances of just one object category, and (b) negative images known not to contain any instance of some object categories.</p><p id="Par42" class="Para">For each image, algorithms produce bounding boxes indicating the position and scale of all instances of all target object categories. The quality of labeling is evaluated by <em class="EmphasisTypeItalic ">recall</em>, or number of target object instances detected, and <em class="EmphasisTypeItalic ">precision</em>, or the number of spurious detections produced by the algorithm (see Sect.&nbsp;<span class="InternalRef"><a href="#Sec25">4.3</a></span>).</p></section></div></section><section id="Sec8" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">3 </span>Dataset Construction at Large Scale</h2><div class="content"><p id="Par43" class="Para">Our process of constructing large-scale object recognition image datasets consists of three key steps.</p><p id="Par44" class="Para">The first step is defining the set of target object categories. To do this, we select from among the existing ImageNet (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>) categories. By using WordNet as a backbone (Miller <span class="CitationRef"><a href="#CR55">1995</a></span>), ImageNet already takes care of disambiguating word meanings and of combining together synonyms into the same object category. Since the selection of object categories needs to be done only once per challenge task, we use a combination of automatic heuristics and manual post-processing to create the list of target categories appropriate for each task. For example, for image classification we may include broader scene categories such as a type of beach, but for single-object localization and object detection we want to focus only on object categories which can be unambiguously localized in images (Sects.&nbsp;<span class="InternalRef"><a href="#Sec10">3.1.1</a></span>,&nbsp;<span class="InternalRef"><a href="#Sec18">3.3.1</a></span>).</p><p id="Par45" class="Para">The second step is collecting a diverse set of candidate images to represent the selected categories. We use both automatic and manual strategies on multiple search engines to do the image collection. The process is modified for the different ILSVRC tasks. For example, for object detection we focus our efforts on collecting scene-like images using generic queries such as “African safari” to find pictures likely to contain multiple animals in one scene (Sect.&nbsp;<span class="InternalRef"><a href="#Sec19">3.3.2</a></span>).</p><p id="Par46" class="Para">The third (and most challenging) step is annotating the millions of collected images to obtain a clean dataset. We carefully design crowdsourcing strategies targeted to each individual ILSVRC task. For example, the bounding box annotation system used for localization and detection tasks consists of three distinct parts in order to include automatic crowdsourced quality control (Sect.&nbsp;<span class="InternalRef"><a href="#Sec15">3.2.1</a></span>). Annotating images fully with all target object categories (on a reasonable budget) for object detection requires an additional hierarchical image labeling system (Sect.&nbsp;<span class="InternalRef"><a href="#Sec20">3.3.3</a></span>).</p><p id="Par47" class="Para">We describe the data collection and annotation procedure for each of the ILSVRC tasks in order: image classification (Sect.&nbsp;<span class="InternalRef"><a href="#Sec9">3.1</a></span>), single-object localization (Sect.&nbsp;<span class="InternalRef"><a href="#Sec14">3.2</a></span>), and object detection (Sect.&nbsp;<span class="InternalRef"><a href="#Sec17">3.3</a></span>), focusing on the three key steps for each dataset.</p><section id="Sec9" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.1 </span>Image Classification Dataset Construction</h3><p id="Par48" class="Para">The image classification task tests the ability of an algorithm to name the objects present in the image, without necessarily localizing them.</p><p id="Par49" class="Para">We describe the choices we made in constructing the ILSVRC image classification dataset: selecting the target object categories from ImageNet (Sect.&nbsp;<span class="InternalRef"><a href="#Sec10">3.1.1</a></span>), collecting a diverse set of candidate images by using multiple search engines and an expanded set of queries in multiple languages (Sect.&nbsp;<span class="InternalRef"><a href="#Sec11">3.1.2</a></span>), and finally filtering the millions of collected images using the carefully designed crowdsourcing strategy of ImageNet (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>) (Sect.&nbsp;<span class="InternalRef"><a href="#Sec12">3.1.3</a></span>).</p><section id="Sec10" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.1.1 </span>Defining Object Categories for the Image Classification Dataset</h4><div id="Par50" class="Para">The 1000 categories used for the image classification task were selected from the ImageNet (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>) categories. The 1000 synsets are selected such that there is no overlap between synsets: for any synsets <span class="InlineEquation" id="IEq2">\(i\)</span> and <span class="InlineEquation" id="IEq3">\(j\)</span>, <span class="InlineEquation" id="IEq4">\(i\)</span> is not an ancestor of <span class="InlineEquation" id="IEq5">\(j\)</span> in the ImageNet hierarchy. These synsets are part of the larger hierarchy and may have children in ImageNet; however, for ILSVRC we do not consider their child subcategories. The synset hierarchy of ILSVRC can be thought of as a “trimmed” version of the complete ImageNet hierarchy. Figure&nbsp;<span class="InternalRef"><a href="#Fig1">1</a></span> visualizes the diversity of the ILSVRC2012 object categories.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig1_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig1_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 1</span><p class="SimplePara">The diversity of data in the ILSVRC image classification and single-object localization tasks. For each of the eight dimensions, we show example object categories along the range of that property. Object scale, number of instances and image clutter for each object category are computed using the metrics defined in Sect.&nbsp;<span class="InternalRef"><a href="#Sec16">3.2.2</a></span> and in Appendix&nbsp;<span class="InternalRef"><a href="#Sec48">1</a></span>. The other properties were computed by asking human subjects to annotate each of the 1000 object categories (Russakovsky et&nbsp;al. <span class="CitationRef"><a href="#CR65">2013</a></span>)</p></div></figcaption></figure></div><div id="Par51" class="Para">The exact 1000 synsets used for the image classification and single-object localization tasks have changed over the years. There are 639 synsets which have been used in all five ILSVRC challenges so far. In the first year of the challenge synsets were selected randomly from the available ImageNet synsets at the time, followed by manual filtering to make sure the object categories were not too obscure. With the introduction of the object localization challenge in 2011 there were 321 synsets that changed: categories such as “New Zealand beach” which were inherently difficult to localize were removed, and some new categories from ImageNet containing object localization annotations were added. In ILSVRC2012, 90 synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine-grained object classification, as shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig2">2</a></span>. The synsets have remained consistent since year 2012. Appendix&nbsp;<span class="InternalRef"><a href="#Sec47">1</a></span> provides the complete list of object categories used in ILSVRC2012-2014.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig2_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig2_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2</span><p class="SimplePara">The ILSVRC dataset contains many more fine-grained classes compared to the standard PASCAL VOC benchmark; for example, instead of the PASCAL “dog” category there are 120 different breeds of dogs in ILSVRC2012-2014 classification and single-object localization tasks</p></div></figcaption></figure></div></section><section id="Sec11" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.1.2 </span>Collecting Candidate Images for the Image Classification Dataset</h4><p id="Par52" class="Para">Image collection for ILSVRC classification task is the same as the strategy employed for constructing ImageNet (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>). Training images are taken directly from ImageNet. Additional images are collected for the ILSVRC using this strategy and randomly partitioned into the validation and test sets.</p><p id="Par53" class="Para">We briefly summarize the process; (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>) contains further details. Candidate images are collected from the Internet by querying several image search engines. For each synset, the queries are the set of WordNet synonyms. Search engines typically limit the number of retrievable images (on the order of a few hundred to a thousand). To obtain as many images as possible, we expand the query set by appending the queries with the word from parent synsets, if the same word appears in the glossary of the target synset. For example, when querying “whippet”, according to WordNet’s glossary a “small slender dog of greyhound type developed in England”, we also use “whippet dog” and “whippet greyhound.” To further enlarge and diversify the candidate pool, we translate the queries into other languages, including Chinese, Spanish, Dutch and Italian. We obtain accurate translations using WordNets in those languages.</p></section><section id="Sec12" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.1.3 </span>Image Classification Dataset Annotation</h4><p id="Par54" class="Para">Annotating images with corresponding object classes follows the strategy employed by ImageNet (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>). We summarize it briefly here.</p><p id="Par55" class="Para">To collect a highly accurate dataset, we rely on humans to verify each candidate image collected in the previous step for a given synset. This is achieved by using Amazon Mechanical Turk (AMT), an online platform on which one can put up tasks for users for a monetary reward. With a global user base, AMT is particularly suitable for large scale labeling. In each of our labeling tasks, we present the users with a set of candidate images and the definition of the target synset (including a link to Wikipedia). We then ask the users to verify whether each image contains objects of the synset. We encourage users to select images regardless of occlusions, number of objects and clutter in the scene to ensure diversity.</p><p id="Par56" class="Para">While users are instructed to make accurate judgment, we need to set up a quality control system to ensure this accuracy. There are two issues to consider. First, human users make mistakes and not all users follow the instructions. Second, users do not always agree with each other, especially for more subtle or confusing synsets, typically at the deeper levels of the tree. The solution to these issues is to have multiple users independently label the same image. An image is considered positive only if it gets a convincing majority of the votes. We observe, however, that different categories require different levels of consensus among users. For example, while five users might be necessary for obtaining a good consensus on Burmese cat images, a much smaller number is needed for cat images. We develop a simple algorithm to dynamically determine the number of agreements needed for different categories of images. For each synset, we first randomly sample an initial subset of images. At least 10 users are asked to vote on each of these images. We then obtain a confidence score table, indicating the probability of an image being a good image given the consensus among user votes. For each of the remaining candidate images in this synset, we proceed with the AMT user labeling until a pre-determined confidence score threshold is reached.</p><p id="Par57" class="Para"><em class="EmphasisTypeItalic ">Empirical Evaluation</em> Evaluation of the accuracy of the large-scale crowdsourced image annotation system was done on the entire ImageNet (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>). A total of 80 synsets were randomly sampled at every tree depth of the mammal and vehicle subtrees. An independent group of subjects verified the correctness of each of the images. An average of 99.7&nbsp;% precision is achieved across the synsets. We expect similar accuracy on ILSVRC image classification dataset since the image annotation pipeline has remained the same. To verify, we manually checked 1500 ILSVRC2012-2014 image classification test set images (the test set has remained unchanged in these 3&nbsp;years). We found 5 annotation errors, corresponding as expected to 99.7&nbsp;% precision.</p></section><section id="Sec13" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.1.4 </span>Image Classification Dataset Statistics</h4><div id="Par58" class="Para">Using the image collection and annotation procedure described in previous sections, we collected a large-scale dataset used for ILSVRC classification task. There are 1000 object classes and approximately 1.2 million training images, 50 thousand validation images and 100 thousand test images. Table&nbsp;<span class="InternalRef"><a href="#Tab2">2</a></span> documents the size of the dataset over the years of the challenge.<div id="Tab2" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 2</span><p class="SimplePara">Scale of ILSVRC image classification task (minimum per class - maximum per class)</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"><col class="tcol5 align-left"><col class="tcol6 align-left"></colgroup><thead><tr><th><p class="SimplePara">Year</p></th><th><p class="SimplePara">Train images (per class)</p></th><th><p class="SimplePara">Val images (per class)</p></th><th colspan="3"><p class="SimplePara">Test images (per class)</p></th></tr></thead><tbody><tr><td><p class="SimplePara">Image classification annotations (1000 object classes)</p></td><td colspan="5">&nbsp;</td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ILSVRC2010</p></td><td><p class="SimplePara">1,261,406 (668–3047)</p></td><td><p class="SimplePara">50,000 (50)</p></td><td colspan="3"><p class="SimplePara">150,000 (150)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ILSVRC2011</p></td><td><p class="SimplePara">1,229,413 (384–1300)</p></td><td><p class="SimplePara">50,000 (50)</p></td><td colspan="3"><p class="SimplePara">100,000 (100)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ILSVRC2012-14</p></td><td><p class="SimplePara">1,281,167 (732–1300)</p></td><td><p class="SimplePara">50,000 (50)</p></td><td colspan="3"><p class="SimplePara">100,000 (100)</p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara">The numbers in parentheses correspond to (minimum per class–maximum per class). The 1000 classes change from year to year but are consistent between image classification and single-object localization tasks in the same year. All images from the image classification task may be used for single-object localization</p></div></div></div></section></section><section id="Sec14" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.2 </span>Single-Object Localization Dataset Construction</h3><p id="Par59" class="Para">The single-object localization task evaluates the ability of an algorithm to localize one instance of an object category. It was introduced as a taster task in ILSVRC 2011, and became an official part of ILSVRC in 2012.</p><p id="Par60" class="Para">The key challenge was developing a scalable crowdsourcing method for object bounding box annotation. Our three-step self-verifying pipeline is described in Sect.&nbsp;<span class="InternalRef"><a href="#Sec15">3.2.1</a></span>. Having the dataset collected, we perform detailed analysis in Sect.&nbsp;<span class="InternalRef"><a href="#Sec16">3.2.2</a></span> to ensure that the dataset is sufficiently varied to be suitable for evaluation of object localization algorithms.</p><p id="Par61" class="Para"><em class="EmphasisTypeItalic ">Object Classes and Candidate Images</em> The object classes for single-object localization task are the same as the object classes for image classification task described above in Sect.&nbsp;<span class="InternalRef"><a href="#Sec9">3.1</a></span>. The training images for localization task are a subset of the training images used for image classification task, and the validation and test images are the same between both tasks.</p><p id="Par62" class="Para"><em class="EmphasisTypeItalic ">Bounding Box Annotation</em> Recall that for the image classification task every image was annotated with one object class label, corresponding to one object that is present in an image. For the single-object localization task, every validation and test image and a subset of the training images are annotated with axis-aligned bounding boxes around every instance of this object.</p><p id="Par63" class="Para">Every bounding box is required to be as small as possible while including all visible parts of the object instance. An alternate annotation procedure could be to annotate the <em class="EmphasisTypeItalic ">full (estimated) extent</em> of the object: e.g., if a person’s legs are occluded and only the torso is visible, the bounding box could be drawn to include the likely location of the legs. However, this alternative procedure is inherently ambiguous and ill-defined, leading to disagreement among annotators and among researchers (what is the true “most likely” extent of this object?). We follow the standard protocol of only annotating visible object parts (Russell et&nbsp;al. <span class="CitationRef"><a href="#CR66">2007</a></span>; Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR19">2010</a></span>).<sup><a href="#Fn5" id="Fn5_source">5</a></sup></p><section id="Sec15" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.2.1 </span>Bounding Box Object Annotation System</h4><p id="Par65" class="Para">We summarize the crowdsourced bounding box annotation system described in detail in Su et&nbsp;al. (<span class="CitationRef"><a href="#CR76">2012</a></span>). The goal is to build a system that is fully automated, highly accurate, and cost-effective. Given a collection of images where the object of interest has been verified to exist, for each image the system collects a tight bounding box for every instance of the object.</p><div id="Par66" class="Para">There are two requirements:<div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><p id="Par67" class="Para"><em class="EmphasisTypeItalic ">Quality</em> Each bounding box needs to be tight, i.e. the smallest among all bounding boxes that contains all visible parts of the object. This facilitates the object detection learning algorithms by providing the precise location of each object instance;</p></li><li><p id="Par68" class="Para"><em class="EmphasisTypeItalic ">Coverage</em> Every object instance needs to have a bounding box. This is important for training localization algorithms because it tells the learning algorithms with certainty what is not the object.</p></li></ul></div>The core challenge of building such a system is effectively controlling the data quality with minimal cost. Our key observation is that drawing a bounding box is significantly more difficult and time consuming than giving answers to multiple choice questions. Thus quality control through additional verification tasks is more cost-effective than consensus-based algorithms. This leads to the following workflow with simple basic subtasks:<div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(1)</span><div class="ItemContent"><p id="Par69" class="Para"><em class="EmphasisTypeItalic ">Drawing</em> A worker draws one bounding box around one instance of an object on the given image.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(2)</span><div class="ItemContent"><p id="Par70" class="Para"><em class="EmphasisTypeItalic ">Quality verification</em> A second worker checks if the bounding box is correctly drawn.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(3)</span><div class="ItemContent"><p id="Par71" class="Para"><em class="EmphasisTypeItalic ">Coverage verification</em> A third worker checks if all object instances have bounding boxes.</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div>The sub-tasks are designed following two principles. First, the tasks are made as simple as possible. For example, instead of asking the worker to draw all bounding boxes on the same image, we ask the worker to draw only one. This reduces the complexity of the task. Second, each task has a fixed and predictable amount of work. For example, assuming that the input images are clean (object presence is correctly verified) and the coverage verification tasks give correct results, the amount of work of the drawing task is always that of providing exactly one bounding box.</div><p id="Par72" class="Para">Quality control on Tasks 2 and 3 is implemented by embedding “gold standard” images where the correct answer is known. Worker training for each of these subtasks is described in detail in Su et&nbsp;al. (<span class="CitationRef"><a href="#CR76">2012</a></span>).</p><p id="Par73" class="Para"><em class="EmphasisTypeItalic ">Empirical Evaluation</em> The system is evaluated on 10 categories with ImageNet (Deng et&nbsp;al. <span class="CitationRef"><a href="#CR14">2009</a></span>): balloon, bear, bed, bench, beach, bird, bookshelf, basketball hoop, bottle, and people. A subset of 200 images are randomly sampled from each category. On the image level, our evaluation shows that 97.9&nbsp;% images are completely covered with bounding boxes. For the remaining 2.1&nbsp;%, some bounding boxes are missing. However, these are all difficult cases: the size is too small, the boundary is blurry, or there is strong shadow.</p><p id="Par74" class="Para">On the bounding box level, 99.2&nbsp;% of all bounding boxes are accurate (the bounding boxes are visibly tight). The remaining 0.8&nbsp;% are somewhat off. No bounding boxes are found to have less than 50&nbsp;% intersection over union overlap with ground truth.</p><p id="Par75" class="Para">Additional evaluation of the overall cost and an analysis of quality control can be found in Su et&nbsp;al. (<span class="CitationRef"><a href="#CR76">2012</a></span>).</p></section><section id="Sec16" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.2.2 </span>Single-Object Localization Dataset Statistics</h4><div id="Par76" class="Para">Using the annotation procedure described above, we collect a large set of bounding box annotations for the ILSVRC single-object classification task. All 50 thousand images in the validation set and 100 thousand images in the test set are annotated with bounding boxes around all instances of the ground truth object class (one object class per image). In addition, in ILSVRC2011 25&nbsp;% of training images are annotated with bounding boxes the same way, yielding more than 310 thousand annotated images with more than 340 thousand annotated object instances. In ILSVRC2012 40&nbsp;% of training images are annotated, yielding more than 520 thousand annotated images with more than 590 thousand annotated object instances. Table&nbsp;<span class="InternalRef"><a href="#Tab3">3</a></span> documents the size of this dataset.<div id="Tab3" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 3</span><p class="SimplePara">Scale of additional annotations for the ILSVRC single-object localization task (minimum per class - maximum per class)</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"><col class="tcol5 align-left"><col class="tcol6 align-left"></colgroup><thead><tr><th><p class="SimplePara">Year</p></th><th><p class="SimplePara">Train images with bbox annotations (per class)</p></th><th><p class="SimplePara">Train bboxes annotated (per class)</p></th><th><p class="SimplePara">Val images with bbox annotations (per class)</p></th><th><p class="SimplePara">Val bboxes annotated (per class)</p></th><th><p class="SimplePara">Test images with bbox annotations</p></th></tr></thead><tbody><tr><td colspan="6"><p class="SimplePara">Additional annotations for single-object localization (1000 object classes)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ILSVRC2011</p></td><td><p class="SimplePara">315,525 (104–1256)</p></td><td><p class="SimplePara">344,233 (114–1502)</p></td><td><p class="SimplePara">50,000 (50)</p></td><td><p class="SimplePara">55,388 (50–118)</p></td><td><p class="SimplePara">100,000</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ILSVRC2012-14</p></td><td><p class="SimplePara">523,966 (91–1268)</p></td><td><p class="SimplePara">593,173 (92–1418)</p></td><td><p class="SimplePara">50,000 (50)</p></td><td><p class="SimplePara">64,058 (50–189)</p></td><td><p class="SimplePara">100,000</p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara">The numbers in parentheses correspond to (minimum per class–maximum per class). The 1000 classes change from year to year but are consistent between image classification and single-object localization tasks in the same year. All images from the image classification task may be used for single-object localization</p></div></div></div><p id="Par77" class="Para">In addition to the size of the dataset, we also analyze the level of difficulty of object localization in these images compared to the PASCAL VOC benchmark. We compute statistics on the ILSVRC2012 single-object localization validation set images compared to PASCAL VOC 2012 validation images.</p><p id="Par78" class="Para">Real-world scenes are likely to contain multiple instances of some objects, and nearby object instances are particularly difficult to delineate. The average object category in ILSVRC has <span class="InlineEquation" id="IEq6">\(1.61\)</span> target object instances on average per positive image, with each instance having on average <span class="InlineEquation" id="IEq7">\(0.47\)</span> neighbors (adjacent instances of the same object category). This is comparable to <span class="InlineEquation" id="IEq8">\(1.69\)</span> instances per positive image and <span class="InlineEquation" id="IEq9">\(0.52\)</span> neighbors per instance for an average object class in PASCAL.</p><p id="Par79" class="Para">As described in Hoiem et&nbsp;al. (<span class="CitationRef"><a href="#CR35">2012</a></span>), smaller objects tend to be significantly more difficult to localize. In the average object category in PASCAL the object occupies 24.1&nbsp;% of the image area, and in ILSVRC 35.8&nbsp;%. However, PASCAL has only 20 object categories while ILSVRC has 1000. The 537 object categories of ILSVRC with the smallest objects on average occupy the same fraction of the image as PASCAL objects: 24.1&nbsp;%. Thus even though on average the object instances tend to be bigger in ILSVRC images, there are more than 25 times more object categories than in PASCAL VOC with the same average object scale.</p><p id="Par80" class="Para">Appendix&nbsp;<span class="InternalRef"><a href="#Sec48">1</a></span> and Russakovsky et&nbsp;al. (<span class="CitationRef"><a href="#CR65">2013</a></span>) have additional comparisons.</p></section></section><section id="Sec17" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.3 </span>Object Detection Dataset Construction</h3><p id="Par81" class="Para">The ILSVRC task of object detection evaluates the ability of an algorithm to name and localize <em class="EmphasisTypeItalic ">all</em> instances of <em class="EmphasisTypeItalic ">all</em> target objects present in an image. It is much more challenging than object localization because some object instances may be small/occluded/difficult to accurately localize, and the algorithm is expected to locate them all, not just the one it finds easiest.</p><p id="Par82" class="Para">There are three key challenges in collecting the object detection dataset. The first challenge is selecting the set of common objects which tend to appear in cluttered photographs and are well-suited for benchmarking object detection performance. Our approach relies on statistics of the object localization dataset and the tradition of the PASCAL VOC challenge (Sect.&nbsp;<span class="InternalRef"><a href="#Sec18">3.3.1</a></span>).</p><p id="Par83" class="Para">The second challenge is obtaining a much more varied set of scene images than those used for the image classification and single-object localization datasets. Section&nbsp;<span class="InternalRef"><a href="#Sec19">3.3.2</a></span> describes the procedure for utilizing as much data from the single-object localization dataset as possible and supplementing it with Flickr images queried using hundreds of manually designed high-level queries.</p><p id="Par84" class="Para">The third, and biggest, challenge is completely annotating this dataset with all the objects. This is done in two parts. Section&nbsp;<span class="InternalRef"><a href="#Sec20">3.3.3</a></span> describes the first part: our hierarchical strategy for obtaining the list of all target objects which occur within every image. This is necessary since annotating in a straight-forward way by creating a task for every (image, object class) pair is no longer feasible at this scale. Appendix&nbsp;<span class="InternalRef"><a href="#Sec51">1</a></span> describes the second part: annotating the bounding boxes around these objects, using the single-object localization bounding box annotation pipeline of Sect.&nbsp;<span class="InternalRef"><a href="#Sec15">3.2.1</a></span> along with extra verification to ensure that <em class="EmphasisTypeItalic ">every</em> instance of the object is annotated with exactly <em class="EmphasisTypeItalic ">one</em> bounding box.</p><section id="Sec18" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.3.1 </span>Defining Object Categories for the Object Detection Dataset</h4><p id="Par85" class="Para">There are 200 object classes hand-selected for the detection task, eacg corresponding to a synset within ImageNet. These were chosen to be mostly basic-level object categories that would be easy for people to identify and label. The rationale is that the object detection system developed for this task can later be combined with a fine-grained classification model to further classify the objects if a finer subdivision is desired.<sup><a href="#Fn6" id="Fn6_source">6</a></sup> As with the 1000 classification classes, the synsets are selected such that there is no overlap: for any synsets <span class="InlineEquation" id="IEq10">\(i\)</span> and <span class="InlineEquation" id="IEq11">\(j\)</span>, <span class="InlineEquation" id="IEq12">\(i\)</span> is not an ancestor of <span class="InlineEquation" id="IEq13">\(j\)</span> in the ImageNet hierarchy.</p><p id="Par87" class="Para">The selection of the 200 object detection classes in 2013 was guided by the ILSVRC 2012 classification and localization dataset. Starting with 1000 object classes and their bounding box annotations we first eliminated all object classes which tended to be too “big” in the image (on average the object area was greater than 50&nbsp;% of the image area). These were classes such as T-shirt, spiderweb, or manhole cover. We then manually eliminated all classes which we did not feel were well-suited for detection, such as hay, barbershop, or poncho. This left 494 object classes which were merged into basic-level categories: for example, different species of birds were merged into just the “bird” class. The classes remained the same in ILSVRC2014. Appendix&nbsp;<span class="InternalRef"><a href="#Sec50">1</a></span> contains the complete list of object categories used in ILSVRC2013-2014 (in the context of the hierarchy described in Sect.&nbsp;<span class="InternalRef"><a href="#Sec20">3.3.3</a></span>).</p><div id="Par88" class="Para">Staying mindful of the tradition of the PASCAL VOC dataset we also tried to ensure that the set of 200 classes contains as many of the 20 PASCAL VOC classes as possible. Table&nbsp;<span class="InternalRef"><a href="#Tab4">4</a></span> shows the correspondences. The changes that were done were to ensure more accurate and consistent crowdsourced annotations. The object class with the weakest correspondence is “potted plant” in PASCAL VOC, corresponding to “flower pot” in ILSVRC. “Potted plant” was one of the most challenging object classes to annotate consistently among the PASCAL VOC classes, and in order to obtain accurate annotations using crowdsourcing we had to restrict the definition to a more concrete object.<div id="Tab4" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 4</span><p class="SimplePara">Correspondences between the object classes in the PASCAL VOC (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR19">2010</a></span>) and the ILSVRC detection task</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"></colgroup><thead><tr><th rowspan="2"><p class="SimplePara">Class name in PASCAL VOC (20 classes)</p></th><th rowspan="2"><p class="SimplePara">Closest class in ILSVRC-DET (200 classes)</p></th><th colspan="2"><p class="SimplePara">Average object scale (%)</p></th></tr><tr><th><p class="SimplePara">PASCAL VOC</p></th><th><p class="SimplePara">ILSVRC-DET</p></th></tr></thead><tbody><tr><td><p class="SimplePara">Aeroplane</p></td><td><p class="SimplePara">Airplane</p></td><td><p class="SimplePara">29.7</p></td><td><p class="SimplePara">22.4</p></td></tr><tr><td><p class="SimplePara">Bicycle</p></td><td><p class="SimplePara">Bicycle</p></td><td><p class="SimplePara">29.3</p></td><td><p class="SimplePara">14.3</p></td></tr><tr><td><p class="SimplePara">Bird</p></td><td><p class="SimplePara">Bird</p></td><td><p class="SimplePara">15.9</p></td><td><p class="SimplePara">20.1</p></td></tr><tr><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Boat</em></p></td><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Watercraft</em></p></td><td><p class="SimplePara">15.2</p></td><td><p class="SimplePara">16.5</p></td></tr><tr><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Bottle</em></p></td><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Wine bottle</em></p></td><td><p class="SimplePara">7.3</p></td><td><p class="SimplePara">10.4</p></td></tr><tr><td><p class="SimplePara">Bus</p></td><td><p class="SimplePara">Bus</p></td><td><p class="SimplePara">29.9</p></td><td><p class="SimplePara">22.1</p></td></tr><tr><td><p class="SimplePara">Car</p></td><td><p class="SimplePara">Car</p></td><td><p class="SimplePara">14.0</p></td><td><p class="SimplePara">13.4</p></td></tr><tr><td><p class="SimplePara">Cat</p></td><td><p class="SimplePara">Domestic cat</p></td><td><p class="SimplePara">46.8</p></td><td><p class="SimplePara">29.8</p></td></tr><tr><td><p class="SimplePara">Chair</p></td><td><p class="SimplePara">Chair</p></td><td><p class="SimplePara">12.8</p></td><td><p class="SimplePara">10.1</p></td></tr><tr><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Cow</em></p></td><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Cattle</em></p></td><td><p class="SimplePara">19.3</p></td><td><p class="SimplePara">13.5</p></td></tr><tr><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Dining table</em></p></td><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Table</em></p></td><td><p class="SimplePara">29.1</p></td><td><p class="SimplePara">30.3</p></td></tr><tr><td><p class="SimplePara">Dog</p></td><td><p class="SimplePara">Dog</p></td><td><p class="SimplePara">37.0</p></td><td><p class="SimplePara">28.9</p></td></tr><tr><td><p class="SimplePara">Horse</p></td><td><p class="SimplePara">Horse</p></td><td><p class="SimplePara">29.5</p></td><td><p class="SimplePara">18.5</p></td></tr><tr><td><p class="SimplePara">Motorbike</p></td><td><p class="SimplePara">Motorcyle</p></td><td><p class="SimplePara">32.0</p></td><td><p class="SimplePara">20.7</p></td></tr><tr><td><p class="SimplePara">Person</p></td><td><p class="SimplePara">Person</p></td><td><p class="SimplePara">17.5</p></td><td><p class="SimplePara">19.3</p></td></tr><tr><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Potted plant</em></p></td><td><p class="SimplePara"><em class="EmphasisTypeItalic ">Flower pot</em></p></td><td><p class="SimplePara">12.3</p></td><td><p class="SimplePara">8.1</p></td></tr><tr><td><p class="SimplePara">Sheep</p></td><td><p class="SimplePara">Sheep</p></td><td><p class="SimplePara">12.2</p></td><td><p class="SimplePara">17.3</p></td></tr><tr><td><p class="SimplePara">Sofa</p></td><td><p class="SimplePara">Sofa</p></td><td><p class="SimplePara">41.7</p></td><td><p class="SimplePara">44.4</p></td></tr><tr><td><p class="SimplePara">Train</p></td><td><p class="SimplePara">Train</p></td><td><p class="SimplePara">35.4</p></td><td><p class="SimplePara">35.1</p></td></tr><tr><td><p class="SimplePara">TV/monitor</p></td><td><p class="SimplePara">TV or monitor</p></td><td><p class="SimplePara">14.6</p></td><td><p class="SimplePara">11.2</p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara">Object scale is the fraction of image area (reported in percent) occupied by an object instance. It is computed on the validation sets of PASCAL VOC 2012 and of ILSVRC-DET. The average object scale is 24.1&nbsp;% across the 20 PASCAL VOC categories and 20.3&nbsp;% across the 20 corresponding ILSVRC-DET categories. Sect.&nbsp;<span class="InternalRef"><a href="#Sec21">3.3.4</a></span> reports additional dataset statistics</p></div></div></div></section><section id="Sec19" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.3.2 </span>Collecting Images for the Object Detection Dataset</h4><div id="Par89" class="Para">Many images for the detection task were collected differently than the images in ImageNet and the classification and single-object localization tasks. Figure&nbsp;<span class="InternalRef"><a href="#Fig3">3</a></span> summarizes the types of images that were collected. Ideally all of these images would be scene images fully annotated with all target categories. However, given budget constraints our goal was to provide as much suitable detection data as possible, even if the images were drawn from a few different sources and distributions.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig3_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig3_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3</span><p class="SimplePara">Summary of images collected for the detection task. Images in <em class="EmphasisTypeItalic ">green</em> (<em class="EmphasisTypeItalic ">bold</em>) boxes have all instances of all 200 detection object classes fully annotated. Table&nbsp;<span class="InternalRef"><a href="#Tab5">5</a></span> lists the complete statistics</p></div></figcaption></figure></div><p id="Par90" class="Para">The validation and test detection set images come from two sources (percent of images from each source in parentheses). The first source (77&nbsp;%) is images from ILSVRC2012 single-object localization validation and test sets corresponding to the 200 detection classes (or their children in the ImageNet hierarchy). Images where the target object occupied more than 50&nbsp;% of the image area were discarded, since they were unlikely to contain other objects of interest. The second source (23&nbsp;%) is images from Flickr collected specifically for detection task. We queried Flickr using a large set of manually defined queries, such as “kitchenette” or “Australian zoo” to retrieve images of scenes likely to contain several objects of interest. Appendix&nbsp;<span class="InternalRef"><a href="#Sec49">1</a></span> contains the full list. We also added pairwise queries, or queries with two target object names such as “tiger lion,” which also often returned cluttered scenes.</p><div id="Par91" class="Para">Figure&nbsp;<span class="InternalRef"><a href="#Fig4">4</a></span> shows a random set of both types of validation images. Images were randomly split, with 33&nbsp;% going into the validation set and 67&nbsp;% into the test set.<sup><a href="#Fn7" id="Fn7_source">7</a></sup><figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig4_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig4_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4</span><p class="SimplePara">Random selection of images in ILSVRC detection validation set. The images in the <em class="EmphasisTypeItalic ">top four rows</em> were taken from ILSVRC2012 single-object localization validation set, and the images in the <em class="EmphasisTypeItalic ">bottom four rows</em> were collected from Flickr using scene-level queries</p></div></figcaption></figure></div><p id="Par93" class="Para">The training set for the detection task comes from three sources of images (percent of images from each source in parentheses). The first source (63&nbsp;%) is all training images from ILSVRC2012 single-object localization task corresponding to the 200 detection classes (or their children in the ImageNet hierarchy). We did not filter by object size, allowing teams to take advantage of all the positive examples available. The second source (24&nbsp;%) is negative images which were part of the original ImageNet collection process but voted as negative: for example, some of the images were collected from Flickr and search engines for the ImageNet synset “animals” but during the manual verification step did not collect enough votes to be considered as containing an “animal.” These images were manually re-verified for the detection task to ensure that they did not in fact contain the target objects. The third source (13&nbsp;%) is images collected from Flickr specifically for the detection task. These images were added for ILSVRC2014 following the same protocol as the second type of images in the validation and test set. This was done to bring the training and testing distributions closer together.</p></section><section id="Sec20" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.3.3 </span>Complete Image-Object Annotation for the Object Detection Dataset</h4><p id="Par94" class="Para">The key challenge in annotating images for the object detection task is that all objects in all images need to be labeled. Suppose there are N inputs (images) which need to be annotated with the presence or absence of K labels (objects). A naïve approach would query humans for each combination of input and label, requiring <span class="InlineEquation" id="IEq14">\(NK\)</span> queries. However, N and K can be very large and the cost of this exhaustive approach quickly becomes prohibitive. For example, annotating 60,000 validation and test images with the presence or absence of 200 object classes for the detection task naïvely would take 80 times more effort than annotating 150,000 validation and test images with 1 object each for the classification task—and this is not even counting the additional cost of collecting bounding box annotations around each object instance. This quickly becomes infeasible.</p><div id="Par95" class="Para">In Deng et&nbsp;al. (<span class="CitationRef"><a href="#CR15">2014</a></span>) we study strategies for scalable multilabel annotation, or for efficiently acquiring multiple labels from humans for a collection of items. We exploit three key observations for labels in real world applications (illustrated in Fig.&nbsp;<span class="InternalRef"><a href="#Fig5">5</a></span>):<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig5_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig5_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5</span><p class="SimplePara">Consider the problem of binary multi-label annotation. For each input (e.g., image) and each label (e.g., object), the goal is to determine the presence or absense (<em class="EmphasisTypeItalic ">plus</em> or <em class="EmphasisTypeItalic ">minus</em>) of the label (e.g., decide if the object is present in the image). Multi-label annotation becomes much more efficient when considering real-world structure of data: correlation between labels, hierarchical organization of concepts, and sparsity of labels</p></div></figcaption></figure><div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(1)</span><div class="ItemContent"><p id="Par96" class="Para"><em class="EmphasisTypeItalic ">Correlation</em> Subsets of labels are often highly correlated. Objects such as a computer keyboard, mouse and monitor frequently co-occur in images. Similarly, some labels tend to all be absent at the same time. For example, all objects that require electricity are usually absent in pictures taken outdoors. This suggests that we could potentially fill in the values of multiple labels by grouping them into only one query for humans. Instead of checking if dog, cat, rabbit etc. are present in the photo, we just check about the “animal” group If the answer is no, then this implies a no for all categories in the group.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(2)</span><div class="ItemContent"><p id="Par97" class="Para"><em class="EmphasisTypeItalic ">Hierarchy</em> The above example of grouping dog, cat, rabbit etc. into animal has implicitly assumed that labels can be grouped together and humans can efficiently answer queries about the group as a whole. This brings up our second key observation: humans organize semantic concepts into hierarchies and are able to efficiently categorize at higher semantic levels (Thorpe et&nbsp;al. <span class="CitationRef"><a href="#CR79">1996</a></span>), e.g. humans can determine the presence of an animal in an image as fast as every type of animal individually. This leads to substantial cost savings.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(3)</span><div class="ItemContent"><p id="Par98" class="Para"><em class="EmphasisTypeItalic ">Sparsity</em> The values of labels for each image tend to be sparse, i.e. an image is unlikely to contain more than a dozen types of objects, a small fraction of the hundreds of object categories. This enables rapid elimination of many objects by quickly filling in no. With a high degree of sparsity, an efficient algorithm can have a cost which grows logarithmically with the number of objects instead of linearly.</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div>We propose algorithmic strategies that exploit the above intuitions. The key is to select a sequence of queries for humans such that we achieve the same labeling results with only a fraction of the cost of the naïve approach. The main challenges include how to measure cost and utility of queries, how to construct good queries, and how to dynamically order them. A detailed description of the generic algorithm, along with theoretical analysis and empirical evaluation, is presented in Deng et&nbsp;al. (<span class="CitationRef"><a href="#CR15">2014</a></span>).</div><div id="Par99" class="Para"><em class="EmphasisTypeItalic ">Application of the Generic Multi-class Labeling Algorithm to Our Setting</em> The generic algorithm automatically selects the most informative queries to ask based on object label statistics learned from the training set. In our case of 200 object classes, since obtaining the training set was by itself challenging we chose to design the queries by hand. We created a hierarchy of queries of the type “is there a... in the image?” For example, one of the high-level questions was “is there an animal in the image?” We ask the crowd workers this question about every image we want to label. The children of the “animal” question would correspond to specific examples of animals: for example, “is there a mammal in the image?” or “is there an animal with no legs?” To annotate images efficiently, these questions are asked only on images determined to contain an animal. The 200 leaf node questions correspond to the 200 target objects, e.g., “is there a cat in the image?”. A few sample iterations of the algorithm are shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig6">6</a></span>.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig6_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig6_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6</span><p class="SimplePara">Our algorithm dynamically selects the next query to efficiently determine the presence or absence of every object in every image. <em class="EmphasisTypeItalic ">Green</em> denotes a positive annotation and <em class="EmphasisTypeItalic ">red</em> denotes a negative annotation. This toy example illustrates a sample progression of the algorithm for one label (cat) on a set of images</p></div></figcaption></figure></div><p id="Par100" class="Para">Algorithm&nbsp;1 is the formal algorithm for labeling an image with the presence or absence of each target object category. With this algorithm in mind, the hierarchy of questions was constructed following the principle that false positives only add extra cost whereas false negatives can significantly affect the quality of the labeling. Thus, it is always better to stick with more general but less ambiguous questions, such as “is there a mammal in the image?” as opposed to asking overly specific but potentially ambiguous questions, such as “is there an animal that can climb trees?” Constructing this hierarchy was a surprisingly time-consuming process, involving multiple iterations to ensure high accuracy of labeling and avoid question ambiguity. Appendix&nbsp;<span class="InternalRef"><a href="#Sec50">1</a></span> shows the constructed hierarchy.</p><div id="Par101" class="Para"><em class="EmphasisTypeItalic ">Bounding Box Annotation</em> Once all images are labeled with the presence or absence of all object categories we use the bounding box system described in Sect.&nbsp;<span class="InternalRef"><a href="#Sec15">3.2.1</a></span> along with some additional modifications of Appendix&nbsp;<span class="InternalRef"><a href="#Sec51">1</a></span> to annotate the location of every instance of every present object category.<figure class="Figure" id="Figa"><div class="MediaObject" id="MO7"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Figa_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Figa_HTML.gif" alt=""></a></div></figure></div></section><section id="Sec21" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.3.4 </span>Object Detection Dataset Statistics</h4><div id="Par102" class="Para">Using the procedure described above, we collect a large-scale dataset for ILSVRC object detection task. There are 200 object classes and approximately 450K training images, 20K validation images and 40K test images. Table&nbsp;<span class="InternalRef"><a href="#Tab5">5</a></span> documents the size of the dataset over the years of the challenge. The major change between ILSVRC2013 and ILSVRC2014 was the addition of 60,658 fully annotated training images.<div id="Tab5" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 5</span><p class="SimplePara">Scale of ILSVRC object detection task</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"><col class="tcol5 align-left"><col class="tcol6 align-left"></colgroup><thead><tr><th><p class="SimplePara">Year</p></th><th><p class="SimplePara">Train images (per class)</p></th><th><p class="SimplePara">Train bboxes annotated (per class)</p></th><th><p class="SimplePara">Val images (per class)</p></th><th><p class="SimplePara">Val bboxes annotated (per class)</p></th><th><p class="SimplePara">Test images</p></th></tr></thead><tbody><tr><td colspan="6"><p class="SimplePara">Object detection annotations (200 object classes)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ILSVRC2013</p></td><td><p class="SimplePara">395909 (417-561-66911 pos, 185-4130-10073 neg)</p></td><td><p class="SimplePara">345854 (438-660-73799)</p></td><td><p class="SimplePara">21121 (23-58-5791 pos, rest neg)</p></td><td><p class="SimplePara">55501 (31-111-12824)</p></td><td><p class="SimplePara">40152</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ILSVRC2014</p></td><td><p class="SimplePara">456567 (461-823-67513 pos, 42945-64614-70626 neg)</p></td><td><p class="SimplePara">478807 (502-1008-74517)</p></td><td><p class="SimplePara">21121 (23-58-5791 pos, rest neg)</p></td><td><p class="SimplePara">55501 (31-111-12824)</p></td><td><p class="SimplePara">40152</p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara">Numbers in parentheses correspond to (minimum per class–median per class–maximum per class)</p></div></div></div><p id="Par103" class="Para">Prior to ILSVRC, the object detection benchmark was the PASCAL VOC challenge (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR19">2010</a></span>). ILSVRC has <span class="InlineEquation" id="IEq40">\(10\)</span> times more object classes than PASCAL VOC (200 vs 20), <span class="InlineEquation" id="IEq41">\(10.6\)</span> times more fully annotated training images (60,658 vs 5,717), <span class="InlineEquation" id="IEq42">\(35.2\)</span> times more training objects (478,807 vs 13,609), <span class="InlineEquation" id="IEq43">\(3.5\)</span> times more validation images (20,121 vs 5823) and <span class="InlineEquation" id="IEq44">\(3.5\)</span> times more validation objects (55,501 vs 15,787). ILSVRC has <span class="InlineEquation" id="IEq45">\(2.8\)</span> annotated objects per image on the validation set, compared to <span class="InlineEquation" id="IEq46">\(2.7\)</span> in PASCAL VOC. The average object in ILSVRC takes up 17.0&nbsp;% of the image area and in PASCAL VOC takes up 20.7&nbsp;%; Table&nbsp;<span class="InternalRef"><a href="#Tab4">4</a></span> contains per-class comparisons. Additionally, ILSVRC contains a wide variety of objects, including tiny objects such as sunglasses (1.3&nbsp;% of image area on average), ping-pong balls (1.5&nbsp;% of image area on average) and basketballs (2.0&nbsp;% of image area on average).</p></section></section></div></section><section id="Sec22" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">4 </span>Evaluation at Large Scale</h2><div class="content"><p id="Par104" class="Para">Once the dataset has been collected, we need to define a standardized evaluation procedure for algorithms. Some measures have already been established by datasets such as the Caltech 101 (Fei-Fei et&nbsp;al. <span class="CitationRef"><a href="#CR22">2004</a></span>) for image classification and PASCAL VOC (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR18">2012</a></span>) for both image classification and object detection. To adapt these procedures to the large-scale setting we had to address three key challenges. First, for the image classification and single-object localization tasks only one object category could be labeled in each image due to the scale of the dataset. This created potential ambiguity during evaluation (addressed in Sect.&nbsp;<span class="InternalRef"><a href="#Sec23">4.1</a></span>). Second, evaluating localization of object instances is inherently difficult in some images which contain a cluster of objects (addressed in Sect.&nbsp;<span class="InternalRef"><a href="#Sec24">4.2</a></span>). Third, evaluating localization of object instances which occupy few pixels in the image is challenging (addressed in Sect.&nbsp;<span class="InternalRef"><a href="#Sec25">4.3</a></span>).</p><p id="Par105" class="Para">In this section we describe the standardized evaluation criteria for each of the three ILSVRC tasks. We elaborate further on these and other more minor challenges with large-scale evaluation. Appendix&nbsp;<span class="InternalRef"><a href="#Sec52">1</a></span> describes the submission protocol and other details of running the competition itself.</p><section id="Sec23" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.1 </span>Image Classification</h3><div id="Par106" class="Para">The scale of ILSVRC classification task (1000 categories and more than a million of images) makes it very expensive to label every instance of every object in every image. Therefore, on this dataset only one object category is labeled in each image. This creates ambiguity in evaluation. For example, an image might be labeled as a “strawberry” but contain both a strawberry and an apple. Then an algorithm would not know which one of the two objects to name. For the image classification task we allowed an algorithm to identify multiple (up to 5) objects in an image and not be penalized as long as one of the objects indeed corresponded to the ground truth label. Figure&nbsp;<span class="InternalRef"><a href="#Fig7">7</a></span> (top row) shows some examples.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO8"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig7_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig7_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 7</span><p class="SimplePara">Tasks in ILSVRC. The first column shows the ground truth labeling on an example image, and the next three show three sample outputs with the corresponding evaluation score</p></div></figcaption></figure></div><p id="Par107" class="Para">Concretely, each image <span class="InlineEquation" id="IEq47">\(i\)</span> has a single class label <span class="InlineEquation" id="IEq48">\(C_i\)</span>. An algorithm is allowed to return 5 labels <span class="InlineEquation" id="IEq49">\(c_{i1},\dots c_{i5}\)</span>, and is considered correct if <span class="InlineEquation" id="IEq50">\(c_{ij} = C_i\)</span> for some <span class="InlineEquation" id="IEq51">\(j\)</span>.</p><div id="Par108" class="Para">Let the error of a prediction <span class="InlineEquation" id="IEq52">\(d_{ij} = d(c_{ij},C_i)\)</span> be <span class="InlineEquation" id="IEq53">\(1\)</span> if <span class="InlineEquation" id="IEq54">\(c_{ij} \ne C_i\)</span> and <span class="InlineEquation" id="IEq55">\(0\)</span> otherwise. The error of an algorithm is the fraction of test images on which the algorithm makes a mistake:<div id="Equ1" class="Equation EquationMathjax"><div class="EquationContent">$$\begin{aligned} \text{ error } = \frac{1}{N} \sum _{i=1}^N \min _j d_{ij} \end{aligned}$$</div> <div class="EquationNumber">(1)</div></div>We used two additional measures of error. First, we evaluated top-1 error. In this case algorithms were penalized if their highest-confidence output label <span class="InlineEquation" id="IEq56">\(c_{i1}\)</span> did not match ground truth class <span class="InlineEquation" id="IEq57">\(C_i\)</span>. Second, we evaluated hierarchical error. The intuition is that confusing two nearby classes (such as two different breeds of dogs) is not as harmful as confusing a dog for a container ship. For the hierarchical criteria, the cost of one misclassification, <span class="InlineEquation" id="IEq58">\(d(c_{ij},C_i)\)</span>, is defined as the height of the lowest common ancestor of <span class="InlineEquation" id="IEq59">\(c_{ij}\)</span> and <span class="InlineEquation" id="IEq60">\(C_i\)</span> in the ImageNet hierarchy. The height of a node is the length of the longest path to a leaf node (leaf nodes have height zero).</div><p id="Par109" class="Para">However, in practice we found that all three measures of error (top-5, top-1, and hierarchical) produced the same ordering of results. Thus, since ILSVRC2012 we have been exclusively using the top-5 metric which is the simplest and most suitable to the dataset.</p></section><section id="Sec24" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.2 </span>Single-Object Localization</h3><p id="Par110" class="Para">The evaluation for single-object localization is similar to object classification, again using a top-5 criteria to allow the algorithm to return unannotated object classes without penalty. However, now the algorithm is considered correct only if it both correctly identifies the target class <span class="InlineEquation" id="IEq61">\(C_i\)</span> and accurately localizes one of its instances. Figure&nbsp;<span class="InternalRef"><a href="#Fig7">7</a></span> (middle row) shows some examples.</p><div id="Par111" class="Para">Concretely, an image is associated with object class <span class="InlineEquation" id="IEq62">\(C_i\)</span>, with all instances of this object class annotated with bounding boxes <span class="InlineEquation" id="IEq63">\(B_{ik}\)</span>. An algorithm returns <span class="InlineEquation" id="IEq64">\(\{(c_{ij},b_{ij})\}_{j=1}^5\)</span> of class labels <span class="InlineEquation" id="IEq65">\(c_{ij}\)</span> and associated locations <span class="InlineEquation" id="IEq66">\(b_{ij}\)</span>. The error of a prediction <span class="InlineEquation" id="IEq67">\(j\)</span> is:<div id="Equ2" class="Equation EquationMathjax"><div class="EquationContent">$$\begin{aligned} d_{ij} = \max (d(c_{ij},C_i),\min _{k}d(b_{ij},B_{ik})) \end{aligned}$$</div> <div class="EquationNumber">(2)</div></div>Here <span class="InlineEquation" id="IEq68">\(d(b_{ij},B_{ik})\)</span> is the error of localization, defined as <span class="InlineEquation" id="IEq69">\(0\)</span> if the area of intersection of boxes <span class="InlineEquation" id="IEq70">\(b_{ij}\)</span> and <span class="InlineEquation" id="IEq71">\(B_{ik}\)</span> divided by the areas of their union is greater than <span class="InlineEquation" id="IEq72">\(0.5\)</span>, and <span class="InlineEquation" id="IEq73">\(1\)</span> otherwise (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR19">2010</a></span>). The error of an algorithm is computed as in Eq.&nbsp;<span class="InternalRef"><a href="#Equ1">1</a></span>.</div><div id="Par112" class="Para">Evaluating localization is inherently difficult in some images. Consider a picture of a bunch of bananas or a carton of apples. It is easy to classify these images as containing bananas or apples, and even possible to localize a few instances of each fruit. However, in order for evaluation to be accurate <em class="EmphasisTypeItalic ">every</em> instance of banana or apple needs to be annotated, and that may be impossible. To handle the images where localizing individual object instances is inherently ambiguous we manually discarded 3.5&nbsp;% of images since ILSVRC2012. Some examples of discarded images are shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig8">8</a></span>.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO10"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig8_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig8_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8</span><p class="SimplePara">Images marked as “difficult” in the ILSVRC2012 single-object localization validation set. Please refer to Sect.&nbsp;<span class="InternalRef"><a href="#Sec24">4.2</a></span> for details</p></div></figcaption></figure></div></section><section id="Sec25" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.3 </span>Object Detection</h3><p id="Par113" class="Para">The criteria for object detection was adopted from PASCAL VOC (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR19">2010</a></span>). It is designed to penalize the algorithm for missing object instances, for duplicate detections of one instance, and for false positive detections. Figure&nbsp;<span class="InternalRef"><a href="#Fig7">7</a></span>(bottom row) shows examples.</p><div id="Par114" class="Para">For each object class and each image <span class="InlineEquation" id="IEq74">\(I_i\)</span>, an algorithm returns predicted detections <span class="InlineEquation" id="IEq75">\((b_{ij},s_{ij})\)</span> of predicted locations <span class="InlineEquation" id="IEq76">\(b_{ij}\)</span> with confidence scores <span class="InlineEquation" id="IEq77">\(s_{ij}\)</span>. These detections are greedily matched to the ground truth boxes <span class="InlineEquation" id="IEq78">\(\{B_{ik}\}\)</span> using Algorithm&nbsp;2. For every detection <span class="InlineEquation" id="IEq79">\(j\)</span> on image <span class="InlineEquation" id="IEq80">\(i\)</span> the algorithm returns <span class="InlineEquation" id="IEq81">\(z_{ij} = 1\)</span> if the detection is matched to a ground truth box according to the threshold criteria, and <span class="InlineEquation" id="IEq82">\(0\)</span> otherwise. For a given object class, let <span class="InlineEquation" id="IEq83">\(N\)</span> be the total number of ground truth instances across all images. Given a threshold <span class="InlineEquation" id="IEq84">\(t\)</span>, define <em class="EmphasisTypeItalic "> recall</em> as the fraction of the <span class="InlineEquation" id="IEq85">\(N\)</span> objects detected by the algorithm, and <em class="EmphasisTypeItalic ">precision</em> as the fraction of correct detections out of the total detections returned by the algorithm. Concretely,<div id="Equ3" class="Equation EquationMathjax"><div class="EquationContent">$$\begin{aligned}&amp;Recall(t) = \frac{\sum _{ij} 1[s_{ij} \ge t] z_{ij} }{N} \end{aligned}$$</div> <div class="EquationNumber">(3)</div></div><div id="Equ4" class="Equation EquationMathjax"><div class="EquationContent">$$\begin{aligned}&amp;Precision(t) = \frac{\sum _{ij} 1[s_{ij} \ge t] z_{ij} }{\sum _{ij} 1[s_{ij} \ge t]} \end{aligned}$$</div> <div class="EquationNumber">(4)</div></div><figure class="Figure" id="Figb"><div class="MediaObject" id="MO14"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Figb_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Figb_HTML.gif" alt=""></a></div></figure></div><p id="Par115" class="Para">The final metric for evaluating an algorithm on a given object class is <em class="EmphasisTypeItalic ">average precision</em> over the different levels of recall achieved by varying the threshold <span class="InlineEquation" id="IEq102">\(t\)</span>. The winner of each object class is then the team with the highest average precision, and then winner of the challenge is the team that wins on the most object classes.<sup><a href="#Fn8" id="Fn8_source">8</a></sup></p><p id="Par117" class="Para"><em class="EmphasisTypeItalic ">Difference with PASCAL VOC</em> Evaluating localization of object instances which occupy very few pixels in the image is challenging. The PASCAL VOC approach was to label such instances as “difficult” and ignore them during evaluation. However, since ILSVRC contains a more diverse set of object classes including, for example, “nail” and “ping pong ball” which have many very small instances, it is important to include even very small object instances in evaluation.</p><div id="Par118" class="Para">In Algorithm&nbsp;2, a predicted bounding box <span class="InlineEquation" id="IEq103">\(b\)</span> is considered to have properly localized by a ground truth bounding box <span class="InlineEquation" id="IEq104">\(B\)</span> if <span class="InlineEquation" id="IEq105">\(IOU(b,B) \ge \text{ thr }(B)\)</span>. The PASCAL VOC metric uses the threshold <span class="InlineEquation" id="IEq106">\(\text{ thr }(B) = 0.5\)</span>. However, for small objects even deviations of a few pixels would be unacceptable according to this threshold. For example, consider an object <span class="InlineEquation" id="IEq107">\(B\)</span> of size <span class="InlineEquation" id="IEq108">\(10 \times 10\)</span> pixels, with a detection window of <span class="InlineEquation" id="IEq109">\(20 \times 20\)</span> pixels which fully contains that object. This would be an error of approximately <span class="InlineEquation" id="IEq110">\(5\)</span> pixels on each dimension, which is average human annotation error. However, the IOU in this case would be <span class="InlineEquation" id="IEq111">\(100/400 = 0.25\)</span>, far below the threshold of <span class="InlineEquation" id="IEq112">\(0.5\)</span>. Thus for smaller objects we loosen the threshold in ILSVRC to allow for the annotation to extend up to 5 pixels on average in each direction around the object. Concretely, if the ground truth box <span class="InlineEquation" id="IEq113">\(B\)</span> is of dimensions <span class="InlineEquation" id="IEq114">\(w \times h\)</span> then<div id="Equ5" class="Equation EquationMathjax"><div class="EquationContent">$$\begin{aligned} \text{ thr }(B) = \min \left( 0.5, \frac{w h}{(w+10)(h+10)} \right) \end{aligned}$$</div> <div class="EquationNumber">(5)</div></div>In practice, this changes the threshold only on objects which are smaller than approximately <span class="InlineEquation" id="IEq115">\(25\times 25\)</span> pixels, and affects 5.5&nbsp;% of objects in the detection validation set.</div><p id="Par119" class="Para"><em class="EmphasisTypeItalic ">Practical Consideration</em> One additional practical consideration for ILSVRC detection evaluation is subtle and comes directly as a result of the scale of ILSVRC. In PASCAL, algorithms would often return many detections per class on the test set, including ones with low confidence scores. This allowed the algorithms to reach the level of high recall at least in the realm of very low precision. On ILSVRC detection test set if an algorithm returns 10 bounding boxes per object per image this would result in <span class="InlineEquation" id="IEq116">\(10 \times 200 \times 40K = 80\)</span>M detections. Each detection contains an image index, a class index, 4 bounding box coordinates, and the confidence score, so it takes on the order of 28 bytes. The full set of detections would then require <span class="InlineEquation" id="IEq117">\(2.24\)</span>Gb to store and submit to the evaluation server, which is impractical. This means that algorithms are implicitly required to limit their predictions to only the most confident locations.</p></section></div></section><section id="Sec26" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">5 </span>Methods</h2><div class="content"><p id="Par120" class="Para">The ILSVRC dataset and the competition has allowed significant algorithmic advances in large-scale image recognition and retrieval.</p><section id="Sec27" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.1 </span>Challenge Entries</h3><div id="Par121" class="Para">This section is organized chronologically, highlighting the particularly innovative and successful methods which participated in the ILSVRC each year. Tables&nbsp;<span class="InternalRef"><a href="#Tab6">6</a></span>,&nbsp;<span class="InternalRef"><a href="#Tab7">7</a></span> and&nbsp;<span class="InternalRef"><a href="#Tab8">8</a></span> list all the participating teams. We see a turning point in 2012 with the development of large-scale convolutional neural networks.<div id="Tab6" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6</span><p class="SimplePara">Teams participating in ILSVRC2010-2012, ordered alphabetically</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"><col class="tcol5 align-left"></colgroup><thead><tr><th><p class="SimplePara">Codename</p></th><th><p class="SimplePara">CLS</p></th><th><p class="SimplePara">LOC</p></th><th><p class="SimplePara">Insitutions</p></th><th><p class="SimplePara">Contributors and references</p></th></tr></thead><tbody><tr><td colspan="5"><p class="SimplePara">ILSVRC 2010</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Hminmax</p></td><td><p class="SimplePara">54.4</p></td><td rowspan="10">&nbsp;</td><td><p class="SimplePara">Massachusetts Institute of Technology</p></td><td><p class="SimplePara">Jim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;IBM</p></td><td><p class="SimplePara">70.1</p></td><td><p class="SimplePara">IBM research<span class="InlineEquation" id="IEq118">\(^\dagger \)</span>, Georgia Tech<span class="InlineEquation" id="IEq119">\(^\ddag \)</span></p></td><td><p class="SimplePara">Lexing Xie<span class="InlineEquation" id="IEq120">\(^\dagger \)</span>, Hua Ouyang<span class="InlineEquation" id="IEq121">\(^\ddag \)</span>, Apostol Natsev<span class="InlineEquation" id="IEq122">\(^\dagger \)</span></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ISIL</p></td><td><p class="SimplePara">44.6</p></td><td><p class="SimplePara">Intelligent Systems and Informatics Lab., The University of Tokyo</p></td><td><p class="SimplePara">Tatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ITNLP</p></td><td><p class="SimplePara">78.7</p></td><td><p class="SimplePara">Harbin Institute of Technology</p></td><td><p class="SimplePara">Deyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;LIG</p></td><td><p class="SimplePara">60.7</p></td><td><p class="SimplePara">Laboratoire d’Informatique de Grenoble</p></td><td><p class="SimplePara">Georges Quénot</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;NEC</p></td><td><p class="SimplePara"> 28.2</p></td><td><p class="SimplePara">NEC Labs America<span class="InlineEquation" id="IEq123">\(^\dagger \)</span>, University of Illinois at Urbana-Champaign<span class="InlineEquation" id="IEq124">\(^\ddag \)</span>, Rutgers<span class="InlineEquation" id="IEq125">\(^\mp \)</span></p></td><td><p class="SimplePara">Yuanqing Lin<span class="InlineEquation" id="IEq126">\(^\dagger \)</span>, Fengjun Lv<span class="InlineEquation" id="IEq127">\(^\dagger \)</span>, Shenghuo Zhu<span class="InlineEquation" id="IEq128">\(^\dagger \)</span>, Ming Yang<span class="InlineEquation" id="IEq129">\(^\dagger \)</span>, Timothee Cour<span class="InlineEquation" id="IEq130">\(^\dagger \)</span>, Kai Yu<span class="InlineEquation" id="IEq131">\(^\dagger \)</span>, LiangLiang Cao<span class="InlineEquation" id="IEq132">\(^\ddag \)</span>, Zhen Li<span class="InlineEquation" id="IEq133">\(^\ddag \)</span>, Min-Hsuan Tsai<span class="InlineEquation" id="IEq134">\(^\ddag \)</span>, Xi Zhou<span class="InlineEquation" id="IEq135">\(^\ddag \)</span>, Thomas Huang<span class="InlineEquation" id="IEq136">\(^\ddag \)</span>, Tong Zhang<span class="InlineEquation" id="IEq137">\(^\mp \)</span>(Lin et&nbsp;al. <span class="CitationRef"><a href="#CR47">2011</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;NII</p></td><td><p class="SimplePara">74.2</p></td><td><p class="SimplePara">National Institute of Informatics, Tokyo,Japan<span class="InlineEquation" id="IEq138">\(^\dagger \)</span>, Hefei Normal Univ. Heifei, China<span class="InlineEquation" id="IEq139">\(^\ddag \)</span></p></td><td><p class="SimplePara">Cai-Zhi Zhu<span class="InlineEquation" id="IEq140">\(^\dagger \)</span>, Xiao Zhou<span class="InlineEquation" id="IEq141">\(^\ddag \)</span>, Shiníchi Satoh<span class="InlineEquation" id="IEq142">\(^\dagger \)</span></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;NTU</p></td><td><p class="SimplePara">58.3</p></td><td><p class="SimplePara">CeMNet, SCE, NTU, Singapore</p></td><td><p class="SimplePara">Zhengxiang Wang, Liang-Tien Chia</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;UCI</p></td><td><p class="SimplePara">46.6</p></td><td><p class="SimplePara">University of California Irvine</p></td><td><p class="SimplePara">Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;XRCE</p></td><td><p class="SimplePara">33.6</p></td><td><p class="SimplePara">Xerox Research Centre Europe</p></td><td><p class="SimplePara">Jorge Sanchez, Florent Perronnin, Thomas Mensink (Perronnin et&nbsp;al. <span class="CitationRef"><a href="#CR64">2010</a></span>)</p></td></tr><tr><td colspan="5"><p class="SimplePara">ILSVRC 2011</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ISI</p></td><td><p class="SimplePara">36.0</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Intelligent Systems and Informatics lab, University of Tokyo</p></td><td><p class="SimplePara">Tatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo Kuniyoshi</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;NII</p></td><td><p class="SimplePara">50.5</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">National Institute of Informatics, Japan</p></td><td><p class="SimplePara">Duy-Dinh Le, Shiníchi Satoh</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;UvA</p></td><td><p class="SimplePara">31.0</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">42.5</strong></p></td><td><p class="SimplePara">University of Amsterdam<span class="InlineEquation" id="IEq143">\(^\dagger \)</span>, University of Trento<span class="InlineEquation" id="IEq144">\(^\ddag \)</span></p></td><td><p class="SimplePara">Koen E. A. van de Sande<span class="InlineEquation" id="IEq145">\(^\dagger \)</span>, Jasper R. R. Uijlings<span class="InlineEquation" id="IEq146">\(^\ddag \)</span>, Arnold W. M. Smeulders<span class="InlineEquation" id="IEq147">\(^\dagger \)</span>, Theo Gevers<span class="InlineEquation" id="IEq148">\(^\dagger \)</span>, Nicu Sebe<span class="InlineEquation" id="IEq149">\(^\ddag \)</span>, Cees Snoek<span class="InlineEquation" id="IEq150">\(^\dagger \)</span>(van de Sande et&nbsp;al. <span class="CitationRef"><a href="#CR85">2011b</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;XRCE</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">25.8</strong></p></td><td><p class="SimplePara">56.5</p></td><td><p class="SimplePara">Xerox Research Centre Europe<span class="InlineEquation" id="IEq151">\(^\dagger \)</span>, CIII<span class="InlineEquation" id="IEq152">\(^\ddag \)</span></p></td><td><p class="SimplePara">Florent Perronnin<span class="InlineEquation" id="IEq153">\(^\dagger \)</span>, Jorge Sanchez<span class="InlineEquation" id="IEq154">\(^\dagger \)</span><span class="InlineEquation" id="IEq155">\(^\ddag \)</span>(Sanchez and Perronnin <span class="CitationRef"><a href="#CR67">2011</a></span>)</p></td></tr><tr><td colspan="5"><p class="SimplePara">ILSVRC 2012</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ISI</p></td><td><p class="SimplePara">26.2</p></td><td><p class="SimplePara">53.6</p></td><td><p class="SimplePara">University of Tokyo<span class="InlineEquation" id="IEq156">\(^\dagger \)</span>, JST PRESTO<span class="InlineEquation" id="IEq157">\(^\ddag \)</span></p></td><td><p class="SimplePara">Naoyuki Gunji<span class="InlineEquation" id="IEq158">\(^\dagger \)</span>, Takayuki Higuchi<span class="InlineEquation" id="IEq159">\(^\dagger \)</span>, Koki Yasumoto<span class="InlineEquation" id="IEq160">\(^\dagger \)</span>, Hiroshi Muraoka<span class="InlineEquation" id="IEq161">\(^\dagger \)</span>, Yoshitaka Ushiku<span class="InlineEquation" id="IEq162">\(^\dagger \)</span>, Tatsuya Harada<span class="InlineEquation" id="IEq163">\(^\dagger \)</span><span class="InlineEquation" id="IEq164">\(^\ddag \)</span>, Yasuo Kuniyoshi<span class="InlineEquation" id="IEq165">\(^\dagger \)</span>(Harada and Kuniyoshi <span class="CitationRef"><a href="#CR31">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;LEAR</p></td><td><p class="SimplePara">34.5</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">LEAR INRIA Grenoble<span class="InlineEquation" id="IEq166">\(^\dagger \)</span>, TVPA Xerox Research Centre Europe<span class="InlineEquation" id="IEq167">\(^\ddag \)</span></p></td><td><p class="SimplePara">Thomas Mensink<span class="InlineEquation" id="IEq168">\(^\dagger \)</span><span class="InlineEquation" id="IEq169">\(^\ddag \)</span>, Jakob Verbeek<span class="InlineEquation" id="IEq170">\(^\dagger \)</span>, Florent Perronnin<span class="InlineEquation" id="IEq171">\(^\ddag \)</span>, Gabriela Csurka<span class="InlineEquation" id="IEq172">\(^\ddag \)</span>(Mensink et&nbsp;al. <span class="CitationRef"><a href="#CR53">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;VGG</p></td><td><p class="SimplePara">27.0</p></td><td><p class="SimplePara">50.0</p></td><td><p class="SimplePara">University of Oxford</p></td><td><p class="SimplePara">Karen Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman (Arandjelovic and Zisserman <span class="CitationRef"><a href="#CR3">2012</a></span>; Sanchez et&nbsp;al. <span class="CitationRef"><a href="#CR68">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;SuperVision</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">16.4</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">34.2</strong></p></td><td><p class="SimplePara">University of Toronto</p></td><td><p class="SimplePara">Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;UvA</p></td><td><p class="SimplePara">29.6</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">University of Amsterdam</p></td><td><p class="SimplePara">Koen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek (Sanchez and Perronnin <span class="CitationRef"><a href="#CR67">2011</a></span>; Scheirer et&nbsp;al. <span class="CitationRef"><a href="#CR69">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;XRCE</p></td><td><p class="SimplePara">27.1</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Xerox Research Centre Europe<span class="InlineEquation" id="IEq173">\(^\dagger \)</span>, LEAR INRIA <span class="InlineEquation" id="IEq174">\(^\ddag \)</span></p></td><td><p class="SimplePara">Florent Perronnin<span class="InlineEquation" id="IEq175">\(^\dagger \)</span>, Zeynep Akata<span class="InlineEquation" id="IEq176">\(^\dagger \)</span><span class="InlineEquation" id="IEq177">\(^\ddag \)</span>, Zaid Harchaoui<span class="InlineEquation" id="IEq178">\(^\ddag \)</span>, Cordelia Schmid<span class="InlineEquation" id="IEq179">\(^\ddag \)</span>(Perronnin et&nbsp;al. <span class="CitationRef"><a href="#CR63">2012</a></span>)</p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara">Each method is identified with a codename used in the text. We report flat top-5 classification and single-object localization error, in percents (lower is better). For teams which submitted multiple entries we report the best score. In 2012, SuperVision also submitted entries trained with the extra data from the ImageNet Fall 2011 release, and obtained 15.3&nbsp;% classification error and 33.5&nbsp;% localization error. Key references are provided where available. More details about the winning entries can be found in Sect.&nbsp;<span class="InternalRef"><a href="#Sec27">5.1</a></span></p></div></div><div id="Tab7" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 7</span><p class="SimplePara">Teams participating in ILSVRC2013, ordered alphabetically</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"><col class="tcol5 align-left"><col class="tcol6 align-left"></colgroup><thead><tr><th><p class="SimplePara">Codename</p></th><th><p class="SimplePara">CLS</p></th><th><p class="SimplePara">LOC</p></th><th><p class="SimplePara">DET</p></th><th><p class="SimplePara">Insitutions</p></th><th><p class="SimplePara">Contributors and references</p></th></tr></thead><tbody><tr><td colspan="6"><p class="SimplePara">ILSVRC 2013</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Adobe</p></td><td><p class="SimplePara">15.2</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Adobe<span class="InlineEquation" id="IEq180">\(^\dagger \)</span>, University of Illinois at Urbana-Champaign<span class="InlineEquation" id="IEq181">\(^\ddag \)</span></p></td><td><p class="SimplePara">Hailin Jin<span class="InlineEquation" id="IEq182">\(^\dagger \)</span>, Zhe Lin<span class="InlineEquation" id="IEq183">\(^\dagger \)</span>, Jianchao Yang<span class="InlineEquation" id="IEq184">\(^\dagger \)</span>, Tom Paine<span class="InlineEquation" id="IEq185">\(^\ddag \)</span>(Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;AHoward</p></td><td><p class="SimplePara">13.6</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Andrew Howard Consulting</p></td><td><p class="SimplePara">Andrew Howard</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;BUPT</p></td><td><p class="SimplePara">25.2</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Beijing University of Posts and Telecommunications<span class="InlineEquation" id="IEq186">\(^\dagger \)</span>, Orange Labs International Center Beijing<span class="InlineEquation" id="IEq187">\(^\ddag \)</span></p></td><td><p class="SimplePara">Chong Huang<span class="InlineEquation" id="IEq188">\(^\dagger \)</span>, Yunlong Bian<span class="InlineEquation" id="IEq189">\(^\dagger \)</span>, Hongliang Bai<span class="InlineEquation" id="IEq190">\(^\ddag \)</span>, Bo Liu<span class="InlineEquation" id="IEq191">\(^\dagger \)</span>, Yanchao Feng<span class="InlineEquation" id="IEq192">\(^\dagger \)</span>, Yuan Dong<span class="InlineEquation" id="IEq193">\(^\dagger \)</span></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Clarifai</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">11.7</strong></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Clarifai</p></td><td><p class="SimplePara">Matthew Zeiler (Zeiler and Fergus <span class="CitationRef"><a href="#CR99">2013</a></span>; Zeiler et&nbsp;al. <span class="CitationRef"><a href="#CR100">2011</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;CogVision</p></td><td><p class="SimplePara">16.1</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Microsoft Research<span class="InlineEquation" id="IEq194">\(^\dagger \)</span>, Harbin Institute of Technology<span class="InlineEquation" id="IEq195">\(^\ddag \)</span></p></td><td><p class="SimplePara">Kuiyuan Yang<span class="InlineEquation" id="IEq196">\(^\dagger \)</span>, Yalong Bai<span class="InlineEquation" id="IEq197">\(^\dagger \)</span>, Yong Rui<span class="InlineEquation" id="IEq198">\(^\ddag \)</span></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;decaf</p></td><td><p class="SimplePara">19.2</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">University of California Berkeley</p></td><td><p class="SimplePara">Yangqing Jia, Jeff Donahue, Trevor Darrell (Donahue et&nbsp;al. <span class="CitationRef"><a href="#CR16">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Deep Punx</p></td><td><p class="SimplePara">20.9</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Saint Petersburg State University</p></td><td><p class="SimplePara">Evgeny Smirnov, Denis Timoshenko, Alexey Korolev (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>; Wan et&nbsp;al. <span class="CitationRef"><a href="#CR91">2013</a></span>; Tang <span class="CitationRef"><a href="#CR78">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Delta</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">6.1</p></td><td><p class="SimplePara">National Tsing Hua University</p></td><td><p class="SimplePara">Che-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, Hao-Che Kao</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;IBM</p></td><td><p class="SimplePara">20.7</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">University of Illinois at Urbana-Champaign<span class="InlineEquation" id="IEq199">\(^\dagger \)</span>, IBM Watson Research Center<span class="InlineEquation" id="IEq200">\(^\ddag \)</span>, IBM Haifa Research Center<span class="InlineEquation" id="IEq201">\(^\mp \)</span></p></td><td><p class="SimplePara">Zhicheng Yan<span class="InlineEquation" id="IEq202">\(^\dagger \)</span>, Liangliang Cao<span class="InlineEquation" id="IEq203">\(^\ddag \)</span>, John R Smith<span class="InlineEquation" id="IEq204">\(^\ddag \)</span>, Noel Codella<span class="InlineEquation" id="IEq205">\(^\ddag \)</span>,Michele Merler<span class="InlineEquation" id="IEq206">\(^\ddag \)</span>, Sharath Pankanti<span class="InlineEquation" id="IEq207">\(^\ddag \)</span>, Sharon Alpert<span class="InlineEquation" id="IEq208">\(^\mp \)</span>, Yochay Tzur<span class="InlineEquation" id="IEq209">\(^\mp \)</span>,</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;MIL</p></td><td><p class="SimplePara">24.4</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">University of Tokyo</p></td><td><p class="SimplePara">Masatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tatsuya Harada</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Minerva</p></td><td><p class="SimplePara">21.7</p></td><td>&nbsp;</td><td>&nbsp;</td><td><p class="SimplePara">Peking University<span class="InlineEquation" id="IEq210">\(^\dagger \)</span>, Microsoft Research<span class="InlineEquation" id="IEq211">\(^\ddag \)</span>, Shanghai Jiao Tong University<span class="InlineEquation" id="IEq212">\(^\mp \)</span>, XiDian University<sup>§</sup>, Harbin Institute of Technology<span class="InlineEquation" id="IEq213">\(^\varsigma \)</span></p></td><td><p class="SimplePara">Tianjun Xiao<span class="InlineEquation" id="IEq214">\(^\dagger \)</span><span class="InlineEquation" id="IEq215">\(^\ddag \)</span>, Minjie Wang<span class="InlineEquation" id="IEq216">\(^\mp \)</span><span class="InlineEquation" id="IEq217">\(^\ddag \)</span>, Jianpeng Li<sup>§</sup><span class="InlineEquation" id="IEq218">\(^\ddag \)</span>, Yalong Bai<span class="InlineEquation" id="IEq219">\(^\varsigma \)</span><span class="InlineEquation" id="IEq220">\(^\ddag \)</span>, Jiaxing Zhang<span class="InlineEquation" id="IEq221">\(^\ddag \)</span>, Kuiyuan Yang<span class="InlineEquation" id="IEq222">\(^\ddag \)</span>, Chuntao Hong<span class="InlineEquation" id="IEq223">\(^\ddag \)</span>, Zheng Zhang<span class="InlineEquation" id="IEq224">\(^\ddag \)</span>(Wang et&nbsp;al. <span class="CitationRef"><a href="#CR92">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;NEC</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">19.6</p></td><td><p class="SimplePara">NEC Labs America<span class="InlineEquation" id="IEq225">\(^\dagger \)</span>, University of Missouri <span class="InlineEquation" id="IEq226">\(^\ddag \)</span></p></td><td><p class="SimplePara">Xiaoyu Wang<span class="InlineEquation" id="IEq227">\(^\dagger \)</span>, Miao Sun<span class="InlineEquation" id="IEq228">\(^\ddag \)</span>, Tianbao Yang<span class="InlineEquation" id="IEq229">\(^\dagger \)</span>, Yuanqing Lin<span class="InlineEquation" id="IEq230">\(^\dagger \)</span>, Tony X. Han<span class="InlineEquation" id="IEq231">\(^\ddag \)</span>, Shenghuo Zhu<span class="InlineEquation" id="IEq232">\(^\dagger \)</span>(Wang et&nbsp;al. <span class="CitationRef"><a href="#CR94">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;NUS</p></td><td><p class="SimplePara">13.0</p></td><td>&nbsp;</td><td>&nbsp;</td><td><p class="SimplePara">National University of Singapore</p></td><td><p class="SimplePara">Min Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal contribution) (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Orange</p></td><td><p class="SimplePara">25.2</p></td><td>&nbsp;</td><td>&nbsp;</td><td><p class="SimplePara">Orange Labs International Center Beijing<span class="InlineEquation" id="IEq233">\(^\dagger \)</span>, Beijing University of Posts and Telecommunications<span class="InlineEquation" id="IEq234">\(^\ddag \)</span></p></td><td><p class="SimplePara">Hongliang BAI<span class="InlineEquation" id="IEq235">\(^\dagger \)</span>, Lezi Wang<span class="InlineEquation" id="IEq236">\(^\ddag \)</span>, Shusheng Cen<span class="InlineEquation" id="IEq237">\(^\ddag \)</span>, YiNan Liu<span class="InlineEquation" id="IEq238">\(^\ddag \)</span>, Kun Tao<span class="InlineEquation" id="IEq239">\(^\dagger \)</span>, Wei Liu<span class="InlineEquation" id="IEq240">\(^\dagger \)</span>, Peng Li<span class="InlineEquation" id="IEq241">\(^\dagger \)</span>, Yuan Dong<span class="InlineEquation" id="IEq242">\(^\dagger \)</span></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;OverFeat</p></td><td><p class="SimplePara">14.2</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">30.0</strong></p></td><td><p class="SimplePara">(19.4)</p></td><td><p class="SimplePara">New York University</p></td><td><p class="SimplePara">Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun (Sermanet et&nbsp;al. <span class="CitationRef"><a href="#CR71">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Quantum</p></td><td><p class="SimplePara">82.0</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Self-employed<span class="InlineEquation" id="IEq243">\(^\dagger \)</span>, Student in Troy High School, Fullerton, CA<span class="InlineEquation" id="IEq244">\(^\ddag \)</span></p></td><td><p class="SimplePara">Henry Shu<span class="InlineEquation" id="IEq245">\(^\dagger \)</span>, Jerry Shu<span class="InlineEquation" id="IEq246">\(^\ddag \)</span>(Batra et&nbsp;al. <span class="CitationRef"><a href="#CR6">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;SYSU</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">10.5</p></td><td><p class="SimplePara">Sun Yat-Sen University, China.</p></td><td><p class="SimplePara">Xiaolong Wang (Felzenszwalb et&nbsp;al. <span class="CitationRef"><a href="#CR23">2010</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Toronto</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">11.5</p></td><td><p class="SimplePara">University of Toronto</p></td><td><p class="SimplePara">Yichuan Tang*, Nitish Srivastava*, Ruslan Salakhutdinov (* = equal contribution)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Trimps</p></td><td><p class="SimplePara">26.2</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">The Third Research Institute of the Ministry of Public Security, P.R. China</p></td><td><p class="SimplePara">Jie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;UCLA</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">9.8</p></td><td><p class="SimplePara">University of California Los Angeles</p></td><td><p class="SimplePara">Yukun Zhu, Jun Zhu, Alan Yuille</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;UIUC</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">1.0</p></td><td><p class="SimplePara">University of Illinois at Urbana-Champaign</p></td><td><p class="SimplePara">Thomas Paine, Kevin Shih, Thomas Huang (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;UvA</p></td><td><p class="SimplePara">14.3</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">22.6</strong></p></td><td><p class="SimplePara">University of Amsterdam, Euvision Technologies</p></td><td><p class="SimplePara">Koen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman, Arnold W. M. Smeulders (van de Sande et&nbsp;al. <span class="CitationRef"><a href="#CR84">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;VGG</p></td><td><p class="SimplePara">15.2</p></td><td><p class="SimplePara">46.4</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Visual Geometry Group, University of Oxford</p></td><td><p class="SimplePara">Karen Simonyan, Andrea Vedaldi, Andrew Zisserman (Simonyan et&nbsp;al. <span class="CitationRef"><a href="#CR74">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;ZF</p></td><td><p class="SimplePara">13.5</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">New York University</p></td><td><p class="SimplePara">Matthew D Zeiler, Rob Fergus (Zeiler and Fergus <span class="CitationRef"><a href="#CR99">2013</a></span>; Zeiler et&nbsp;al. <span class="CitationRef"><a href="#CR100">2011</a></span>)</p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara">Each method is identified with a codename used in the text. For classificaton and single-object localization we report flat top-5 error, in percents (lower is better). For detection we report mean average precision, in percents (higher is better). Even though the winner of the challenge was determined by the number of object categories won, this correlated strongly with mAP. Parentheses indicate the team used outside training data and was not part of the official competition. Some competing teams also submitted entries trained with outside data: Clarifai with 11.2&nbsp;% classification error, NEC with 20.9&nbsp;% detection mAP. Key references are provided where available. More details about the winning entries can be found in Sect.&nbsp;<span class="InternalRef"><a href="#Sec27">5.1</a></span></p></div></div><div id="Tab8" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 8</span><p class="SimplePara">Teams participating in ILSVRC2014, ordered alphabetically</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"><col class="tcol5 align-left"><col class="tcol6 align-left"><col class="tcol7 align-left"><col class="tcol8 align-left"><col class="tcol9 align-left"></colgroup><thead><tr><th><p class="SimplePara">Codename</p></th><th><p class="SimplePara">CLS</p></th><th><p class="SimplePara">CLSo</p></th><th><p class="SimplePara">LOC</p></th><th><p class="SimplePara">LOCo</p></th><th><p class="SimplePara">DET</p></th><th><p class="SimplePara">DETo</p></th><th><p class="SimplePara">Insitutions</p></th><th><p class="SimplePara">Contributors and references</p></th></tr></thead><tbody><tr><td colspan="9"><p class="SimplePara">ILSVRC 2014</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Adobe</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">11.6</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">30.1</strong></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Adobe<span class="InlineEquation" id="IEq247">\(^\dagger \)</span>, UIUC<span class="InlineEquation" id="IEq248">\(^\ddag \)</span></p></td><td><p class="SimplePara">Hailin Jin<span class="InlineEquation" id="IEq249">\(^\dagger \)</span>, Zhaowen Wang<span class="InlineEquation" id="IEq250">\(^\ddag \)</span>, Jianchao Yang<span class="InlineEquation" id="IEq251">\(^\dagger \)</span>, Zhe Lin<span class="InlineEquation" id="IEq252">\(^\dagger \)</span></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;AHoward</p></td><td><p class="SimplePara">8.1</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq253">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Howard Vision Technologies</p></td><td><p class="SimplePara">Andrew Howard (Howard <span class="CitationRef"><a href="#CR36">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;BDC</p></td><td><p class="SimplePara">11.3</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq254">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Institute for Infocomm Research<span class="InlineEquation" id="IEq255">\(^\dagger \)</span>, Universit Pierre et Marie Curie<span class="InlineEquation" id="IEq256">\(^\ddag \)</span></p></td><td><p class="SimplePara">Olivier Morre<span class="InlineEquation" id="IEq257">\(^\dagger \)</span><span class="InlineEquation" id="IEq258">\(^\ddag \)</span>, Hanlin Goh<span class="InlineEquation" id="IEq259">\(^\dagger \)</span>, Antoine Veillard<span class="InlineEquation" id="IEq260">\(^\ddag \)</span>, Vijay Chandrasekhar<span class="InlineEquation" id="IEq261">\(^\dagger \)</span>(Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Berkeley</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">34.5</p></td><td><p class="SimplePara">UC Berkeley</p></td><td><p class="SimplePara">Ross Girshick, Jeff Donahue, Sergio Guadarrama, Trevor Darrell, Jitendra Malik (Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR26">2013</a></span>, <span class="CitationRef"><a href="#CR27">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;BREIL</p></td><td><p class="SimplePara">16.0</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq262">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">KAIST department of EE</p></td><td><p class="SimplePara">Jun-Cheol Park, Yunhun Jang, Hyungwon Choi, JaeYoung Jun (Chatfield et&nbsp;al. <span class="CitationRef"><a href="#CR9">2014</a></span>; Jia <span class="CitationRef"><a href="#CR39">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Brno</p></td><td><p class="SimplePara">17.6</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">52.0</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Brno University of Technology</p></td><td><p class="SimplePara">Martin Kolář, Michal Hradiš, Pavel Svoboda (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>; Mikolov et&nbsp;al. <span class="CitationRef"><a href="#CR54">2013</a></span>; Jia <span class="CitationRef"><a href="#CR39">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;CASIA-2</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">28.6</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Chinese Academy of Science<span class="InlineEquation" id="IEq263">\(^\dagger \)</span>, Southeast University<span class="InlineEquation" id="IEq264">\(^\ddag \)</span></p></td><td><p class="SimplePara">Peihao Huang<span class="InlineEquation" id="IEq265">\(^\dagger \)</span>, Yongzhen Huang<span class="InlineEquation" id="IEq266">\(^\dagger \)</span>, Feng Liu<span class="InlineEquation" id="IEq267">\(^\ddag \)</span>, Zifeng Wu<span class="InlineEquation" id="IEq268">\(^\dagger \)</span>, Fang Zhao<span class="InlineEquation" id="IEq269">\(^\dagger \)</span>, Liang Wang<span class="InlineEquation" id="IEq270">\(^\dagger \)</span>, Tieniu Tan<span class="InlineEquation" id="IEq271">\(^\dagger \)</span>(Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR27">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;CASIAWS</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">11.4</strong></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq272">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">CRIPAC, CASIA</p></td><td><p class="SimplePara">Weiqiang Ren, Chong Wang, Yanhua Chen, Kaiqi Huang, Tieniu Tan (Arbeláez et&nbsp;al. <span class="CitationRef"><a href="#CR4">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Cldi</p></td><td><p class="SimplePara">13.9</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">46.9</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">KAIST<span class="InlineEquation" id="IEq273">\(^\dagger \)</span>, Cldi Inc.<span class="InlineEquation" id="IEq274">\(^\ddag \)</span></p></td><td><p class="SimplePara">Kyunghyun Paeng<span class="InlineEquation" id="IEq275">\(^\dagger \)</span>, Donggeun Yoo<span class="InlineEquation" id="IEq276">\(^\dagger \)</span>, Sunggyun Park<span class="InlineEquation" id="IEq277">\(^\dagger \)</span>, Jungin Lee<span class="InlineEquation" id="IEq278">\(^\ddag \)</span>, Anthony S. Paek<span class="InlineEquation" id="IEq279">\(^\ddag \)</span>, In So Kweon<span class="InlineEquation" id="IEq280">\(^\dagger \)</span>, Seong Dae Kim<span class="InlineEquation" id="IEq281">\(^\dagger \)</span>(Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>; Perronnin et&nbsp;al. <span class="CitationRef"><a href="#CR64">2010</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;CUHK</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">40.7</p></td><td><p class="SimplePara">The Chinese University of Hong Kong</p></td><td><p class="SimplePara">Wanli Ouyang, Ping Luo, Xingyu Zeng, Shi Qiu, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Yuanjun Xiong, Chen Qian, Zhenyao Zhu, Ruohui Wang, Chen-Change Loy, Xiaogang Wang, Xiaoou Tang (Ouyang et&nbsp;al. <span class="CitationRef"><a href="#CR59">2014</a></span>; Ouyang and Wang <span class="CitationRef"><a href="#CR58">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;DeepCNet</p></td><td><p class="SimplePara">17.5</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq282">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">University of Warwick</p></td><td><p class="SimplePara">Ben Graham (Graham <span class="CitationRef"><a href="#CR29">2013</a></span>; Schmidhuber <span class="CitationRef"><a href="#CR70">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;DeepInsight</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">40.5</p></td><td><p class="SimplePara">NLPR<span class="InlineEquation" id="IEq283">\(^\dagger \)</span>, HKUST<span class="InlineEquation" id="IEq284">\(^\ddag \)</span></p></td><td><p class="SimplePara">Junjie Yan<span class="InlineEquation" id="IEq285">\(^\dagger \)</span>, Naiyan Wang<span class="InlineEquation" id="IEq286">\(^\ddag \)</span>, Stan Z. Li<span class="InlineEquation" id="IEq287">\(^\dagger \)</span>, Dit-Yan Yeung<span class="InlineEquation" id="IEq288">\(^\ddag \)</span>(Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR27">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;FengjunLv</p></td><td><p class="SimplePara">17.4</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq289">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Fengjun Lv Consulting</p></td><td><p class="SimplePara">Fengjun Lv (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>; Harel et&nbsp;al. <span class="CitationRef"><a href="#CR32">2007</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;GoogLeNet</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">6.7</strong></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">26.4</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">43.9</strong></p></td><td><p class="SimplePara">Google</p></td><td><p class="SimplePara">Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Drago Anguelov, Dumitru Erhan, Andrew Rabinovich (Szegedy et&nbsp;al. <span class="CitationRef"><a href="#CR77">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;HKUST</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">28.9</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Hong Kong U. of Science and Tech.<span class="InlineEquation" id="IEq290">\(^\dagger \)</span>, Chinese U. of H. K.<span class="InlineEquation" id="IEq291">\(^\ddag \)</span>, Stanford U.<span class="InlineEquation" id="IEq292">\(^\mp \)</span></p></td><td><p class="SimplePara">Cewu Lu<span class="InlineEquation" id="IEq293">\(^\dagger \)</span>, Hei Law*<span class="InlineEquation" id="IEq294">\(^\dagger \)</span>, Hao Chen*<span class="InlineEquation" id="IEq295">\(^\ddag \)</span>, Qifeng Chen*<span class="InlineEquation" id="IEq296">\(^\mp \)</span>, Yao Xiao*<span class="InlineEquation" id="IEq297">\(^\dagger \)</span>Chi Keung Tang<span class="InlineEquation" id="IEq298">\(^\dagger \)</span>(Uijlings et&nbsp;al. <span class="CitationRef"><a href="#CR82">2013</a></span>; Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR26">2013</a></span>; Perronnin et&nbsp;al. <span class="CitationRef"><a href="#CR64">2010</a></span>; Felzenszwalb et&nbsp;al. <span class="CitationRef"><a href="#CR23">2010</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;libccv</p></td><td><p class="SimplePara">16.0</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq299">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">libccv.org</p></td><td><p class="SimplePara">Liu Liu (Zeiler and Fergus <span class="CitationRef"><a href="#CR99">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;MIL</p></td><td><p class="SimplePara">18.3</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">33.7</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">30.4</p></td><td><p class="SimplePara">The University of Tokyo<span class="InlineEquation" id="IEq300">\(^\dagger \)</span>, IIT Guwahati<span class="InlineEquation" id="IEq301">\(^\ddag \)</span></p></td><td><p class="SimplePara">Senthil Purushwalkam<span class="InlineEquation" id="IEq302">\(^\dagger \)</span><span class="InlineEquation" id="IEq303">\(^\ddag \)</span>, Yuichiro Tsuchiya<span class="InlineEquation" id="IEq304">\(^\dagger \)</span>, Atsushi Kanehira<span class="InlineEquation" id="IEq305">\(^\dagger \)</span>, Asako Kanezaki<span class="InlineEquation" id="IEq306">\(^\dagger \)</span>, Tatsuya Harada<span class="InlineEquation" id="IEq307">\(^\dagger \)</span>(Kanezaki et&nbsp;al. <span class="CitationRef"><a href="#CR41">2014</a></span>; Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR26">2013</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;MPG_UT</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">26.4</p></td><td><p class="SimplePara">The University of Tokyo</p></td><td><p class="SimplePara">Riku Togashi, Keita Iwamoto, Tomoaki Iwase, Hideki Nakayama (Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR27">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;MSRA</p></td><td><p class="SimplePara">8.1</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">35.5</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">35.1</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Microsoft Research<span class="InlineEquation" id="IEq308">\(^\dagger \)</span>, Xi’an Jiaotong U.<span class="InlineEquation" id="IEq309">\(^\ddag \)</span>, U. of Science and Tech. of China<span class="InlineEquation" id="IEq310">\(^\mp \)</span></p></td><td><p class="SimplePara">Kaiming He<span class="InlineEquation" id="IEq311">\(^\dagger \)</span>, Xiangyu Zhang<span class="InlineEquation" id="IEq312">\(^\ddag \)</span>, Shaoqing Ren<span class="InlineEquation" id="IEq313">\(^\mp \)</span>, Jian Sun<span class="InlineEquation" id="IEq314">\(^\dagger \)</span>(He et&nbsp;al. <span class="CitationRef"><a href="#CR33">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;NUS</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">37.2</strong></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">National University of Singapore<span class="InlineEquation" id="IEq315">\(^\dagger \)</span>, IBM Research Australia<span class="InlineEquation" id="IEq316">\(^\ddag \)</span></p></td><td><p class="SimplePara">Jian Dong<span class="InlineEquation" id="IEq317">\(^\dagger \)</span>, Yunchao Wei<span class="InlineEquation" id="IEq318">\(^\dagger \)</span>, Min Lin<span class="InlineEquation" id="IEq319">\(^\dagger \)</span>, Qiang Chen<span class="InlineEquation" id="IEq320">\(^\ddag \)</span>, Wei Xia<span class="InlineEquation" id="IEq321">\(^\dagger \)</span>, Shuicheng Yan<span class="InlineEquation" id="IEq322">\(^\dagger \)</span>(Lin et&nbsp;al. <span class="CitationRef"><a href="#CR46">2014a</a></span>; Chen et&nbsp;al. <span class="CitationRef"><a href="#CR10">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;NUS-BST</p></td><td><p class="SimplePara">9.8</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq323">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">National Univ. of Singapore<span class="InlineEquation" id="IEq324">\(^\dagger \)</span>, Beijing Samsung Telecom R&amp;D Center<span class="InlineEquation" id="IEq325">\(^\dagger \)</span></p></td><td><p class="SimplePara">Min Lin<span class="InlineEquation" id="IEq326">\(^\dagger \)</span>, Jian Dong<span class="InlineEquation" id="IEq327">\(^\dagger \)</span>, Hanjiang Lai<span class="InlineEquation" id="IEq328">\(^\dagger \)</span>, Junjun Xiong<span class="InlineEquation" id="IEq329">\(^\ddag \)</span>, Shuicheng Yan<span class="InlineEquation" id="IEq330">\(^\dagger \)</span>(Lin et&nbsp;al. <span class="CitationRef"><a href="#CR46">2014a</a></span>; Howard <span class="CitationRef"><a href="#CR36">2014</a></span>; Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Orange</p></td><td><p class="SimplePara">15.2</p></td><td><p class="SimplePara">14.8</p></td><td><p class="SimplePara">42.8</p></td><td><p class="SimplePara">42.7</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">27.7</p></td><td><p class="SimplePara">Orange Labs Beijing<span class="InlineEquation" id="IEq331">\(^\dagger \)</span>, BUPT China<span class="InlineEquation" id="IEq332">\(^\ddag \)</span></p></td><td><p class="SimplePara">Hongliang Bai<span class="InlineEquation" id="IEq333">\(^\dagger \)</span>, Yinan Liu<span class="InlineEquation" id="IEq334">\(^\dagger \)</span>, Bo Liu<span class="InlineEquation" id="IEq335">\(^\ddag \)</span>, Yanchao Feng<span class="InlineEquation" id="IEq336">\(^\ddag \)</span>, Kun Tao<span class="InlineEquation" id="IEq337">\(^\dagger \)</span>, Yuan Dong<span class="InlineEquation" id="IEq338">\(^\dagger \)</span>(Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR27">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;PassBy</p></td><td><p class="SimplePara">16.7</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq339">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">LENOVO<span class="InlineEquation" id="IEq340">\(^\dagger \)</span>, HKUST<span class="InlineEquation" id="IEq341">\(^\ddag \)</span>, U. of Macao<span class="InlineEquation" id="IEq342">\(^\mp \)</span></p></td><td><p class="SimplePara">Lin Sun<span class="InlineEquation" id="IEq343">\(^\dagger \)</span><span class="InlineEquation" id="IEq344">\(^\ddag \)</span>, Zhanghui Kuang<span class="InlineEquation" id="IEq345">\(^\dagger \)</span>, Cong Zhao<span class="InlineEquation" id="IEq346">\(^\dagger \)</span>, Kui Jia<span class="InlineEquation" id="IEq347">\(^\mp \)</span>, Oscar C.Au<span class="InlineEquation" id="IEq348">\(^\ddag \)</span>(Jia <span class="CitationRef"><a href="#CR39">2013</a></span>; Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;SCUT</p></td><td><p class="SimplePara">18.8</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq349">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">South China Univ. of Technology</p></td><td><p class="SimplePara">Guo Lihua, Liao Qijun, Ma Qianli, Lin Junbin</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Southeast</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">30.5</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Southeast U.<span class="InlineEquation" id="IEq350">\(^\dagger \)</span>, Chinese A. of Sciences<span class="InlineEquation" id="IEq351">\(^\ddag \)</span></p></td><td><p class="SimplePara">Feng Liu<span class="InlineEquation" id="IEq352">\(^\dagger \)</span>, Zifeng Wu<span class="InlineEquation" id="IEq353">\(^\ddag \)</span>, Yongzhen Huang<span class="InlineEquation" id="IEq354">\(^\ddag \)</span></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;SYSU</p></td><td><p class="SimplePara">14.4</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">31.9</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Sun Yat-Sen University</p></td><td><p class="SimplePara">Liliang Zhang, Tianshui Chen, Shuye Zhang, Wanglan He, Liang Lin, Dengguang Pang, Lingbo Liu</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;Trimps</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">11.5</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">42.2</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">33.7</p></td><td><p class="SimplePara">The Third Research Institute of the Ministry of Public Security</p></td><td><p class="SimplePara">Jie Shao, Xiaoteng Zhang, JianYing Zhou, Jian Wang, Jian Chen, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu (Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR27">2014</a></span>; Manen et&nbsp;al. <span class="CitationRef"><a href="#CR52">2013</a></span>; Howard <span class="CitationRef"><a href="#CR36">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;TTIC</p></td><td><p class="SimplePara">10.2</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">48.3</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">Toyota Technological Institute at Chicago<span class="InlineEquation" id="IEq355">\(^\dagger \)</span>, Ecole Centrale Paris<span class="InlineEquation" id="IEq356">\(^\ddag \)</span></p></td><td><p class="SimplePara">George Papandreou<span class="InlineEquation" id="IEq357">\(^\dagger \)</span>, Iasonas Kokkinos<span class="InlineEquation" id="IEq358">\(^\ddag \)</span>(Papandreou <span class="CitationRef"><a href="#CR60">2014</a></span>; Papandreou et&nbsp;al. <span class="CitationRef"><a href="#CR61">2014</a></span>; Jojic et&nbsp;al. <span class="CitationRef"><a href="#CR40">2003</a></span>; Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>; Sermanet et&nbsp;al. <span class="CitationRef"><a href="#CR71">2013</a></span>; Dubout and Fleuret <span class="CitationRef"><a href="#CR17">2012</a></span>; Iandola et&nbsp;al. <span class="CitationRef"><a href="#CR38">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;UI</p></td><td><p class="SimplePara">99.5</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq359">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">University of Isfahan</p></td><td><p class="SimplePara">Fatemeh Shafizadegan, Elham Shabaninia (Yang et&nbsp;al. <span class="CitationRef"><a href="#CR97">2009</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;UvA</p></td><td><p class="SimplePara">12.1</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq360">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">32.0</p></td><td><p class="SimplePara">35.4</p></td><td><p class="SimplePara">U. of Amsterdam and Euvision Tech.</p></td><td><p class="SimplePara">Koen van de Sande, Daniel Fontijne, Cees Snoek, Harro Stokman, Arnold Smeulders (van de Sande et&nbsp;al. <span class="CitationRef"><a href="#CR84">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;VGG</p></td><td><p class="SimplePara">7.3</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">25.3</strong></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">University of Oxford</p></td><td><p class="SimplePara">Karen Simonyan, Andrew Zisserman (Simonyan and Zisserman <span class="CitationRef"><a href="#CR73">2014</a></span>)</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;XYZ</p></td><td><p class="SimplePara">11.2</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq361">\(\circ \)</span></p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">–</p></td><td><p class="SimplePara">The University of Queensland</p></td><td><p class="SimplePara">Zhongwen Xu and Yi Yang (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>; Jia <span class="CitationRef"><a href="#CR39">2013</a></span>; Zeiler and Fergus <span class="CitationRef"><a href="#CR99">2013</a></span>; Lin et&nbsp;al. <span class="CitationRef"><a href="#CR46">2014a</a></span>)</p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara">Each method is identified with a codename used in the text. For classificaton and single-object localization we report flat top-5 error, in percents (lower is better). For detection we report mean average precision, in percents (higher is better). CLSo,LOCo,DETo corresponds to entries using outside training data (officially allowed in ILSVRC2014). <span class="InlineEquation" id="IEq362">\(\circ \)</span>means localization error greater than 60&nbsp;% (localization submission was required with every classification submission). Key references are provided where available. More details about the winning entries can be found in Sect.&nbsp;<span class="InternalRef"><a href="#Sec27">5.1</a></span></p></div></div></div><p id="Par122" class="Para"><em class="EmphasisTypeItalic ">ILSVRC2010</em> The first year the challenge consisted of just the classification task. The winning entry from NEC team (Lin et&nbsp;al. <span class="CitationRef"><a href="#CR47">2011</a></span>) used SIFT (Lowe <span class="CitationRef"><a href="#CR50">2004</a></span>) and LBP (Ahonen et&nbsp;al. <span class="CitationRef"><a href="#CR1">2006</a></span>) features with two non-linear coding representations (Zhou et&nbsp;al. <span class="CitationRef"><a href="#CR102">2010</a></span>; Wang et&nbsp;al. <span class="CitationRef"><a href="#CR93">2010</a></span>) and a stochastic SVM. The honorable mention XRCE team (Perronnin et&nbsp;al. <span class="CitationRef"><a href="#CR64">2010</a></span>) used an improved Fisher vector representation (Perronnin and Dance <span class="CitationRef"><a href="#CR62">2007</a></span>) along with PCA dimensionality reduction and data compression followed by a linear SVM. Fisher vector-based methods have evolved over 5&nbsp;years of the challenge and continued performing strongly in every ILSVRC from 2010 to 2014.</p><p id="Par123" class="Para"><em class="EmphasisTypeItalic ">ILSVRC2011</em> The winning classification entry in 2011 was the 2010 runner-up team XRCE, applying high-dimensional image signatures (Perronnin et&nbsp;al. <span class="CitationRef"><a href="#CR64">2010</a></span>) with compression using product quantization (Sanchez and Perronnin <span class="CitationRef"><a href="#CR67">2011</a></span>) and one-vs-all linear SVMs. The single-object localization competition was held for the first time, with two brave entries. The winner was the UvA team using a selective search approach to generate class-independent object hypothesis regions (van de Sande et&nbsp;al. <span class="CitationRef"><a href="#CR85">2011b</a></span>), followed by dense sampling and vector quantization of several color SIFT features (van de Sande et&nbsp;al. <span class="CitationRef"><a href="#CR86">2010</a></span>), pooling with spatial pyramid matching (Lazebnik et&nbsp;al. <span class="CitationRef"><a href="#CR45">2006</a></span>), and classifying with a histogram intersection kernel SVM (Maji and Malik <span class="CitationRef"><a href="#CR51">2009</a></span>) trained on a GPU (van de Sande et&nbsp;al. <span class="CitationRef"><a href="#CR87">2011a</a></span>).</p><p id="Par124" class="Para"><em class="EmphasisTypeItalic ">ILSVRC2012</em> This was a turning point for large-scale object recognition, when large-scale deep neural networks entered the scene. The undisputed winner of both the classification and localization tasks in 2012 was the SuperVision team. They trained a large, deep convolutional neural network on RGB values, with 60 million parameters using an efficient GPU implementation and a novel hidden-unit dropout trick (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>; Hinton et&nbsp;al. <span class="CitationRef"><a href="#CR34">2012</a></span>). The second place in image classification went to the ISI team, which used Fisher vectors (Sanchez and Perronnin <span class="CitationRef"><a href="#CR67">2011</a></span>) and a streamlined version of Graphical Gaussian Vectors (Harada and Kuniyoshi <span class="CitationRef"><a href="#CR31">2012</a></span>), along with linear classifiers using Passive-Aggressive (PA) algorithm (Crammer et&nbsp;al. <span class="CitationRef"><a href="#CR11">2006</a></span>). The second place in single-object localization went to the VGG, with an image classification system including dense SIFT features and color statistics (Lowe <span class="CitationRef"><a href="#CR50">2004</a></span>), a Fisher vector representation (Sanchez and Perronnin <span class="CitationRef"><a href="#CR67">2011</a></span>), and a linear SVM classifier, plus additional insights from (Arandjelovic and Zisserman <span class="CitationRef"><a href="#CR3">2012</a></span>; Sanchez et&nbsp;al. <span class="CitationRef"><a href="#CR68">2012</a></span>). Both ISI and VGG used (Felzenszwalb et&nbsp;al. <span class="CitationRef"><a href="#CR23">2010</a></span>) for object localization; SuperVision used a regression model trained to predict bounding box locations. Despite the weaker detection model, SuperVision handily won the object localization task. A detailed analysis and comparison of the SuperVision and VGG submissions on the single-object localization task can be found in Russakovsky et&nbsp;al. (<span class="CitationRef"><a href="#CR65">2013</a></span>). The influence of the success of the SuperVision model can be clearly seen in ILSVRC2013 and ILSVRC2014.</p><p id="Par125" class="Para"><em class="EmphasisTypeItalic ">ILSVRC2013</em> There were 24 teams participating in the ILSVRC2013 competition, compared to 21 in the previous 3&nbsp;years <em class="EmphasisTypeItalic ">combined</em>. Following the success of the deep learning-based method in 2012, the vast majority of entries in 2013 used deep convolutional neural networks in their submission. The winner of the classification task was Clarifai, with several large deep convolutional networks averaged together. The network architectures were chosen using the visualization technique of (Zeiler and Fergus <span class="CitationRef"><a href="#CR99">2013</a></span>), and they were trained on the GPU following (Zeiler et&nbsp;al. <span class="CitationRef"><a href="#CR100">2011</a></span>) using the dropout technique (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>).</p><p id="Par126" class="Para">The winning single-object localization OverFeat submission was based on an integrated framework for using convolutional networks for classification, localization and detection with a multiscale sliding window approach (Sermanet et&nbsp;al. <span class="CitationRef"><a href="#CR71">2013</a></span>). They were the only team tackling all three tasks.</p><p id="Par127" class="Para">The winner of object detection task was UvA team, which utilized a new way of efficient encoding (van de Sande et&nbsp;al. <span class="CitationRef"><a href="#CR84">2014</a></span>) densely sampled color descriptors (van de Sande et&nbsp;al. <span class="CitationRef"><a href="#CR86">2010</a></span>) pooled using a multi-level spatial pyramid in a selective search framework (Uijlings et&nbsp;al. <span class="CitationRef"><a href="#CR82">2013</a></span>). The detection results were rescored using a full-image convolutional network classifier.</p><p id="Par128" class="Para"><em class="EmphasisTypeItalic ">ILSVRC2014</em> 2014 attracted the most submissions, with 36 teams submitting 123 entries compared to just 24 teams in 2013—a 1.5<span class="InlineEquation" id="IEq363">\(\times \)</span> increase in participation.<sup><a href="#Fn9" id="Fn9_source">9</a></sup> As in 2013 almost all teams used convolutional neural networks as the basis for their submission. Significant progress has been made in just 1&nbsp;year: image classification error was almost halved since ILSVRC2013 and object detection mean average precision almost doubled compared to ILSVRC2013. Please refer to Sect.&nbsp;<span class="InternalRef"><a href="#Sec30">6.1</a></span> for details.</p><p id="Par130" class="Para">In 2014 teams were allowed to use outside data for training their models in the competition, so there were six tracks: provided and outside data tracks in each of image classification, single-object localization, and object detection tasks.</p><p id="Par131" class="Para">The winning image classification with provided data team was GoogLeNet, which explored an improved convolutional neural network architecture combining the multi-scale idea with intuitions gained from the Hebbian principle. Additional dimension reduction layers allowed them to increase both the depth and the width of the network significantly without incurring significant computational overhead. In the image classification with external data track, CASIAWS won by using weakly supervised object localization from only classification labels to improve image classification. MCG region proposals (Arbeláez et&nbsp;al. <span class="CitationRef"><a href="#CR4">2014</a></span>) pretrained on PASCAL VOC 2012 data are used to extract region proposals, regions are represented using convolutional networks, and a multiple instance learning strategy is used to learn weakly supervised object detectors to represent the image.</p><p id="Par132" class="Para">In the single-object localization with provided data track, the winning team was VGG, which explored the effect of convolutional neural network depth on its accuracy by using three different architectures with up to 19 weight layers with rectified linear unit non-linearity, building off of the implementation of Caffe (Jia <span class="CitationRef"><a href="#CR39">2013</a></span>). For localization they used per-class bounding box regression similar to OverFeat (Sermanet et&nbsp;al. <span class="CitationRef"><a href="#CR71">2013</a></span>). In the single-object localization with external data track, Adobe used 2000 additional ImageNet classes to train the classifiers in an integrated convolutional neural network framework for both classification and localization, with bounding box regression. At test time they used k-means to find bounding box clusters and rank the clusters according to the classification scores.</p><p id="Par133" class="Para">In the object detection with provided data track, the winning team NUS used the RCNN framework (Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR26">2013</a></span>) with the network-in-network method (Lin et&nbsp;al. <span class="CitationRef"><a href="#CR46">2014a</a></span>) and improvements of (Howard <span class="CitationRef"><a href="#CR36">2014</a></span>). Global context information was incorporated following (Chen et&nbsp;al. <span class="CitationRef"><a href="#CR10">2014</a></span>). In the object detection with external data track, the winning team was GoogLeNet (which also won image classification with provided data). It is truly remarkable that the same team was able to win at both image classification and object detection, indicating that their methods are able to not only classify the image based on scene information but also accurately localize multiple object instances. Just like most teams participating in this track, GoogLeNet used the image classification dataset as extra training data.</p></section><section id="Sec28" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.2 </span>Large Scale Algorithmic Innovations</h3><p id="Par134" class="Para">ILSVRC over the past 5&nbsp;years has paved the way for several breakthroughs in computer vision.</p><p id="Par135" class="Para">The field of categorical object recognition has dramatically evolved in the large-scale setting. Section&nbsp;<span class="InternalRef"><a href="#Sec27">5.1</a></span> documents the progress, starting from coded SIFT features and evolving to large-scale convolutional neural networks dominating at all three tasks of image classification, single-object localization, and object detection. With the availability of so much training data (along with an efficient algorithmic implementation and GPU computing resources) it became possible to learn neural networks directly from the image data, without needing to create multi-stage hand-tuned pipelines of extracted features and discriminative classifiers. The major breakthrough came in 2012 with the win of the SuperVision team on image classification and single-object localization tasks (Krizhevsky et&nbsp;al. <span class="CitationRef"><a href="#CR43">2012</a></span>), and by 2014 all of the top contestants were relying heavily on convolutional neural networks.</p><p id="Par136" class="Para">Further, over the past few years there has been a lot of focus on large-scale recognition in the computer vision community . Best paper awards at top vision conferences in 2013 were awarded to large-scale recognition methods: at CVPR 2013 to “Fast, Accurate Detection of 100,000 Object Classes on a Single Machine” (Dean et&nbsp;al. <span class="CitationRef"><a href="#CR13">2013</a></span>) and at ICCV 2013 to “From Large Scale Image Categorization to Entry-Level Categories” (Ordonez et&nbsp;al. <span class="CitationRef"><a href="#CR57">2013</a></span>). Additionally, several influential lines of research have emerged, such as large-scale weakly supervised localization work of (Kuettel et&nbsp;al. <span class="CitationRef"><a href="#CR44">2012</a></span>) which was awarded the best paper award in ECCV 2012 and large-scale zero-shot learning, e.g., (Frome et&nbsp;al. <span class="CitationRef"><a href="#CR24">2013</a></span>).</p></section></div></section><section id="Sec29" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">6 </span>Results and Analysis</h2><div class="content"><section id="Sec30" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">6.1 </span>Improvements over the Years</h3><div id="Par137" class="Para">State-of-the-art accuracy has improved significantly from ILSVRC2010 to ILSVRC2014, showcasing the massive progress that has been made in large-scale object recognition over the past 5&nbsp;years. The performance of the winning ILSVRC entries for each task and each year are shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig9">9</a></span>. The improvement over the years is clearly visible. In this section we quantify and analyze this improvement.<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO16"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig9_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig9_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 9</span><p class="SimplePara">Performance of winning entries in the ILSVRC2010-2014 competitions in each of the three tasks (details about the entries and numerical results are in Sect.&nbsp;<span class="InternalRef"><a href="#Sec27">5.1</a></span>). There is a steady reduction of error every year in object classification and single-object localization tasks, and a 1.9<span class="InlineEquation" id="IEq364">\(\times \)</span> improvement in mean average precision in object detection. There are two considerations in making these comparisons. (1) The object categories used in ISLVRC changed between years 2010 and 2011, and between 2011 and 2012. However, the large scale of the data (1000 object categories, 1.2 million training images) has remained the same, making it possible to compare results. Image classification and single-object localization entries shown here use only provided training data. (2) The size of the object detection training data has increased significantly between years 2013 and 2014 (Sect.&nbsp;<span class="InternalRef"><a href="#Sec17">3.3</a></span>). Section&nbsp;<span class="InternalRef"><a href="#Sec30">6.1</a></span> discusses the relative effects of training data increase versus algorithmic improvements</p></div></figcaption></figure></div><section id="Sec31" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.1.1 </span>Image Classification and Single-Object Localization Improvement over the Years</h4><p id="Par138" class="Para">There has been a 4.2<span class="InlineEquation" id="IEq365">\(\times \)</span> reduction in image classification error (from 28.2 to 6.7&nbsp;%) and a 1.7<span class="InlineEquation" id="IEq366">\(\times \)</span> reduction in single-object localization error (from 42.5 to 25.3&nbsp;%) since the beginning of the challenge. For consistency, here we consider only teams that use the provided training data. Even though the exact object categories have changed (Sect.&nbsp;<span class="InternalRef"><a href="#Sec10">3.1.1</a></span>), the large scale of the dataset has remained the same (Table&nbsp;<span class="InternalRef"><a href="#Tab3">3</a></span>), making the results comparable across the years. The dataset has not changed since 2012, and there has been a 2.4<span class="InlineEquation" id="IEq367">\(\times \)</span> reduction in image classification error (from 16.4 to 6.7&nbsp;%) and a 1.3<span class="InlineEquation" id="IEq368">\(\times \)</span> in single-object localization error (from 33.5 to 25.3&nbsp;%) in the past 3&nbsp;years.</p></section><section id="Sec32" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.1.2 </span>Object Detection Improvement over the Years</h4><p id="Par139" class="Para">Object detection accuracy as measured by the mean average precision (mAP) has increased 1.9<span class="InlineEquation" id="IEq369">\(\times \)</span> since the introduction of this task, from 22.6&nbsp;% mAP in ILSVRC2013 to 43.9&nbsp;% mAP in ILSVRC2014. However, these results are not directly comparable for two reasons. First, the size of the object detection training data has increased significantly from 2013 to 2014 (Sect.&nbsp;<span class="InternalRef"><a href="#Sec17">3.3</a></span>). Second, the 43.9&nbsp;% mAP result was obtained with the addition of the image classification and single-object localization training data. Here we attempt to understand the relative effects of the training set size increase versus algorithmic improvements. All models are evaluated on the same ILSVRC2013-2014 object detection test set.</p><p id="Par140" class="Para">First, we quantify the effects of increasing detection training data between the two challenges by comparing the same model trained on ILSVRC2013 detection data versus ILSVRC2014 detection data. The UvA team’s framework from 2013 achieved 22.6&nbsp;% with ILSVRC2013 data (Table&nbsp;<span class="InternalRef"><a href="#Tab7">7</a></span>) and 26.3&nbsp;% with ILSVRC2014 data and no other modifications.<sup><a href="#Fn10" id="Fn10_source">10</a></sup> The absolute increase in mAP was 3.7&nbsp;%. The RCNN model achieved 31.4&nbsp;% mAP with ILSVRC2013 detection plus image classification data (Girshick et&nbsp;al. <span class="CitationRef"><a href="#CR26">2013</a></span>) and 34.5&nbsp;% mAP with ILSVRC2014 detection plus image classification data (Berkeley team in Table&nbsp;<span class="InternalRef"><a href="#Tab8">8</a></span>). The absolute increase in mAP by expanding ILSVRC2013 detection data to ILSVRC2014 was 3.1&nbsp;%.</p><p id="Par142" class="Para">Second, we quantify the effects of adding in the external data for training object detection models. The NEC model in 2013 achieved 19.6&nbsp;% mAP trained on ILSVRC2013 detection data alone and 20.9&nbsp;% mAP trained on ILSVRC2013 detection plus classification data (Table&nbsp;<span class="InternalRef"><a href="#Tab7">7</a></span>). The absolute increase in mAP was 1.3&nbsp;%. The UvA team’s best entry in 2014 achieved 32.0&nbsp;% mAP trained on ILSVRC2014 detection data and 35.4&nbsp;% mAP trained on ILSVRC2014 detection plus classification data. The absolute increase in mAP was 3.4&nbsp;%.</p><p id="Par143" class="Para">Thus, we conclude based on the evidence so far that expanding the ILSVRC2013 detection set to the ILSVRC2014 set, as well as adding in additional training data from the classification task, all account for approximately 1–4&nbsp;% in absolute mAP improvement for the models. For comparison, we can also attempt to quantify the effect of algorithmic innovation. The UvA team’s 2013 framework achieved 26.3&nbsp;% mAP on ILSVRC2014 data as mentioned above, and their improved method in 2014 obtained 32.0&nbsp;% mAP (Table&nbsp;<span class="InternalRef"><a href="#Tab8">8</a></span>). This is 5.8&nbsp;% absolute increase in mAP over just 1&nbsp;year from algorithmic innovation alone.</p><p id="Par144" class="Para">In summary, we conclude that the absolute 21.3&nbsp;% increase in mAP between winning entries of ILSVRC2013 (22.6&nbsp;% mAP) and of ILSVRC2014 (43.9&nbsp;% mAP) is the result of impressive algorithmic innovation and not just a consequence of increased training data. However, increasing the ISLVRC2014 object detection training dataset further <em class="EmphasisTypeItalic ">is</em> likely to produce additional improvements in detection accuracy for current algorithms.</p></section></section><section id="Sec33" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">6.2 </span>Statistical Significance</h3><p id="Par145" class="Para">One important question to ask is whether results of different submissions to ILSVRC are statistically significantly different from each other. Given the large scale, it is no surprise that even minor differences in accuracy are statistically significant; we seek to quantify exactly how much of a difference is enough.</p><div id="Par146" class="Para">Following the strategy employed by PASCAL VOC (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR20">2014</a></span>), for each method we obtain a confidence interval of its score using bootstrap sampling. During each bootstrap round, we sample <span class="InlineEquation" id="IEq389">\(N\)</span> images with replacement from all the available <span class="InlineEquation" id="IEq390">\(N\)</span> test images and evaluate the performance of the algorithm on those sampled images. This can be done very efficiently by precomputing the accuracy on each image. Given the results of all the bootstrapping rounds we discard the lower and the upper <span class="InlineEquation" id="IEq391">\(\alpha \)</span> fraction. The range of the remaining results represents the <span class="InlineEquation" id="IEq392">\(1-2\alpha \)</span> confidence interval. We run a large number of bootstrapping rounds (from 20,000 until convergence). Table&nbsp;<span class="InternalRef"><a href="#Tab9">9</a></span> shows the results of the top entries to each task of ILSVRC2012-2014. The winning methods are statistically significantly different from the other methods, even at the 99.9&nbsp;% level.<div id="Tab9" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 9</span><p class="SimplePara">We use bootstrapping to construct 99.9&nbsp;each ILSVRC task in 2012–2014</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"></colgroup><thead><tr><th><p class="SimplePara">Year</p></th><th><p class="SimplePara">Codename</p></th><th><p class="SimplePara">Error (%)</p></th><th><p class="SimplePara">99.9&nbsp;% Conf Int</p></th></tr></thead><tbody><tr><td colspan="4"><p class="SimplePara">Image classification</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;<strong class="EmphasisTypeBold ">2014</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">GoogLeNet</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">6.66</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">6.40–6.92</strong></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">VGG</p></td><td><p class="SimplePara">7.32</p></td><td><p class="SimplePara">7.05–7.60</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">MSRA</p></td><td><p class="SimplePara">8.06</p></td><td><p class="SimplePara">7.78–8.34</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">AHoward</p></td><td><p class="SimplePara">8.11</p></td><td><p class="SimplePara">7.83–8.39</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">DeeperVision</p></td><td><p class="SimplePara">9.51</p></td><td><p class="SimplePara">9.21–9.82</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">Clarifai<span class="InlineEquation" id="IEq370">\(^\dagger \)</span></p></td><td><p class="SimplePara">11.20</p></td><td><p class="SimplePara">10.87–11.53</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">CASIAWS<span class="InlineEquation" id="IEq371">\(^\dagger \)</span></p></td><td><p class="SimplePara">11.36</p></td><td><p class="SimplePara">11.03–11.69</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">Trimps<span class="InlineEquation" id="IEq372">\(^\dagger \)</span></p></td><td><p class="SimplePara">11.46</p></td><td><p class="SimplePara">11.13–11.80</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">Adobe<span class="InlineEquation" id="IEq373">\(^\dagger \)</span></p></td><td><p class="SimplePara">11.58</p></td><td><p class="SimplePara">11.25–11.91</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;<strong class="EmphasisTypeBold ">2013</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">Clarifai</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">11.74</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">11.41–12.08</strong></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">NUS</p></td><td><p class="SimplePara">12.95</p></td><td><p class="SimplePara">12.60–13.30</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">ZF</p></td><td><p class="SimplePara">13.51</p></td><td><p class="SimplePara">13.14–13.87</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">AHoward</p></td><td><p class="SimplePara">13.55</p></td><td><p class="SimplePara">13.20–13.91</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">OverFeat</p></td><td><p class="SimplePara">14.18</p></td><td><p class="SimplePara">13.83–14.54</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">Orange<span class="InlineEquation" id="IEq374">\(^\dagger \)</span></p></td><td><p class="SimplePara">14.80</p></td><td><p class="SimplePara">14.43–15.17</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2012</p></td><td><p class="SimplePara">SuperVision<span class="InlineEquation" id="IEq375">\(^\dagger \)</span></p></td><td><p class="SimplePara">15.32</p></td><td><p class="SimplePara">14.94–15.69</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;<strong class="EmphasisTypeBold ">2012</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">SuperVision</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">16.42</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">16.04–16.80</strong></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2012</p></td><td><p class="SimplePara">ISI</p></td><td><p class="SimplePara">26.17</p></td><td><p class="SimplePara">25.71–26.65</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2012</p></td><td><p class="SimplePara">VGG</p></td><td><p class="SimplePara">26.98</p></td><td><p class="SimplePara">26.53–27.43</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2012</p></td><td><p class="SimplePara">XRCE</p></td><td><p class="SimplePara">27.06</p></td><td><p class="SimplePara">26.60–27.52</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2012</p></td><td><p class="SimplePara">UvA</p></td><td><p class="SimplePara">29.58</p></td><td><p class="SimplePara">29.09–30.04</p></td></tr><tr><td colspan="4"><p class="SimplePara">Single-object localization</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;<strong class="EmphasisTypeBold ">2014</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">VGG</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">25.32</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">24.87–25.78</strong></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">GoogLeNet</p></td><td><p class="SimplePara">26.44</p></td><td><p class="SimplePara">25.98 – 26.92</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;<strong class="EmphasisTypeBold ">2013</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">OverFeat</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">29.88</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">29.38–30.35</strong></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">Adobe<span class="InlineEquation" id="IEq376">\(^\dagger \)</span></p></td><td><p class="SimplePara">30.10</p></td><td><p class="SimplePara">29.61–30.58</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">SYSU</p></td><td><p class="SimplePara">31.90</p></td><td><p class="SimplePara">31.40–32.40</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2012</p></td><td><p class="SimplePara">SuperVision<span class="InlineEquation" id="IEq377">\(^\dagger \)</span></p></td><td><p class="SimplePara">33.55</p></td><td><p class="SimplePara">33.05–34.04</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">MIL</p></td><td><p class="SimplePara">33.74</p></td><td><p class="SimplePara">33.24–34.25</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;<strong class="EmphasisTypeBold ">2012</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">SuperVision</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">34.19</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">33.67–34.69</strong></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">MSRA</p></td><td><p class="SimplePara">35.48</p></td><td><p class="SimplePara">34.97–35.99</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">Trimps<span class="InlineEquation" id="IEq378">\(^\dagger \)</span></p></td><td><p class="SimplePara">42.22</p></td><td><p class="SimplePara">41.69–42.75</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">Orange<span class="InlineEquation" id="IEq379">\(^\dagger \)</span></p></td><td><p class="SimplePara">42.70</p></td><td><p class="SimplePara">42.18–43.24</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">VGG</p></td><td><p class="SimplePara">46.42</p></td><td><p class="SimplePara">45.90–46.95</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2012</p></td><td><p class="SimplePara">VGG</p></td><td><p class="SimplePara">50.03</p></td><td><p class="SimplePara">49.50–50.57</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2012</p></td><td><p class="SimplePara">ISI</p></td><td><p class="SimplePara">53.65</p></td><td><p class="SimplePara">53.10–54.17</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">CASIAWS<span class="InlineEquation" id="IEq380">\(^\dagger \)</span></p></td><td><p class="SimplePara">61.96</p></td><td><p class="SimplePara">61.44–62.48</p></td></tr><tr><td colspan="4"><p class="SimplePara">Object detection</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">GoogLeNet<span class="InlineEquation" id="IEq381">\(^\dagger \)</span></p></td><td><p class="SimplePara">43.93</p></td><td><p class="SimplePara">42.92–45.65</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">CUHK<span class="InlineEquation" id="IEq382">\(^\dagger \)</span></p></td><td><p class="SimplePara">40.67</p></td><td><p class="SimplePara">39.68–42.30</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">DeepInsight<span class="InlineEquation" id="IEq383">\(^\dagger \)</span></p></td><td><p class="SimplePara">40.45</p></td><td><p class="SimplePara">39.49–42.06</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;<strong class="EmphasisTypeBold ">2014</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">NUS</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">37.21</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">36.29–38.80</strong></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">UvA<span class="InlineEquation" id="IEq384">\(^\dagger \)</span></p></td><td><p class="SimplePara">35.42</p></td><td><p class="SimplePara">34.63–36.92</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">MSRA</p></td><td><p class="SimplePara">35.11</p></td><td><p class="SimplePara">34.36–36.70</p></td></tr></tbody></table><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"><col class="tcol4 align-left"></colgroup><thead><tr><th><p class="SimplePara">Year</p></th><th><p class="SimplePara">Codename</p></th><th><p class="SimplePara">AP (%)</p></th><th><p class="SimplePara">99.9&nbsp;% Conf Int</p></th></tr></thead><tbody><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">Berkeley<span class="InlineEquation" id="IEq385">\(^\dagger \)</span></p></td><td><p class="SimplePara">34.52</p></td><td><p class="SimplePara">33.67–36.12</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">UvA</p></td><td><p class="SimplePara">32.03</p></td><td><p class="SimplePara">31.28–33.49</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">Southeast</p></td><td><p class="SimplePara">30.48</p></td><td><p class="SimplePara">29.70–31.93</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2014</p></td><td><p class="SimplePara">HKUST</p></td><td><p class="SimplePara">28.87</p></td><td><p class="SimplePara">28.03–30.20</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;<strong class="EmphasisTypeBold ">2013</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">UvA</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">22.58</strong></p></td><td><p class="SimplePara"><strong class="EmphasisTypeBold ">22.00–23.82</strong></p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">NEC<span class="InlineEquation" id="IEq386">\(^\dagger \)</span></p></td><td><p class="SimplePara">20.90</p></td><td><p class="SimplePara">20.40–22.15</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">NEC</p></td><td><p class="SimplePara">19.62</p></td><td><p class="SimplePara">19.14–20.85</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">OverFeat<span class="InlineEquation" id="IEq387">\(^\dagger \)</span></p></td><td><p class="SimplePara">19.40</p></td><td><p class="SimplePara">18.82–20.61</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">Toronto</p></td><td><p class="SimplePara">11.46</p></td><td><p class="SimplePara">10.98–12.34</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">SYSU</p></td><td><p class="SimplePara">10.45</p></td><td><p class="SimplePara">10.04–11.32</p></td></tr><tr><td><p class="SimplePara">&nbsp;&nbsp;&nbsp;2013</p></td><td><p class="SimplePara">UCLA</p></td><td><p class="SimplePara">9.83</p></td><td><p class="SimplePara">9.48–10.77</p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara"><span class="InlineEquation" id="IEq388">\(^\dagger \)</span> Means the entry used external training data. The winners using the provided data for each track and each year are bolded. The difference between the winning method and the runner-up each year is significant even at the 99.9&nbsp;% level</p></div></div></div></section><section id="Sec34" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">6.3 </span>Current State of Categorical Object Recognition</h3><p id="Par147" class="Para">Besides looking at just the average accuracy across hundreds of object categories and tens of thousands of images, we can also delve deeper to understand where mistakes are being made and where researchers’ efforts should be focused to expedite progress.</p><p id="Par148" class="Para">To do so, in this section we will be analyzing an “optimistic” measurement of state-of-the-art recognition performance instead of focusing on the differences in individual algorithms. For each task and each object class, we compute the best performance of <em class="EmphasisTypeItalic ">any</em> entry submitted to <em class="EmphasisTypeItalic ">any</em> ILSVRC2012-2014, including methods using additional training data. Since the test sets have remained the same, we can directly compare all the entries in the past 3&nbsp;years to obtain the most “optimistic” measurement of state-of-the-art accuracy on each category.</p><p id="Par149" class="Para">For consistency with the object detection metric (higher is better), in this section we will be using image classification and single-object localization <em class="EmphasisTypeItalic ">accuracy</em> instead of error, where <span class="InlineEquation" id="IEq393">\(accuracy = 1-error\)</span>.</p><section id="Sec35" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.3.1 </span>Range of Accuracy Across Object Classes</h4><div id="Par150" class="Para">Figure&nbsp;<span class="InternalRef"><a href="#Fig10">10</a></span> shows the distribution of accuracy achieved by the “optimistic” models across the object categories. The image classification model achieves 94.6&nbsp;% accuracy on average (or 5.4&nbsp;% error), but there remains a 41.0&nbsp;% absolute difference inaccuracy between the most and least accurate object class. The single-object localization model achieves 81.5&nbsp;% accuracy on average (or 18.5&nbsp;% error), with a 77.0&nbsp;% range in accuracy across the object classes. The object detection model achieves 44.7&nbsp;% average precision, with an 84.7&nbsp;% range across the object classes. It is clear that the ILSVRC dataset is far from saturated: performance on many categories has remained poor despite the strong overall performance of the models.<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO17"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig10_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig10_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 10</span><p class="SimplePara">For each object class, we consider the best performance of any entry submitted to ILSVRC2012-2014, including entries using additional training data. The plots show the distribution of these “optimistic” per-class results. Performance is measured as accuracy for image classification (<em class="EmphasisTypeItalic ">left</em>) and for single-object localization (<em class="EmphasisTypeItalic ">middle</em>), and as average precision for object detection (<em class="EmphasisTypeItalic ">right</em>). While the results are very promising in image classification, the ILSVRC datasets are far from saturated: many object classes continue to be challenging for current algorithms</p></div></figcaption></figure></div></section><section id="Sec36" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.3.2 </span>Qualitative Examples of Easy and Hard Classes</h4><div id="Par151" class="Para">Figures&nbsp;<span class="InternalRef"><a href="#Fig11">11</a></span> and&nbsp;<span class="InternalRef"><a href="#Fig12">12</a></span> show the easiest and hardest classes for each task, i.e., classes with the best and worst results obtained with the “optimistic” models.<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO18"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig11_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig11_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 11</span><p class="SimplePara">For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for each task. The <em class="EmphasisTypeItalic ">numbers</em> in <em class="EmphasisTypeItalic ">parentheses</em> indicate classification and localization accuracy. For image classification the 10 easiest classes are randomly selected from among 121 object classes with 100&nbsp;% accuracy. Object detection results are shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig12">12</a></span></p></div></figcaption></figure><figure class="Figure" id="Fig12"><div class="MediaObject" id="MO19"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig12_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig12_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12</span><p class="SimplePara">For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries using additional training data). Given these “optimistic” results we show the easiest and harder classes for the object detection task, i.e., classes with best and worst results. The <em class="EmphasisTypeItalic ">numbers</em> in <em class="EmphasisTypeItalic ">parentheses</em> indicate average precision. Image classification and single-object localization results are shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig11">11</a></span></p></div></figcaption></figure></div><p id="Par152" class="Para">For image classification, 121 out of 1000 object classes have 100&nbsp;% image classification accuracy according to the optimistic estimate. Figure&nbsp;<span class="InternalRef"><a href="#Fig11">11</a></span> (top) shows a random set of 10 of them. They contain a variety of classes, such as mammals like “red fox” and animals with distinctive structures like “stingray”. The hardest classes in the image classification task, with accuracy as low as 59.0&nbsp;%, include metallic and see-through man-made objects, such as “hook” and “water bottle,” the material “velvet” and the highly varied scene class “restaurant.”</p><p id="Par153" class="Para">For single-object localization, the 10 easiest classes with 99.0–100&nbsp;% accuracy are all mammals and birds. The hardest classes include metallic man-made objects such as “letter opener” and “ladle”, plus thin structures such as “pole” and “spacebar” and highly varied classes such as “wing”. The most challenging class “spacebar” has a only 23.0&nbsp;% localization accuracy.</p><p id="Par154" class="Para">Object detection results are shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig12">12</a></span>. The easiest classes are living organisms such as “dog” and “tiger”, plus “basketball” and “volleyball” with distinctive shape and color, and a somewhat surprising “snowplow.” The easiest class “butterfly” is not yet perfectly detected but is very close with <span class="InlineEquation" id="IEq394">\(92.7\,\%\)</span> AP. The hardest classes are as expected small thin objects such as “flute” and “nail”, and the highly varied “lamp” and “backpack” classes, with as low as <span class="InlineEquation" id="IEq395">\(8.0\,\%\)</span> AP.</p></section><section id="Sec37" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.3.3 </span>Per-Class Accuracy as a Function of Image Properties</h4><p id="Par155" class="Para">We now take a closer look at the image properties to try to understand why current algorithms perform well on some object classes but not others. One hypothesis is that variation in accuracy comes from the fact that instances of some classes tend to be much smaller in images than instances of other classes, and smaller objects may be harder for computers to recognize. In this section we argue that while accuracy is correlated with object scale in the image, not all variation in accuracy can be accounted for by scale alone.</p><p id="Par156" class="Para">For every object class, we compute its <em class="EmphasisTypeItalic ">average scale</em>, or the average fraction of image area occupied by an instance of the object class on the ILSVRC2012-2014 validation set. Since the images and object classes in the image classification and single-object localization tasks are the same, we use the bounding box annotations of the single-object localization dataset for both tasks. In that dataset the object classes range from “swimming trunks” with scale of <span class="InlineEquation" id="IEq396">\(1.5\,\%\)</span> to “spider web” with scale of <span class="InlineEquation" id="IEq397">\(85.6\,\%\)</span>. In the object detection validation dataset the object classes range from “sunglasses” with scale of <span class="InlineEquation" id="IEq398">\(1.3\,\%\)</span> to “sofa” with scale of <span class="InlineEquation" id="IEq399">\(44.4\,\%\)</span>.</p><div id="Par157" class="Para">Figure&nbsp;<span class="InternalRef"><a href="#Fig13">13</a></span> shows the performance of the “optimistic” method as a function of the average scale of the object in the image. Each dot corresponds to one object class. We observe a very weak positive correlation between object scale and image classification accuracy: <span class="InlineEquation" id="IEq400">\(\rho = 0.14\)</span>. For single-object localization and object detection the correlation is stronger, at <span class="InlineEquation" id="IEq401">\(\rho = 0.40\)</span> and <span class="InlineEquation" id="IEq402">\(\rho = 0.41\)</span> respectively. It is clear that not all variation in accuracy can be accounted for by scale alone. Nevertheless, in the next section we will normalize for object scale to ensure that this factor is not affecting our conclusions.<figure class="Figure" id="Fig13"><div class="MediaObject" id="MO20"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig13_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig13_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 13</span><p class="SimplePara">Performance of the “optimistic” method as a function of object scale in the image, on each task. Each <em class="EmphasisTypeItalic ">dot</em> corresponds to one object class. Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of that object class on the ILSVRC2014 validation set. “Optimistic” performance (y-axis) corresponds to the best performance on the test set of any entry submitted to ILSVRC2012-2014 (including entries with additional training data). The test set has remained the same over these 3&nbsp;years. We see that accuracy tends to increase as the objects get bigger in the image. However, it is clear that far from all the variation in accuracy on these classes can be accounted for by scale alone</p></div></figcaption></figure></div></section><section id="Sec38" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.3.4 </span>Per-Class Accuracy as a Function of Object Properties</h4><p id="Par158" class="Para">Besides considering image-level properties we can also observe how accuracy changes as a function of intrinsic object properties. We define three properties inspired by human vision: the real-world size of the object, whether it’s deformable within instance, and how textured it is. For each property, the object classes are assigned to one of a few bins (listed below). These properties are illustrated in Fig.&nbsp;<span class="InternalRef"><a href="#Fig1">1</a></span>.</p><p id="Par159" class="Para">Human subjects annotated each of the 1000 image classification and single-object localization object classes from ILSVRC2012-2014 with these properties (Russakovsky et&nbsp;al. <span class="CitationRef"><a href="#CR65">2013</a></span>). By construction (see Sect.&nbsp;<span class="InternalRef"><a href="#Sec18">3.3.1</a></span>), each of the 200 object detection classes is either also one of 1000 object classes or is an ancestor of one or more of the 1000 classes in the ImageNet hierarchy. To compute the values of the properties for each object detection class, we simply average the annotated values of the descendant classes.</p><div id="Par160" class="Para">In this section we draw the following conclusions about state-of-the-art recognition accuracy as a function of these object properties:<div class="UnorderedList"><ul class="UnorderedListMarkDash"><li><p id="Par161" class="Para"><em class="EmphasisTypeItalic ">Real-world size</em><em class="EmphasisTypeItalic ">XS for extra small (e.g. nail), small (e.g. fox), medium (e.g. bookcase), large (e.g. car) or XL for extra large (e.g. church)</em> The image classification and single-object localization “optimistic” models performs better on large and extra large real-world objects than on smaller ones. The “optimistic” object detection model surprisingly performs better on extra small objects than on small or medium ones.</p></li><li><p id="Par162" class="Para"><em class="EmphasisTypeItalic ">Deformability within instance</em><em class="EmphasisTypeItalic ">Rigid (e.g., mug) or deformable (e.g., water snake)</em> The “optimistic” model on each of the three tasks performs statistically significantly better on deformable objects compared to rigid ones. However, this effect disappears when analyzing natural objects separately from man-made objects.</p></li><li><p id="Par163" class="Para"><em class="EmphasisTypeItalic ">Amount of texture</em><em class="EmphasisTypeItalic ">none (e.g. punching bag), low (e.g. horse), medium (e.g. sheep) or high (e.g. honeycomb)</em> The “optimistic” model on each of the three tasks is significantly better on objects with at least low level of texture compared to untextured objects.</p></li></ul></div>These and other findings are justified and discussed in detail below.</div><p id="Par164" class="Para"><em class="EmphasisTypeItalic ">Experimental Setup</em> We observed in Sect.&nbsp;<span class="InternalRef"><a href="#Sec37">6.3.3</a></span> that objects that occupy a larger area in the image tend to be somewhat easier to recognize. To make sure that differences in object scale are not influencing results in this section, we normalize each bin by object scale. We discard object classes with the largest scales from each bin as needed until the average object scale of object classes in each bin across one property is the same (or as close as possible). For real-world size property for example, the resulting average object scale in each of the five bins is 31.6–31.7&nbsp;% in the image classification and single-object localization tasks, and 12.9–13.4&nbsp;% in the object detection task.<sup><a href="#Fn11" id="Fn11_source">11</a></sup></p><p id="Par166" class="Para">Figure&nbsp;<span class="InternalRef"><a href="#Fig14">14</a></span> shows the average performance of the “optimistic” model on the object classes that fall into each bin for each property. We analyze the results in detail below. Unless otherwise specified, the reported accuracies below are after the scale normalization step.</p><div id="Par167" class="Para">To evaluate statistical significance, we compute the 95&nbsp;% confidence interval for accuracy using bootstrapping: we repeatedly sample the object classes within the bin with replacement, discard some as needed to normalize by scale, and compute the average accuracy of the “optimistic” model on the remaining classes. We report the <span class="InlineEquation" id="IEq403">\(95\,\%\)</span> confidence intervals (CI) in parentheses.<figure class="Figure" id="Fig14"><div class="MediaObject" id="MO21"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig14_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig14_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 14</span><p class="SimplePara">Performance of the “optimistic” computer vision model as a function of object properties. The x-axis corresponds to object properties annotated by human labelers for each object class (Russakovsky et&nbsp;al. <span class="CitationRef"><a href="#CR65">2013</a></span>) and illustrated in Fig.&nbsp;<span class="InternalRef"><a href="#Fig1">1</a></span>. The y-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is different for each task to make the trends more visible. The <em class="EmphasisTypeItalic ">black circle</em> is the average accuracy of the model on all object classes that fall into each bin. We control for the effects of object scale by normalizing the object scale within each bin (details in Sect.&nbsp;<span class="InternalRef"><a href="#Sec38">6.3.4</a></span>). The <em class="EmphasisTypeItalic ">color bars</em> show the model accuracy averaged across the remaining classes. <em class="EmphasisTypeItalic ">Error bars</em> show the <span class="InlineEquation" id="IEq404">\(95\,\%\)</span> confidence interval obtained with bootstrapping. Some bins are missing <em class="EmphasisTypeItalic ">color bars</em> because less than 5 object classes remained in the bin after scale normalization. For example, the <em class="EmphasisTypeItalic ">bar</em> for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane, bus, train) and after normalizing by scale no classes remain</p></div></figcaption></figure></div><p id="Par168" class="Para"><em class="EmphasisTypeItalic ">Real-World Size</em> In Fig.&nbsp;<span class="InternalRef"><a href="#Fig14">14</a></span> (top, left) we observe that in the image classification task the “optimistic” model tends to perform significantly better on objects which are larger in the real-world. The classification accuracy is 93.6–93.9&nbsp;% on XS, S and M objects compared to <span class="InlineEquation" id="IEq405">\(97.0\,\%\)</span> on L and <span class="InlineEquation" id="IEq406">\(96.4\,\%\)</span> on XL objects. Since this is after normalizing for scale and thus can’t be explained by the objects’ size in the image, we conclude that either (1) larger real-world objects are easier for the model to recognize, or (2) larger real-world objects usually occur in images with very distinctive backgrounds.</p><p id="Par169" class="Para">To distinguish between the two cases we look Fig.&nbsp;<span class="InternalRef"><a href="#Fig14">14</a></span> (top, middle). We see that in the single-object localization task, the L objects are easy to localize at <span class="InlineEquation" id="IEq407">\(82.4\,\%\)</span> localization accuracy. XL objects, however, tend to be the hardest to localize with only <span class="InlineEquation" id="IEq408">\(73.4\,\%\)</span> localization accuracy. We conclude that the appearance of L objects must be easier for the model to learn, while XL objects tend to appear in distinctive backgrounds. The image background make these XL classes easier for the image-level classifier, but the individual instances are difficult to accurately localize. Some examples of L objects are “killer whale,” “schooner,” and “lion,” and some examples of XL objects are “boathouse,” “mosque,” “toyshop” and “steel arch bridge.”</p><p id="Par170" class="Para">In Fig.&nbsp;<span class="InternalRef"><a href="#Fig14">14</a></span> (top,right) corresponding to the object detection task, the influence of real-world object size is not as apparent. One of the key reasons is that many of the XL and L object classes of the image classification and single-object localization datasets were removed in constructing the detection dataset (Sect.&nbsp;<span class="InternalRef"><a href="#Sec18">3.3.1</a></span>) since they were not basic categories well-suited for detection. There were only 3 XL object classes remaining in the dataset (“train,” “airplane” and “bus”), and none after scale normalization.We omit them from the analysis. The average precision of XS, S, M objects (44.5, 39.0, and 38.5&nbsp;% mAP respectively) is statistically insignificant from average precision on L objects: <span class="InlineEquation" id="IEq409">\(95\,\%\)</span> confidence interval of L objects is 37.5–59.5&nbsp;%. This may be due to the fact that there are only 6 L object classes remaining after scale normalization; all other real-world size bins have at least 18 object classes.</p><p id="Par171" class="Para">Finally, it is interesting that performance on XS objects of 44.5 mAP (CI 40.5–47.6&nbsp;%) is statistically significantly better than performance on S or M objects with 39.0 and <span class="InlineEquation" id="IEq410">\(38.5\,\%\)</span> mAP respectively. Some examples of XS objects are “strawberry,” “bow tie” and “rugby ball.”</p><p id="Par172" class="Para"><em class="EmphasisTypeItalic ">Deformability Within Instance</em> In Fig.&nbsp;<span class="InternalRef"><a href="#Fig14">14</a></span>(second row) it is clear that the “optimistic” model performs statistically significantly worse on rigid objects than on deformable objects. Image classification accuracy is <span class="InlineEquation" id="IEq411">\(93.2\,\%\)</span> on rigid objects (CI 92.6–93.8&nbsp;%), much smaller than 95.7&nbsp;% on deformable ones. Single-object localization accuracy is <span class="InlineEquation" id="IEq412">\(76.2\,\%\)</span> on rigid objects (CI 74.9–77.4&nbsp;%), much smaller than <span class="InlineEquation" id="IEq413">\(84.7\,\%\)</span> on deformable ones. Object detection mAP is <span class="InlineEquation" id="IEq414">\(40.1\,\%\)</span> on rigid objects (CI 37.2–42.9&nbsp;%), much smaller than <span class="InlineEquation" id="IEq415">\(44.8\,\%\)</span> on deformable ones.</p><p id="Par173" class="Para">We can further analyze the effects of deformability after separating object classes into “natural” and “man-made” bins based on the ImageNet hierarchy. Deformability is highly correlated with whether the object is natural or man-made: <span class="InlineEquation" id="IEq416">\(0.72\)</span> correlation for image classification and single-object localization classes, and <span class="InlineEquation" id="IEq417">\(0.61\)</span> for object detection classes. Figure&nbsp;<span class="InternalRef"><a href="#Fig14">14</a></span>(third row) shows the effect of deformability on performance of the model for man-made and natural objects separately.</p><p id="Par174" class="Para">Man-made classes are significantly harder than natural classes: classification accuracy <span class="InlineEquation" id="IEq418">\(92.8\,\%\)</span> (CI 92.3–93.3&nbsp;%) for man-made versus <span class="InlineEquation" id="IEq419">\(97.0\,\%\)</span> for natural, localization accuracy <span class="InlineEquation" id="IEq420">\(75.5\,\%\)</span> (CI 74.3–76.5&nbsp;%) for man-made versus <span class="InlineEquation" id="IEq421">\(88.5\,\%\)</span> for natural, and detection mAP <span class="InlineEquation" id="IEq422">\(38.7\,\%\)</span> (CI 35.6–41.3&nbsp;%) for man-made versus <span class="InlineEquation" id="IEq423">\(50.9\,\%\)</span> for natural. However, whether the classes are rigid or deformable within this subdivision is no longer significant in most cases. For example, the image classification accuracy is <span class="InlineEquation" id="IEq424">\(92.3\,\%\)</span> (CI 91.4–93.1&nbsp;%) on man-made rigid objects and <span class="InlineEquation" id="IEq425">\(91.8\,\%\)</span> on man-made deformable objects—not statistically significantly different.</p><p id="Par175" class="Para">There are two cases where the differences in performance <em class="EmphasisTypeItalic ">are</em> statistically significant. First, for single-object localization, natural deformable objects are easier than natural rigid objects: localization accuracy of <span class="InlineEquation" id="IEq426">\(87.9\,\%\)</span> (CI 85.9–90.1&nbsp;%) on natural deformable objects is higher than <span class="InlineEquation" id="IEq427">\(85.8\,\%\)</span> on natural rigid objects—falling slightly outside the 95&nbsp;% confidence interval. This difference in performance is likely because deformable natural animals tend to be easier to localize than rigid natural fruit.</p><p id="Par176" class="Para">Second, for object detection, man-made rigid objects are easier than man-made deformable objects: <span class="InlineEquation" id="IEq428">\(38.5\,\%\)</span> mAP (CI 35.2–41.7&nbsp;%) on man-made rigid objects is higher than <span class="InlineEquation" id="IEq429">\(33.0\,\%\)</span> mAP on man-made deformable objects. This is because man-made rigid objects include classes like “traffic light” or “car” whereas the man-made deformable objects contain challenging classes like “plastic bag,” “swimming trunks” or “stethoscope.”</p><p id="Par177" class="Para"><em class="EmphasisTypeItalic ">Amount of Texture</em> Finally, we analyze the effect that object texture has on the accuracy of the “optimistic” model. Figure&nbsp;<span class="InternalRef"><a href="#Fig14">14</a></span>(fourth row) demonstrates that the model performs better as the amount of texture on the object increases. The most significant difference is between the performance on untextured objects and the performance on objects with low texture. Image classification accuracy is <span class="InlineEquation" id="IEq430">\(90.5\,\%\)</span> on untextured objects (CI 89.3–91.6&nbsp;%), lower than <span class="InlineEquation" id="IEq431">\(94.6\,\%\)</span> on low-textured objects. Single-object localization accuracy is <span class="InlineEquation" id="IEq432">\(71.4\,\%\)</span> on untextured objects (CI 69.1–73.3&nbsp;%), lower than <span class="InlineEquation" id="IEq433">\(80.2\,\%\)</span> on low-textured objects. Object detection mAP is <span class="InlineEquation" id="IEq434">\(33.2\,\%\)</span> on untextured objects (CI 29.5–35.9&nbsp;%), lower than <span class="InlineEquation" id="IEq435">\(42.9\,\%\)</span> on low-textured objects.</p><p id="Par178" class="Para">Texture is correlated with whether the object is natural or man-made, at <span class="InlineEquation" id="IEq436">\(0.35\)</span> correlation for image classification and single-object localization, and <span class="InlineEquation" id="IEq437">\(0.46\)</span> correlation for object detection. To determine if this is a contributing factor, in Fig.&nbsp;<span class="InternalRef"><a href="#Fig14">14</a></span>(bottom row) we break up the object classes into natural and man-made and show the accuracy on objects with no texture versus objects with low texture. We observe that the model is still statistically significantly better on low-textured object classes than on untextured ones, both on man-made and natural object classes independently.<sup><a href="#Fn12" id="Fn12_source">12</a></sup></p></section></section><section id="Sec39" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">6.4 </span>Human Accuracy on Large-Scale Image Classification</h3><p id="Par180" class="Para">Recent improvements in state-of-the-art accuracy on the ILSVRC dataset are easier to put in perspective when compared to human-level accuracy. In this section we compare the performance of the leading large-scale image classification method with the performance of humans on this task.</p><p id="Par181" class="Para">To support this comparison, we developed an interface that allowed a human labeler to annotate images with up to five ILSVRC target classes. We compare human errors to those of the winning ILSRC2014 image classification model, GoogLeNet (Sect.&nbsp;<span class="InternalRef"><a href="#Sec27">5.1</a></span>). For this analysis we use a random sample of 1500 ILSVRC2012-2014 image classification test set images.</p><p id="Par182" class="Para"><em class="EmphasisTypeItalic ">Annotation Interface</em> Our web-based annotation interface consists of one test set image and a list of 1000 ILSVRC categories on the side. Each category is described by its title, such as “cowboy boot.” The categories are sorted in the topological order of the ImageNet hierarchy, which places semantically similar concepts nearby in the list. For example, all motor vehicle-related classes are arranged contiguously in the list. Every class category is additionally accompanied by a row of 13 examples images from the training set to allow for faster visual scanning. The user of the interface selects 5 categories from the list by clicking on the desired items. Since our interface is web-based, it allows for natural scrolling through the list, and also search by text.</p><p id="Par183" class="Para"><em class="EmphasisTypeItalic ">Annotation Protocol</em> We found the task of annotating images with one of 1000 categories to be an extremely challenging task for an untrained annotator. The most common error that an untrained annotator is susceptible to is a failure to consider a relevant class as a possible label because they are unaware of its existence.</p><p id="Par184" class="Para">Therefore, in evaluating the human accuracy we relied primarily on expert annotators who learned to recognize a large portion of the 1000 ILSVRC classes. During training, the annotators labeled a few hundred validation images for practice and later switched to the test set images.</p><section id="Sec40" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.4.1 </span>Quantitative Comparison of Human and Computer Accuracy on Large-Scale Image Classification</h4><p id="Par185" class="Para">We report results based on experiments with two expert annotators. The first annotator (A1) trained on 500 images and annotated 1500 test images. The second annotator (A2) trained on 100 images and then annotated 258 test images. The average pace of labeling was approximately 1 image per minute, but the distribution is strongly bimodal: some images are quickly recognized, while some images (such as those of fine-grained breeds of dogs, birds, or monkeys) may require multiple minutes of concentrated effort.</p><div id="Par186" class="Para">The results are reported in Table <span class="InternalRef"><a href="#Tab10">10</a></span>.<div id="Tab10" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 10</span><p class="SimplePara">Human classification results on the ILSVRC2012-2014 classification test set, for two expert annotators A1 and A2</p></div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-left"><col class="tcol3 align-left"></colgroup><thead><tr><th><p class="SimplePara">Relative confusion</p></th><th><p class="SimplePara">A1</p></th><th><p class="SimplePara">A2</p></th></tr></thead><tbody><tr><td><p class="SimplePara">Human succeeds, GoogLeNet succeeds</p></td><td><p class="SimplePara">1352</p></td><td><p class="SimplePara">219</p></td></tr><tr><td><p class="SimplePara">Human succeeds, GoogLeNet fails</p></td><td><p class="SimplePara">72</p></td><td><p class="SimplePara">8</p></td></tr><tr><td><p class="SimplePara">Human fails, GoogLeNet succeeds</p></td><td><p class="SimplePara">46</p></td><td><p class="SimplePara">24</p></td></tr><tr><td><p class="SimplePara">Human fails, GoogLeNet fails</p></td><td><p class="SimplePara">30</p></td><td><p class="SimplePara">7</p></td></tr><tr><td><p class="SimplePara">Total number of images</p></td><td><p class="SimplePara">1500</p></td><td><p class="SimplePara">258</p></td></tr><tr><td><p class="SimplePara">Estimated GoogLeNet classification error</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq438">\(6.8\,\%\)</span></p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq439">\(5.8\,\%\)</span></p></td></tr><tr><td><p class="SimplePara">Estimated human classification error</p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq440">\(5.1\,\%\)</span></p></td><td><p class="SimplePara"><span class="InlineEquation" id="IEq441">\(12.0\,\%\)</span></p></td></tr></tbody></table></div><div class="TableFooter"><p class="SimplePara">We report top-5 classification error</p></div></div></div><p id="Par187" class="Para"><em class="EmphasisTypeItalic ">Annotator 1</em> Annotator A1 evaluated a total of 1500 test set images. The GoogLeNet classification error on this sample was estimated to be <span class="InlineEquation" id="IEq442">\(6.8\,\%\)</span> (recall that the error on full test set of 100,000 images is <span class="InlineEquation" id="IEq443">\(6.7\,\%\)</span>, as shown in Table&nbsp;<span class="InternalRef"><a href="#Tab8">8</a></span>). The human error was estimated to be <span class="InlineEquation" id="IEq444">\(\mathbf 5.1\,\% \)</span>. Thus, annotator A1 achieves a performance superior to GoogLeNet, by approximately <span class="InlineEquation" id="IEq445">\(1.7\,\%\)</span>. We can analyze the statistical significance of this result under the null hypothesis that they are from the same distribution. In particular, comparing the two proportions with a z-test yields a one-sided <span class="InlineEquation" id="IEq446">\(p\)</span>-value of <span class="InlineEquation" id="IEq447">\(p = 0.022\)</span>. Thus, we can conclude that this result is statistically significant at the <span class="InlineEquation" id="IEq448">\(95\,\%\)</span> confidence level.</p><p id="Par188" class="Para"><em class="EmphasisTypeItalic ">Annotator 2</em> Our second annotator (A2) trained on a smaller sample of only 100 images and then labeled 258 test set images. As seen in Table&nbsp;<span class="InternalRef"><a href="#Tab10">10</a></span>, the final classification error is significantly worse, at approximately <span class="InlineEquation" id="IEq449">\(12.0\,\%\)</span> Top-5 error. The majority of these errors (<span class="InlineEquation" id="IEq450">\(48.8\,\%\)</span>) can be attributed to the annotator failing to spot and consider the ground truth label as an option.</p><p id="Par189" class="Para">Thus, we conclude that a significant amount of training time is necessary for a human to achieve competitive performance on ILSVRC. However, with a sufficient amount of training, a human annotator is still able to outperform the GoogLeNet result (<span class="InlineEquation" id="IEq451">\(p = 0.022\)</span>) by approximately <span class="InlineEquation" id="IEq452">\(1.7\,\%\)</span>.</p><p id="Par190" class="Para"><em class="EmphasisTypeItalic ">Annotator Comparison</em> We also compare the prediction accuracy of the two annotators. Of a total of 204 images that both A1 and A2 labeled, 174 (<span class="InlineEquation" id="IEq453">\(85\,\%\)</span>) were correctly labeled by both A1 and A2, 19 (<span class="InlineEquation" id="IEq454">\(9\,\%\)</span>) were correctly labeled by A1 but not A2, 6 (<span class="InlineEquation" id="IEq455">\(3\,\%\)</span>) were correctly labeled by A2 but not A1, and 5 (<span class="InlineEquation" id="IEq456">\(2\,\%\)</span>) were incorrectly labeled by both. These include 2 images that we consider to be incorrectly labeled in the ground truth.</p><p id="Par191" class="Para">In particular, our results suggest that the human annotators do not exhibit strong overlap in their predictions. We can approximate the performance of an “optimistic” human classifier by assuming an image to be correct if at least one of A1 or A2 correctly labeled the image. On this sample of 204 images, we approximate the error rate of an “optimistic” human annotator at <span class="InlineEquation" id="IEq457">\(2.4\,\%\)</span>, compared to the GoogLeNet error rate of <span class="InlineEquation" id="IEq458">\(4.9\,\%\)</span>.</p></section><section id="Sec41" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.4.2 </span>Analysis of Human and Computer Errors on Large-Scale Image Classification</h4><div id="Par192" class="Para">We manually inspected both human and GoogLeNet errors to gain an understanding of common error types and how they compare. For purposes of this section, we only discuss results based on the larger sample of 1500 images that were labeled by annotator A1. Examples of representative mistakes are in Fig.&nbsp;<span class="InternalRef"><a href="#Fig15">15</a></span>. The analysis and insights below were derived specifically from GoogLeNet predictions, but we suspect that many of the same errors may be present in other methods.<figure class="Figure" id="Fig15"><div class="MediaObject" id="MO22"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig15_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig15_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 15</span><p class="SimplePara">Representative validation images that highlight common sources of error. For each image, we display the ground truth in <em class="EmphasisTypeItalic ">blue</em>, and top 5 predictions from GoogLeNet follow (<em class="EmphasisTypeItalic ">red</em> wrong, <em class="EmphasisTypeItalic ">green</em> right). GoogLeNet predictions on the validation set images were graciously provided by members of the GoogLeNet team. From <em class="EmphasisTypeItalic ">left</em> to <em class="EmphasisTypeItalic ">right</em>: Images that contain multiple objects, images of extreme closeups and uncharacteristic views, images with filters, images that significantly benefit from the ability to read text, images that contain very small and thin objects, images with abstract representations, and example of a fine-grained image that GoogLeNet correctly identifies but a human would have significant difficulty with</p></div></figcaption></figure></div><div id="Par193" class="Para"><em class="EmphasisTypeItalic ">Types of Errors in Both Computer and Human Annotations</em><div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(1)</span><div class="ItemContent"><p id="Par194" class="Para"><strong class="EmphasisTypeBold ">Multiple objects</strong> Both GoogLeNet and humans struggle with images that contain multiple ILSVRC classes (usually many more than five), with little indication of which object is the focus of the image. This error is only present in the Classification setting, since every image is constrained to have exactly one correct label. In total, we attribute 24 (<span class="InlineEquation" id="IEq459">\(24\,\%\)</span>) of GoogLeNet errors and 12 (<span class="InlineEquation" id="IEq460">\(16\,\%\)</span>) of human errors to this category. It is worth noting that humans can have a slight advantage in this error type, since it can sometimes be easy to identify the most salient object in the image.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(2)</span><div class="ItemContent"><p id="Par195" class="Para"><strong class="EmphasisTypeBold ">Incorrect annotations</strong> We found that approximately 5 out of 1500 images (<span class="InlineEquation" id="IEq461">\(0.3\,\%\)</span>) were incorrectly annotated in the ground truth. This introduces an approximately equal number of errors for both humans and GoogLeNet.</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div><em class="EmphasisTypeItalic ">Types of Errors that the Computer is More Susceptible to than the Human</em><div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(1)</span><div class="ItemContent"><p id="Par196" class="Para"><strong class="EmphasisTypeBold ">Object small or thin</strong> GoogLeNet struggles with recognizing objects that are very small or thin in the image, even if that object is the only object present. Examples of this include an image of a standing person wearing sunglasses, a person holding a quill in their hand, or a small ant on a stem of a flower. We estimate that approximately 22 (<span class="InlineEquation" id="IEq462">\(21\,\%\)</span>) of GoogLeNet errors fall into this category, while none of the human errors do. In other words, in our sample of images, no image was mislabeled by a human because they were unable to identify a very small or thin object. This discrepancy can be attributed to the fact that a human can very effectively leverage context and affordances to accurately infer the identity of small objects (for example, a few barely visible feathers near person’s hand as very likely belonging to a mostly occluded quill).</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(2)</span><div class="ItemContent"><p id="Par197" class="Para"><strong class="EmphasisTypeBold ">Image filters</strong> Many people enhance their photos with filters that distort the contrast and color distributions of the image. We found that 13 (<span class="InlineEquation" id="IEq463">\(13\,\%\)</span>) of the images that GoogLeNet incorrectly classified contained a filter. Thus, we posit that GoogLeNet is not very robust to these distortions. In comparison, only one image among the human errors contained a filter, but we do not attribute the source of the error to the filter.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(3)</span><div class="ItemContent"><p id="Par198" class="Para"><strong class="EmphasisTypeBold ">Abstract representations</strong>. GoogLeNet struggles with images that depict objects of interest in an abstract form, such as 3D-rendered images, paintings, sketches, plush toys, or statues. An example is the abstract shape of a bow drawn with a light source in night photography, a 3D-rendered robotic scorpion, or a shadow on the ground, of a child on a swing. We attribute approximately 6 (<span class="InlineEquation" id="IEq464">\(6\,\%\)</span>) of GoogLeNet errors to this type of error and believe that humans are significantly more robust, with no such errors seen in our sample.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(4)</span><div class="ItemContent"><p id="Par199" class="Para"><strong class="EmphasisTypeBold ">Miscellaneous sources</strong> Additional sources of error that occur relatively infrequently include extreme closeups of parts of an object, unconventional viewpoints such as a rotated image, images that can significantly benefit from the ability to read text (e.g. a featureless container identifying itself as “face powder”), objects with heavy occlusions, and images that depict a collage of multiple images. In general, we found that humans are more robust to all of these types of error.</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div><em class="EmphasisTypeItalic ">Types of Errors that the Human is More Susceptible to than the Computer</em><div class="OrderedList"><ol><li class="ListItem"><span class="ItemNumber">(1)</span><div class="ItemContent"><p id="Par200" class="Para"><strong class="EmphasisTypeBold ">Fine-grained recognition</strong> We found that humans are noticeably worse at fine-grained recognition (e.g. dogs, monkeys, snakes, birds), even when they are in clear view. To understand the difficulty, consider that there are more than 120 species of dogs in the dataset. We estimate that 28 (<span class="InlineEquation" id="IEq465">\(37\,\%\)</span>) of the human errors fall into this category, while only 7 (<span class="InlineEquation" id="IEq466">\(7\,\%\)</span>) of GoogLeNet errors do.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(2)</span><div class="ItemContent"><p id="Par201" class="Para"><strong class="EmphasisTypeBold ">Class unawareness</strong> The annotator may sometimes be unaware of the ground truth class present as a label option. When pointed out as an ILSVRC class, it is usually clear that the label applies to the image. These errors get progressively less frequent as the annotator becomes more familiar with ILSVRC classes. Approximately 18 (<span class="InlineEquation" id="IEq467">\(24\,\%\)</span>) of the human errors fall into this category.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><span class="ItemNumber">(3)</span><div class="ItemContent"><p id="Par202" class="Para"><strong class="EmphasisTypeBold ">Insufficient training data</strong> Recall that the annotator is only presented with 13 examples of a class under every category name. However, 13 images are not always enough to adequately convey the allowed class variations. For example, a brown dog can be incorrectly dismissed as a “Kelpie” if all examples of a “Kelpie” feature a dog with black coat. However, if more than 13 images were listed it would have become clear that a “Kelpie” may have brown coat. Approximately 4 (<span class="InlineEquation" id="IEq468">\(5\,\%\)</span>) of human errors fall into this category.</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div></div></section><section id="Sec42" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">6.4.3 </span>Conclusions from Human Image Classification Experiments</h4><p id="Par203" class="Para">We investigated the performance of trained human annotators on a sample of 1500 ILSVRC test set images. Our results indicate that a trained human annotator is capable of outperforming the best model (GoogLeNet) by approximately <span class="InlineEquation" id="IEq469">\(1.7\,\%\)</span> (<span class="InlineEquation" id="IEq470">\(p = 0.022\)</span>).</p><p id="Par204" class="Para">We expect that some sources of error may be relatively easily eliminated (e.g. robustness to filters, rotations, collages, effectively reasoning over multiple scales), while others may prove more elusive (e.g. identifying abstract representations of objects). On the other hand, a large majority of human errors come from fine-grained categories and class unawareness. We expect that the former can be significantly reduced with fine-grained expert annotators, while the latter could be reduced with more practice and greater familiarity with ILSVRC classes. Our results also hint that human errors are not strongly correlated and that human ensembles may further reduce human error rate.</p><p id="Par205" class="Para">It is clear that humans will soon outperform state-of-the-art ILSVRC image classification models only by use of significant effort, expertise, and time. One interesting follow-up question for future investigation is how computer-level accuracy compares with human-level accuracy on more complex image understanding tasks.</p></section></section></div></section><section id="Sec43" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">7 </span>Conclusions</h2><div class="content"><p id="Par206" class="Para">In this paper we described the large-scale data collection process of ILSVRC, provided a summary of the most successful algorithms on this data, and analyzed the success and failure modes of these algorithms. In this section we discuss some of the key lessons we learned over the years of ILSVRC, strive to address the key criticisms of the datasets and the challenges we encountered over the years, and conclude by looking forward into the future.</p><section id="Sec44" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">7.1 </span>Lessons Learned</h3><p id="Par207" class="Para">The key lesson of collecting the datasets and running the challenges for 5&nbsp;years is this: All human intelligence tasks need to be exceptionally well-designed. We learned this lesson both when annotating the dataset using Amazon Mechanical Turk workers (Sect.&nbsp;<span class="InternalRef"><a href="#Sec8">3</a></span>) and even when trying to evaluate human-level image classification accuracy using expert labelers (Sect.&nbsp;<span class="InternalRef"><a href="#Sec39">6.4</a></span>). The first iteration of the labeling interface was always bad—generally meaning <em class="EmphasisTypeItalic ">completely unusable</em>. If there was any inherent ambiguity in the questions posed (and there almost always was), workers found it and accuracy suffered. If there is one piece of advice we can offer to future research, it is to very carefully design, continuously monitor, and extensively sanity-check all crowdsourcing tasks.</p><p id="Par208" class="Para">The other lesson, already well-known to large-scale researchers, is this: Scaling up the dataset always reveals unexpected challenges. From designing complicated multi-step annotation strategies (Sect.&nbsp;<span class="InternalRef"><a href="#Sec15">3.2.1</a></span>) to having to modify the evaluation procedure (Sect.&nbsp;<span class="InternalRef"><a href="#Sec22">4</a></span>), we had to continuously adjust to the large-scale setting. On the plus side, of course, the major breakthroughs in object recognition accuracy (Sect.&nbsp;<span class="InternalRef"><a href="#Sec26">5</a></span>) and the analysis of the strength and weaknesses of current algorithms as a function of object class properties (Sect.&nbsp;<span class="InternalRef"><a href="#Sec34">6.3</a></span>) would never have been possible on a smaller scale.</p></section><section id="Sec45" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">7.2 </span>Criticism</h3><p id="Par209" class="Para">In the past 5&nbsp;years, we encountered three major criticisms of the ILSVRC dataset and the corresponding challenge: (1) the ILSVRC dataset is insufficiently challenging, (2) the ILSVRC dataset contains annotation errors, and (3) the rules of ILSVRC competition are too restrictive. We discuss these in order.</p><p id="Par210" class="Para">The first criticism is that the objects in the dataset tend to be large and centered in the images, making the dataset insufficiently challenging. In Sect.&nbsp;<span class="InternalRef"><a href="#Sec16">3.2.2</a></span> and&nbsp;<span class="InternalRef"><a href="#Sec21">3.3.4</a></span> we tried to put those concerns to rest by analyzing the statistics of the ILSVRC dataset and concluding that it is comparable with, and in many cases much more challenging than, the long-standing PASCAL VOC benchmark (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR19">2010</a></span>).</p><p id="Par211" class="Para">The second is regarding the errors in ground truth labeling. We went through several rounds of in-house post-processing of the annotations obtained using crowdsourcing, and corrected many common sources of errors (e.g., Appendix&nbsp;<span class="InternalRef"><a href="#Sec51">1</a></span>). The major remaining source of annotation errors stem from fine-grained object classes, e.g., labelers failing to distinguish different species of birds. This is a tradeoff that had to be made: in order to annotate data at this scale on a reasonable budget, we had to rely on non-expert crowd labelers. However, overall the dataset is encouragingly clean. By our estimates, <span class="InlineEquation" id="IEq471">\(99.7\,\%\)</span> precision is achieved in the image classification dataset (Sects.&nbsp;<span class="InternalRef"><a href="#Sec12">3.1.3</a></span>,&nbsp;<span class="InternalRef"><a href="#Sec39">6.4</a></span>) and <span class="InlineEquation" id="IEq472">\(97.9\,\%\)</span> of images that went through the bounding box annotation system have all instances of the target object class labeled with bounding boxes (Sect.&nbsp;<span class="InternalRef"><a href="#Sec15">3.2.1</a></span>).</p><p id="Par212" class="Para">The third criticism we encountered is over the rules of the competition regarding using external training data. In ILSVRC2010-2013, algorithms had to only use the provided training and validation set images and annotations for training their models. With the growth of the field of large-scale unsupervised feature learning, however, questions began to arise about what exactly constitutes “outside” data: for example, are image features trained on a large pool of “outside” images in an unsupervised fashion allowed in the competition? After much discussion, in ILSVRC2014 we took the first step towards addressing this problem. We followed the PASCAL VOC strategy and created two tracks in the competition: entries using only “provided” data and entries using “outside” data, meaning <em class="EmphasisTypeItalic ">any</em> images or annotations not provided as part of ILSVRC training or validation sets. However, in the future this strategy will likely need to be further revised as the computer vision field evolves. For example, competitions can consider allowing the use of any image features which are publically available, even if these features were learned on an external source of data.</p></section><section id="Sec46" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">7.3 </span>The Future</h3><p id="Par213" class="Para">Given the massive algorithmic breakthroughs over the past 5&nbsp;years, we are very eager to see what will happen in the next 5&nbsp;years. There are many potential directions of improvement and growth for ILSVRC and other large-scale image datasets.</p><p id="Par214" class="Para">First, continuing the trend of moving towards richer image understanding (from image classification to single-object localization to object detection), the next challenge would be to tackle pixel-level object segmentation. The recently released large-scale COCO dataset (Lin et&nbsp;al. <span class="CitationRef"><a href="#CR48">2014b</a></span>) is already taking a step in that direction.</p><p id="Par215" class="Para">Second, as datasets grow even larger in scale, it may become impossible to fully annotate them manually. The scale of ILSVRC is already imposing limits on the manual annotations that are feasible to obtain: for example, we had to restrict the number of objects labeled per image in the image classification and single-object localization datasets. In the future, with billions of images, it will become impossible to obtain even one clean label for every image. Datasets such as Yahoo’s Flickr Creative Commons 100M,<sup><a href="#Fn13" id="Fn13_source">13</a></sup> released with weak human tags but no centralized annotation, will become more common.</p><p id="Par217" class="Para">The growth of unlabeled or only partially labeled large-scale datasets implies two things. First, algorithms will have to rely more on weakly supervised training data. Second, even evaluation might have to be done <em class="EmphasisTypeItalic ">after</em> the algorithms make predictions, not before. This means that rather than evaluating <em class="EmphasisTypeItalic ">accuracy</em> (how many of the test images or objects did the algorithm get right) or <em class="EmphasisTypeItalic ">recall</em> (how many of the desired images or objects did the algorithm manage to find), both of which require a fully annotated test set, we will be focusing more on <em class="EmphasisTypeItalic ">precision</em>: of the predictions that the algorithm made, how many were deemed correct by humans.</p><p id="Par218" class="Para">We are eagerly awaiting the future development of object recognition datasets and algorithms, and are grateful that ILSVRC served as a stepping stone along this path.</p></section></div></section><section id="Footnotes" class="FootnoteSection Section1 RenderAsSection1"><h2 class="Heading">Footnotes</h2><div class="content"><ol><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a>.</span><div class="FootnoteContent" id="Fn1"><p id="Par3" class="Para">In this paper, we will be using the term <em class="EmphasisTypeItalic ">object recognition</em> broadly to encompass both <em class="EmphasisTypeItalic ">image classification</em> (a task requiring an algorithm to determine what object classes are present in the image) as well as <em class="EmphasisTypeItalic ">object detection</em> (a task requiring an algorithm to localize all objects present in the image).</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a>.</span><div class="FootnoteContent" id="Fn2"><p id="Par5" class="Para">In 2010, the test annotations were later released publicly; since then the test annotation have been kept hidden.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a>.</span><div class="FootnoteContent" id="Fn3"><p id="Par29" class="Para">In addition, ILSVRC in 2012 also included a taster fine-grained classification task, where algorithms would classify dog photographs into one of 120 dog breeds (Khosla et&nbsp;al. <span class="CitationRef"><a href="#CR42">2011</a></span>). Fine-grained classification has evolved into its own Fine-Grained classification challenge in 2013 (Berg et&nbsp;al. <span class="CitationRef"><a href="#CR8">2013</a></span>), which is outside the scope of this paper.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a>.</span><div class="FootnoteContent" id="Fn4"><p id="Par34" class="Para"><span class="ExternalRef"><a target="_top" rel="noopener noreferrer" href="http://www.flickr.com/"><span class="RefSource">www.flickr.com</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a>.</span><div class="FootnoteContent" id="Fn5"><p id="Par64" class="Para">Some datasets such as PASCAL VOC (Everingham et&nbsp;al. <span class="CitationRef"><a href="#CR19">2010</a></span>) and LabelMe (Russell et&nbsp;al. <span class="CitationRef"><a href="#CR66">2007</a></span>) are able to provide more detailed annotations: for example, marking individual object instances as being <em class="EmphasisTypeItalic ">truncated</em>. We chose not to provide this level of detail in favor of annotating more images and more object instances.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn6_source">6</a>.</span><div class="FootnoteContent" id="Fn6"><p id="Par86" class="Para">Some of the training objects are actually annotated with more detailed classes: for example, one of the 200 object classes is the category “dog,” and some training instances are annotated with the specific dog breed.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn7_source">7</a>.</span><div class="FootnoteContent" id="Fn7"><p id="Par92" class="Para">The validation/test split is consistent with ILSVRC2012: validation images of ILSVRC2012 remained in the validation set of ILSVRC2013, and ILSVRC2012 test images remained in ILSVRC2013 test set.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn8_source">8</a>.</span><div class="FootnoteContent" id="Fn8"><p id="Par116" class="Para">In this paper we focus on the mean average precision across all categories as the measure of a team’s performance. This is done for simplicity and is justified since the ordering of teams by mean average precision was always the same as the ordering by object categories won.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn9_source">9</a>.</span><div class="FootnoteContent" id="Fn9"><p id="Par129" class="Para">Table&nbsp;<span class="InternalRef"><a href="#Tab8">8</a></span> omits 4 teams which submitted results but chose not to officially participate in the challenge.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn10_source">10</a>.</span><div class="FootnoteContent" id="Fn10"><p id="Par141" class="Para">Personal communication with members of the UvA team.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn11_source">11</a>.</span><div class="FootnoteContent" id="Fn11"><p id="Par165" class="Para">For rigid versus deformable objects, the average scale in each bin is 34.1–34.2&nbsp;% for classification and localization, and 13.5–13.7&nbsp;% for detection. For texture, the average scale in each of the four bins is 31.1–31.3&nbsp;% for classification and localization, and 12.7–12.8&nbsp;% for detection.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn12_source">12</a>.</span><div class="FootnoteContent" id="Fn12"><p id="Par179" class="Para">Natural object detection classes are removed from this analysis because there are only 3 and 13 natural untextured and low-textured classes respectively, and none remain after scale normalization. All other bins contain at least 9 object classes after scale normalization.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn13_source">13</a>.</span><div class="FootnoteContent" id="Fn13"><p id="Par216" class="Para"><span class="ExternalRef"><a target="_top" rel="noopener noreferrer" href="http://webscope.sandbox.yahoo.com/catalog.php?datatype=i&amp;did=67"><span class="RefSource">http://webscope.sandbox.yahoo.com/catalog.php?datatype=i&amp;did=67</span></a></span>.</p></div></li></ol></div></section></div><section id="Notes" class="Section1 RenderAsSection1"><h2 class="Heading">Notes</h2><div class="content"><section><h3 class="Heading">Acknowledgments</h3><p class="SimplePara">We thank Stanford University, UNC Chapel Hill, Google and Facebook for sponsoring the challenges, and NVIDIA for providing computational resources to participants of ILSVRC2014. We thank our advisors over the years: Lubomir Bourdev, Alexei Efros, Derek Hoiem, Jitendra Malik, Chuck Rosenberg and Andrew Zisserman. We thank the PASCAL VOC organizers for partnering with us in running ILSVRC2010-2012. We thank all members of the Stanford vision lab for supporting the challenges and putting up with us along the way. Finally, and most importantly, we thank all researchers that have made the ILSVRC effort a success by competing in the challenges and by using the datasets to advance computer vision.</p></section></div></section><aside class="Appendix" id="App1"><section id="Sec47" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Appendix 1: ILSVRC2012-2014 Image Classification and Single-Object Localization Object Categories</h2><div class="content"><div id="Par219" class="Para"><figure class="Figure" id="Figc"><div class="MediaObject" id="MO23"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Figc_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Figc_HTML.gif" alt=""></a></div></figure></div></div></section></aside><aside class="Appendix" id="App2"><section id="Sec48" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Appendix 2: Additional Single-Object Localization Dataset Statistics</h2><div class="content"><div id="Par220" class="Para">We consider two additional metrics of object localization difficulty: chance performance of localization and the level of clutter. We use these metrics to compare ILSVRC2012-2014 single-object localization dataset to the PASCAL VOC 2012 object detection benchmark. The measures of localization difficulty are computed on the validation set of both datasets. According to both of these measures of difficulty there is a subset of ILSVRC which is as challenging as PASCAL but more than an order of magnitude greater in size. Figure&nbsp;<span class="InternalRef"><a href="#Fig16">16</a></span> shows the distributions of different properties (object scale, chance performance of localization and level of clutter) across the different classes in the two datasets.<figure class="Figure" id="Fig16"><div class="MediaObject" id="MO24"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig16_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fig16_HTML.gif" alt=""></a></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 16</span><p class="SimplePara">Distribution of various measures of localization difficulty on the ILSVRC2012-2014 single-object localization (<em class="EmphasisTypeItalic ">dark green</em>) and PASCAL VOC 2012 (<em class="EmphasisTypeItalic ">light blue</em>) validation sets. Object scale is fraction of image area occupied by an average object instance. Chance performance of localization and level of clutter are defined in Appendix&nbsp;<span class="InternalRef"><a href="#Sec48">1</a></span>. The plots on <em class="EmphasisTypeItalic ">top</em> contain the full ILSVRC validation set with 1000 classes; the plots on the <em class="EmphasisTypeItalic ">bottom</em> contain 200 ILSVRC classes with the lowest chance performance of localization. All plots contain all 20 classes of PASCAL VOC</p></div></figcaption></figure></div><div id="Par221" class="Para"><em class="EmphasisTypeItalic ">Chance Performance of Localization (CPL)</em> Chance performance on a dataset is a common metric to consider. We define the CPL measure as the expected accuracy of a detector which first randomly samples an object instance of that class and then uses its bounding box directly as the proposed localization window on all other images (after rescaling the images to the same size). Concretely, let <span class="InlineEquation" id="IEq473">\(B_1,B_2,\dots ,B_N\)</span> be all the bounding boxes of the object instances within a class, then<div id="Equ6" class="Equation EquationMathjax"><div class="EquationContent">$$\begin{aligned} \text{ CPL } = \frac{\sum _i \sum _{j \ne i} IOU(B_i,B_j)\ge 0.5}{N(N-1)} \end{aligned}$$</div> <div class="EquationNumber">(6)</div></div>Some of the most difficult ILSVRC categories to localize according to this metric are basketball, swimming trunks, ping pong ball and rubber eraser, all with less than <span class="InlineEquation" id="IEq474">\(0.2\,\%\)</span> CPL. This measure correlates strongly (<span class="InlineEquation" id="IEq475">\(\rho = 0.9\)</span>) with the average scale of the object (fraction of image occupied by object). The average CPL across the <span class="InlineEquation" id="IEq476">\(1000\)</span> ILSVRC categories is <span class="InlineEquation" id="IEq477">\(20.8\,\%\)</span>. The 20 PASCAL categories have an average CPL of <span class="InlineEquation" id="IEq478">\(8.7\,\%\)</span>, which is the same as the CPL of the <span class="InlineEquation" id="IEq479">\(562\)</span> most difficult categories of ILSVRC.</div><div id="Par222" class="Para"><em class="EmphasisTypeItalic ">Clutter</em> Intuitively, even small objects are easy to localize on a plain background. To quantify clutter we employ the objectness measure of (Alexe et&nbsp;al. <span class="CitationRef"><a href="#CR2">2012</a></span>), which is a class-generic object detector evaluating how likely a window in the image contains a coherent object (of any class) as opposed to background (sky, water, grass). For every image <span class="InlineEquation" id="IEq480">\(m\)</span> containing target object instances at positions <span class="InlineEquation" id="IEq481">\(B_1^m,B_2^m,\dots \)</span>, we use the publicly available objectness software to sample 1000 windows <span class="InlineEquation" id="IEq482">\(W_1^m,W_2^m,\dots W_{1000}^m\)</span>, in order of decreasing probability of the window containing any generic object. Let <span class="EmphasisTypeSmallCaps ">obj</span>(m) be the number of generic object-looking windows sampled before localizing an instance of the target category, i.e., <span class="InlineEquation" id="IEq483">\(\text{ obj }(m) = \min \{k: \max _i \text{ iou }(W_k^m,B_i^m) \ge 0.5\}\)</span>. For a category containing M images, we compute the average number of such windows per image and define<div id="Equ7" class="Equation EquationMathjax"><div class="EquationContent">$$\begin{aligned} \textsc {Clutter} = \log _{2} \large \left( \frac{1}{M}\sum _m \textsc {Obj}(m) \large \right) \end{aligned}$$</div> <div class="EquationNumber">(7)</div></div>The higher the clutter of a category, the harder the objects are to localize according to generic cues. If an object can’t be localized with the first 1000 windows (as is the case for <span class="InlineEquation" id="IEq484">\(1\,\%\)</span> of images on average per category in ILSVRC and <span class="InlineEquation" id="IEq485">\(5\,\%\)</span> in PASCAL), we set <span class="EmphasisTypeSmallCaps ">obj</span><span class="InlineEquation" id="IEq486">\((m)=1001\)</span>. The fact that more than <span class="InlineEquation" id="IEq487">\(95\,\%\)</span> of objects can be localized with these windows imply that the objectness cue is already quite strong, so objects that require many windows on average will be extremely difficult to detect: e.g., ping pong ball (clutter of 9.57, or 758 windows on average), basketball (clutter of 9.21), puck (clutter of 9.17) in ILSVRC. The most difficult object in PASCAL is bottle with clutter score of <span class="InlineEquation" id="IEq488">\(8.47\)</span>. On average, ILSVRC has clutter score of <span class="InlineEquation" id="IEq489">\(3.59\)</span>. The most difficult subset of ILSVRC with 250 object categories has an order of magnitude more categories and the same average amount of clutter (of <span class="InlineEquation" id="IEq490">\(5.90\)</span>) as the PASCAL dataset.</div></div></section></aside><aside class="Appendix" id="App3"><section id="Sec49" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Appendix 3: Manually Curated Queries for Obtaining Object Detection Scene Images</h2><div class="content"><div id="Par223" class="Para">In Sect.&nbsp;<span class="InternalRef"><a href="#Sec19">3.3.2</a></span> we discussed three types of queries we used for collecting the object detection images: (1) single object category name or a synonym; (2) a pair of object category names; (3) a manual query, typically targetting one or more object categories with insufficient data. Here we provide a list of the 129 manually curated queries:<figure class="Figure" id="Figd"><div class="MediaObject" id="MO27"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Figd_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Figd_HTML.gif" alt=""></a></div></figure></div></div></section></aside><aside class="Appendix" id="App4"><section id="Sec50" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Appendix 4: Hierarchy of Questions for Full Image Annotation</h2><div class="content"><p id="Par224" class="Para">The following is a hierarchy of questions manually constructed for crowdsourcing full annotation of images with the presence or absence of 200 object detection categories in ILSVRC2013 and ILSVRC2014. All questions are of the form “is there a ... in the image?” Questions marked with <span class="InlineEquation" id="IEq491">\(\bullet \)</span> are asked on every image. If the answer to a question is determined to be “no” then the answer to all descendant questions is assumed to be “no”. The 200 numbered leaf nodes correspond to the 200 object detection categories.</p><div id="Par225" class="Para">The goal in the hierarchy construction is to save cost (by asking as few questions as possible on every image) while avoiding any ambiguity in questions which would lead to false negatives during annotation. This hierarchy is not tree-structured; some questions have multiple parents.<figure class="Figure" id="Fige"><div class="MediaObject" id="MO28"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fige_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Fige_HTML.gif" alt=""></a></div></figure><figure class="Figure" id="Figf"><div class="MediaObject" id="MO29"><a href="https://static-content-springer-com.proxy.library.cornell.edu/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Figf_HTML.gif" target="_top" rel="noopener noreferrer"><span class="u-screenreader-only">Open image in new window</span><img src="11263_2015_816_Figf_HTML.gif" alt=""></a></div></figure></div></div></section></aside><aside class="Appendix" id="App5"><section id="Sec51" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Appendix 5: Modification to Bounding Box System for Object Detection</h2><div class="content"><p id="Par226" class="Para">The bounding box annotation system described in Sect.&nbsp;<span class="InternalRef"><a href="#Sec15">3.2.1</a></span> is used for annotating images for both the single-object localization dataset and the object detection dataset. However, two additional manual post-processing are needed to ensure accuracy in the object detection scenario:</p><p id="Par227" class="Para"><em class="EmphasisTypeItalic ">Ambiguous Objects</em> The first common source of error was that workers were not able to accurately differentiate some object classes during annotation. Some commonly confused labels were seal and sea otter, backpack and purse, banjo and guitar, violin and cello, brass instruments (trumpet, trombone, french horn and brass), flute and oboe, ladle and spatula. Despite our best efforts (providing positive and negative example images in the annotation task, adding text explanations to alert the user to the distinction between these categories) these errors persisted.</p><p id="Par228" class="Para">In the single-object localization setting, this problem was not as prominent for two reasons. First, the way the data was collected imposed a strong prior on the object class which was present. Second, since only one object category needed to be annotated per image, ambiguous images could be discarded: for example, if workers couldn’t agree on whether or not a trumpet was in fact present, this image could simply be removed. In contrast, for the object detection setting consensus had to be reached for all target categories on all images.</p><p id="Par229" class="Para">To fix this problem, once bounding box annotations were collected we manually looked through all cases where the bounding boxes for two different object classes had significant overlap with each other (about <span class="InlineEquation" id="IEq744">\(3\,\%\)</span> of the collected boxes). About a quarter of these boxes were found to correspond to incorrect objects and were removed. Crowdsourcing this post-processing step (with very stringent accuracy constraints) would be possible but it occurred in few enough cases that it was faster (and more accurate) to do this in-house.</p><p id="Par230" class="Para"><em class="EmphasisTypeItalic ">Duplicate Annotations</em> The second common source of error were duplicate bounding boxes drawn on the same object instance. Despite instructions not to draw more than one bounding box around the same object instance and constraints in the annotation UI enforcing at least a 5 pixel difference between different bounding boxes, these errors persisted. One reason was that sometimes the initial bounding box was not perfect and subsequent labelers drew a slightly improved alternative.</p><p id="Par231" class="Para">This type of error was also present in the single-object localization scenario but was not a major cause for concern. A duplicate bounding box is a slightly perturbed but still correct positive example, and single-object localization is only concerned with correctly localizing one object instance. For the detection task algorithms are evaluated on the ability to localize <em class="EmphasisTypeItalic ">every</em> object instance, and penalized for duplicate detections, so it is imperative that these labeling errors are corrected (even if they only appear in about <span class="InlineEquation" id="IEq745">\(0.6\,\%\)</span> of cases).</p><p id="Par232" class="Para">Approximately <span class="InlineEquation" id="IEq746">\(1\,\%\)</span> of bounding boxes were found to have significant overlap of more than <span class="InlineEquation" id="IEq747">\(50\,\%\)</span> with another bounding box of the same object class.We again manually verified all of these cases in-house. In approximately <span class="InlineEquation" id="IEq748">\(40\,\%\)</span> of the cases the two bounding boxes correctly corresponded to different people in a crowd, to stacked plates, or to musical instruments nearby in an orchestra. In the other <span class="InlineEquation" id="IEq749">\(60\,\%\)</span> of cases one of the boxes was randomly removed.</p><p id="Par233" class="Para">These verification steps complete the annotation procedure of bounding boxes around every instance of every object class in validation, test and a subset of training images for the detection task.</p><p id="Par234" class="Para"><em class="EmphasisTypeItalic ">Training Set Annotation</em> With the optimized algorithm of Sect.&nbsp;<span class="InternalRef"><a href="#Sec20">3.3.3</a></span> we fully annotated the validation and test sets. However, annotating <em class="EmphasisTypeItalic ">all</em> training images with all target object classes was still a budget challenge. Positive training images taken from the single-object localization dataset already had bounding box annotations of all instances of one object class on each image. We extended the existing annotations to the detection dataset by making two modification. First, we corrected any bounding box omissions resulting from merging fine-grained categories: i.e., if an image belonged to the “dalmatian” category and all instances of “dalmatian” were annotated with bounding boxes for single-object localization, we ensured that all remaining “dog” instances are also annotated for the object detection task. Second, we collected significantly more training data for the person class because the existing annotation set was not diverse enough to be representative (the only people categories in the single-object localization task are scuba diver, groom, and ballplayer). To compensate, we additionally annotated people in a large fraction of the existing training set images.</p></div></section></aside><aside class="Appendix" id="App6"><section id="Sec52" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading">Appendix 6: Competition Protocol</h2><div class="content"><p id="Par235" class="Para"><em class="EmphasisTypeItalic ">Competition Format</em> At the beginning of the competition period each year we release the new training/validation/test images, training/validation annotations, and competition specification for the year. We then specify a deadline for submission, usually approximately 4 months after the release of data. Teams are asked to upload a text file of their predicted annotations on test images by this deadline to a provided server. We then evaluate all submissions and release the results.</p><p id="Par236" class="Para">For every task we released code that takes a text file of automatically generated image annotations and compares it with the ground truth annotations to return a quantitative measure of algorithm accuracy. Teams can use this code to evaluate their performance on the validation data.</p><p id="Par237" class="Para">As described in Everingham et&nbsp;al. (<span class="CitationRef"><a href="#CR20">2014</a></span>), there are three options for measuring performance on test data: (i) Release test images and annotations, and allow participants to assess performance themselves; (ii) Release test images but not test annotations—participants submit results and organizers assess performance; (iii) Neither test images nor annotations are released—participants submit software and organizers run it on new data and assess performance. In line with the PASCAL VOC choice, we opted for option (ii). Option (i) allows too much leeway in overfitting to the test data; option (iii) is infeasible, especially given the scale of our test set (40K–100K images).</p><p id="Par238" class="Para">We released ILSVRC2010 test annotations for the image classification task, but all other test annotations have remained hidden to discourage fine-tuning results on the test data.</p><p id="Par239" class="Para"><em class="EmphasisTypeItalic ">Evaluation Protocol After the Challenge</em> After the challenge period we set up an automatic evaluation server that researchers can use throughout the year to continue evaluating their algorithms against the ground truth test annotations. We limit teams to 2 submissions per week to discourage parameter tuning on the test data, and in practice we have never had a problem with researchers abusing the system.</p></div></section></aside><section class="Section1 RenderAsSection1" id="Bib1" tabindex="-1"><h3 class="Heading">References</h3><div class="content"><ol class="BibliographyWrapper"><li class="Citation"><div class="CitationContent" id="CR1">Ahonen, T., Hadid, A., &amp; Pietikinen, M. (2006). Face description with local binary patterns: Application to face recognition. <em class="EmphasisTypeItalic ">Pattern Analysis and Machine Intelligence</em>, <em class="EmphasisTypeItalic ">28</em>(14), 2037–2041.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1109/TPAMI.2006.244"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Face%20description%20with%20local%20binary%20patterns%3A%20Application%20to%20face%20recognition&amp;author=T.%20Ahonen&amp;author=A.%20Hadid&amp;author=M.%20Pietikinen&amp;journal=Pattern%20Analysis%20and%20Machine%20Intelligence&amp;volume=28&amp;issue=14&amp;pages=2037-2041&amp;publication_year=2006"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR2">Alexe, B., Deselares, T., &amp; Ferrari, V. (2012). Measuring the objectness of image windows. <em class="EmphasisTypeItalic ">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em class="EmphasisTypeItalic ">34</em>(11), 2189–2202.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1109/TPAMI.2012.28"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Measuring%20the%20objectness%20of%20image%20windows&amp;author=B.%20Alexe&amp;author=T.%20Deselares&amp;author=V.%20Ferrari&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;volume=34&amp;issue=11&amp;pages=2189-2202&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR3">Arandjelovic, R., &amp; Zisserman, A. (2012). Three things everyone should know to improve object retrieval. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Arandjelovic%2C%20R.%2C%20%26%20Zisserman%2C%20A.%20%282012%29.%20Three%20things%20everyone%20should%20know%20to%20improve%20object%20retrieval.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR4">Arbeláez, P., Pont-Tuset, J., Barron, J., Marques, F., &amp; Malik, J. (2014). Multiscale combinatorial grouping. In <em class="EmphasisTypeItalic ">Computer vision and pattern recognition</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Arbel%C3%A1ez%2C%20P.%2C%20Pont-Tuset%2C%20J.%2C%20Barron%2C%20J.%2C%20Marques%2C%20F.%2C%20%26%20Malik%2C%20J.%20%282014%29.%20Multiscale%20combinatorial%20grouping.%20In%20Computer%20vision%20and%20pattern%20recognition."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR5">Arbelaez, P., Maire, M., Fowlkes, C., &amp; Malik, J. (2011). Contour detection and hierarchical image segmentation. <em class="EmphasisTypeItalic ">IEEE Transaction on Pattern Analysis and Machine Intelligence</em>, <em class="EmphasisTypeItalic ">33</em>, 898–916.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1109/TPAMI.2010.161"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Contour%20detection%20and%20hierarchical%20image%20segmentation&amp;author=P.%20Arbelaez&amp;author=M.%20Maire&amp;author=C.%20Fowlkes&amp;author=J.%20Malik&amp;journal=IEEE%20Transaction%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;volume=33&amp;pages=898-916&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR6">Batra, D., Agrawal, H., Banik, P., Chavali, N., Mathialagan, C. S., &amp; Alfadda, A. (2013). Cloudcv: Large-scale distributed computer vision as a cloud service.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Batra%2C%20D.%2C%20Agrawal%2C%20H.%2C%20Banik%2C%20P.%2C%20Chavali%2C%20N.%2C%20Mathialagan%2C%20C.%20S.%2C%20%26%20Alfadda%2C%20A.%20%282013%29.%20Cloudcv%3A%20Large-scale%20distributed%20computer%20vision%20as%20a%20cloud%20service."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR7">Bell, S., Upchurch, P., Snavely, N., &amp; Bala, K. (2013). OpenSurfaces: A richly annotated catalog of surface appearance. In <em class="EmphasisTypeItalic ">ACM transactions on graphics (SIGGRAPH)</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Bell%2C%20S.%2C%20Upchurch%2C%20P.%2C%20Snavely%2C%20N.%2C%20%26%20Bala%2C%20K.%20%282013%29.%20OpenSurfaces%3A%20A%20richly%20annotated%20catalog%20of%20surface%20appearance.%20In%20ACM%20transactions%20on%20graphics%20%28SIGGRAPH%29."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR8">Berg, A., Farrell, R., Khosla, A., Krause, J., Fei-Fei, L., Li, J., &amp; Maji, S. (2013). Fine-grained competition. <span class="ExternalRef"><a target="_top" rel="noopener noreferrer" href="https://sites-google-com.proxy.library.cornell.edu/site/fgcomp2013/"><span class="RefSource">https://sites-google-com.proxy.library.cornell.edu/site/fgcomp2013/</span></a></span>.<span class="Occurrences"></span></div></li><li class="Citation"><div class="CitationContent" id="CR9">Chatfield, K., Simonyan, K., Vedaldi, A., &amp; Zisserman, A. (2014). Return of the devil in the details: Delving deep into convolutional nets. CoRR, abs/1405.3531.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Chatfield%2C%20K.%2C%20Simonyan%2C%20K.%2C%20Vedaldi%2C%20A.%2C%20%26%20Zisserman%2C%20A.%20%282014%29.%20Return%20of%20the%20devil%20in%20the%20details%3A%20Delving%20deep%20into%20convolutional%20nets.%20CoRR%2C%20abs%2F1405.3531."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR10">Chen, Q., Song, Z., Huang, Z., Hua, Y., &amp; Yan, S. (2014). Contextualizing object detection and classification. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Chen%2C%20Q.%2C%20Song%2C%20Z.%2C%20Huang%2C%20Z.%2C%20Hua%2C%20Y.%2C%20%26%20Yan%2C%20S.%20%282014%29.%20Contextualizing%20object%20detection%20and%20classification.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR11">Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., &amp; Singer, Y. (2006). Online passive-aggressive algorithms. <em class="EmphasisTypeItalic ">Journal of Machine Learning Research</em>, <em class="EmphasisTypeItalic ">7</em>, 551–585.<span class="Occurrences"><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_top" rel="noopener noreferrer" href="http://www.emis.de.proxy.library.cornell.edu/MATH-item?1222.68177"><span><span>MATH</span></span></a></span><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_top" rel="noopener noreferrer" href="http://www.ams.org.proxy.library.cornell.edu/mathscinet-getitem?mr=2274378"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Online%20passive-aggressive%20algorithms&amp;author=K.%20Crammer&amp;author=O.%20Dekel&amp;author=J.%20Keshet&amp;author=S.%20Shalev-Shwartz&amp;author=Y.%20Singer&amp;journal=Journal%20of%20Machine%20Learning%20Research&amp;volume=7&amp;pages=551-585&amp;publication_year=2006"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR12">Criminisi, A. (2004). Microsoft Research Cambridge (MSRC) object recognition image database (version 2.0). <span class="ExternalRef"><a target="_top" rel="noopener noreferrer" href="http://research.microsoft.com/vision/cambridge/recognition"><span class="RefSource">http://research.microsoft.com/vision/cambridge/recognition</span></a></span>.<span class="Occurrences"></span></div></li><li class="Citation"><div class="CitationContent" id="CR13">Dean, T., Ruzon, M., Segal, M., Shlens, J., Vijayanarasimhan, S., &amp; Yagnik, J. (2013). Fast, accurate detection of 100,000 object classes on a single machine. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Dean%2C%20T.%2C%20Ruzon%2C%20M.%2C%20Segal%2C%20M.%2C%20Shlens%2C%20J.%2C%20Vijayanarasimhan%2C%20S.%2C%20%26%20Yagnik%2C%20J.%20%282013%29.%20Fast%2C%20accurate%20detection%20of%20100%2C000%20object%20classes%20on%20a%20single%20machine.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR14">Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Deng%2C%20J.%2C%20Dong%2C%20W.%2C%20Socher%2C%20R.%2C%20Li%2C%20L.-J.%2C%20Li%2C%20K.%2C%20%26%20Fei-Fei%2C%20L.%20%282009%29.%20ImageNet%3A%20A%20large-scale%20hierarchical%20image%20database.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR15">Deng, J., Russakovsky, O., Krause, J., Bernstein, M., Berg, A. C., &amp; Fei-Fei, L. (2014). Scalable multi-label annotation. In <em class="EmphasisTypeItalic ">CHI</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Deng%2C%20J.%2C%20Russakovsky%2C%20O.%2C%20Krause%2C%20J.%2C%20Bernstein%2C%20M.%2C%20Berg%2C%20A.%20C.%2C%20%26%20Fei-Fei%2C%20L.%20%282014%29.%20Scalable%20multi-label%20annotation.%20In%20CHI."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR16">Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., &amp; Darrell, T. (2013). Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Donahue%2C%20J.%2C%20Jia%2C%20Y.%2C%20Vinyals%2C%20O.%2C%20Hoffman%2C%20J.%2C%20Zhang%2C%20N.%2C%20Tzeng%2C%20E.%2C%20%26%20Darrell%2C%20T.%20%282013%29.%20Decaf%3A%20A%20deep%20convolutional%20activation%20feature%20for%20generic%20visual%20recognition.%20CoRR%2C%20abs%2F1310.1531."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR17">Dubout, C., &amp; Fleuret, F. (2012). Exact acceleration of linear object detectors. In <em class="EmphasisTypeItalic ">Proceedings of the European conference on computer vision (ECCV)</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Dubout%2C%20C.%2C%20%26%20Fleuret%2C%20F.%20%282012%29.%20Exact%20acceleration%20of%20linear%20object%20detectors.%20In%20Proceedings%20of%20the%20European%20conference%20on%20computer%20vision%20%28ECCV%29."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR18">Everingham, M., Gool, L. V., Williams, C., Winn, J., &amp; Zisserman, A. (2005–2012). PASCAL Visual Object Classes Challenge (VOC). <span class="ExternalRef"><a target="_top" rel="noopener noreferrer" href="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html"><span class="RefSource">http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html</span></a></span>.<span class="Occurrences"></span></div></li><li class="Citation"><div class="CitationContent" id="CR19">Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., &amp; Zisserman, A. (2010). The Pascal Visual Object Classes (VOC) challenge. <em class="EmphasisTypeItalic ">International Journal of Computer Vision</em>, <em class="EmphasisTypeItalic ">88</em>(2), 303–338.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1007/s11263-009-0275-4"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=The%20Pascal%20Visual%20Object%20Classes%20%28VOC%29%20challenge&amp;author=M.%20Everingham&amp;author=L.%20Gool&amp;author=CKI.%20Williams&amp;author=J.%20Winn&amp;author=A.%20Zisserman&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;volume=88&amp;issue=2&amp;pages=303-338&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR20">Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C. K. I., Winn, J., &amp; Zisserman, A. (2014). The Pascal Visual Object Classes (VOC) challenge—A retrospective. <em class="EmphasisTypeItalic ">International Journal of Computer Vision</em>, <em class="EmphasisTypeItalic ">111</em>, 98–136.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1007/s11263-014-0733-5"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=The%20Pascal%20Visual%20Object%20Classes%20%28VOC%29%20challenge%E2%80%94A%20retrospective&amp;author=M.%20Everingham&amp;author=SMA.%20Eslami&amp;author=L.%20Gool&amp;author=CKI.%20Williams&amp;author=J.%20Winn&amp;author=A.%20Zisserman&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;volume=111&amp;pages=98-136&amp;publication_year=2014"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR21">Fei-Fei, L., &amp; Perona, P. (2005). A Bayesian hierarchical model for learning natural scene categories. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Fei-Fei%2C%20L.%2C%20%26%20Perona%2C%20P.%20%282005%29.%20A%20Bayesian%20hierarchical%20model%20for%20learning%20natural%20scene%20categories.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR22">Fei-Fei, L., Fergus, R., &amp; Perona, P. (2004). Learning generative visual models from few examples: An incremental bayesian approach tested on 101 object categories. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Fei-Fei%2C%20L.%2C%20Fergus%2C%20R.%2C%20%26%20Perona%2C%20P.%20%282004%29.%20Learning%20generative%20visual%20models%20from%20few%20examples%3A%20An%20incremental%20bayesian%20approach%20tested%20on%20101%20object%20categories.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR23">Felzenszwalb, P., Girshick, R., McAllester, D., &amp; Ramanan, D. (2010). Object detection with discriminatively trained part based models. <em class="EmphasisTypeItalic ">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em class="EmphasisTypeItalic ">32</em>(9), 1627–1645.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1109/TPAMI.2009.167"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Object%20detection%20with%20discriminatively%20trained%20part%20based%20models&amp;author=P.%20Felzenszwalb&amp;author=R.%20Girshick&amp;author=D.%20McAllester&amp;author=D.%20Ramanan&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;volume=32&amp;issue=9&amp;pages=1627-1645&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR24">Frome, A., Corrado, G., Shlens, J., Bengio, S., Dean, J., Ranzato, M., &amp; Mikolov, T. (2013). Devise: A deep visual-semantic embedding model. In <em class="EmphasisTypeItalic ">Advances in neural information processing systems, NIPS</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Frome%2C%20A.%2C%20Corrado%2C%20G.%2C%20Shlens%2C%20J.%2C%20Bengio%2C%20S.%2C%20Dean%2C%20J.%2C%20Ranzato%2C%20M.%2C%20%26%20Mikolov%2C%20T.%20%282013%29.%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model.%20In%20Advances%20in%20neural%20information%20processing%20systems%2C%20NIPS."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR25">Geiger, A., Lenz, P., Stiller, C., &amp; Urtasun, R. (2013). Vision meets robotics: The kitti dataset. <em class="EmphasisTypeItalic ">International Journal of Robotics Research</em>, <em class="EmphasisTypeItalic ">32</em>, 1231–1237.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1177/0278364913491297"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Vision%20meets%20robotics%3A%20The%20kitti%20dataset&amp;author=A.%20Geiger&amp;author=P.%20Lenz&amp;author=C.%20Stiller&amp;author=R.%20Urtasun&amp;journal=International%20Journal%20of%20Robotics%20Research&amp;volume=32&amp;pages=1231-1237&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR26">Girshick, R. B., Donahue, J., Darrell, T., &amp; Malik, J. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation (v4). CoRR.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Girshick%2C%20R.%20B.%2C%20Donahue%2C%20J.%2C%20Darrell%2C%20T.%2C%20%26%20Malik%2C%20J.%20%282013%29.%20Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation%20%28v4%29.%20CoRR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR27">Girshick, R., Donahue, J., Darrell, T., &amp; Malik., J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Girshick%2C%20R.%2C%20Donahue%2C%20J.%2C%20Darrell%2C%20T.%2C%20%26%20Malik.%2C%20J.%20%282014%29.%20Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR28">Gould, S., Fulton, R., &amp; Koller, D. (2009). Decomposing a scene into geometric and semantically consistent regions. In <em class="EmphasisTypeItalic ">ICCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Gould%2C%20S.%2C%20Fulton%2C%20R.%2C%20%26%20Koller%2C%20D.%20%282009%29.%20Decomposing%20a%20scene%20into%20geometric%20and%20semantically%20consistent%20regions.%20In%20ICCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR29">Graham, B. (2013). Sparse arrays of signatures for online character recognition. CoRR.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Graham%2C%20B.%20%282013%29.%20Sparse%20arrays%20of%20signatures%20for%20online%20character%20recognition.%20CoRR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR30">Griffin, G., Holub, A., &amp; Perona, P. (2007). Caltech-256 object category dataset. Technical report 7694, Caltech.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Griffin%2C%20G.%2C%20Holub%2C%20A.%2C%20%26%20Perona%2C%20P.%20%282007%29.%20Caltech-256%20object%20category%20dataset.%20Technical%20report%207694%2C%20Caltech."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR31">Harada, T., &amp; Kuniyoshi, Y. (2012). Graphical Gaussian vector for image categorization. In <em class="EmphasisTypeItalic ">NIPS</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Harada%2C%20T.%2C%20%26%20Kuniyoshi%2C%20Y.%20%282012%29.%20Graphical%20Gaussian%20vector%20for%20image%20categorization.%20In%20NIPS."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR32">Harel, J., Koch, C., &amp; Perona, P. (2007). Graph-based visual saliency. In <em class="EmphasisTypeItalic ">NIPS</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Harel%2C%20J.%2C%20Koch%2C%20C.%2C%20%26%20Perona%2C%20P.%20%282007%29.%20Graph-based%20visual%20saliency.%20In%20NIPS."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR33">He, K., Zhang, X., Ren, S., &amp; Su, J. (2014). Spatial pyramid pooling in deep convolutional networks for visual recognition. In <em class="EmphasisTypeItalic ">ECCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=He%2C%20K.%2C%20Zhang%2C%20X.%2C%20Ren%2C%20S.%2C%20%26%20Su%2C%20J.%20%282014%29.%20Spatial%20pyramid%20pooling%20in%20deep%20convolutional%20networks%20for%20visual%20recognition.%20In%20ECCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR34">Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Hinton%2C%20G.%20E.%2C%20Srivastava%2C%20N.%2C%20Krizhevsky%2C%20A.%2C%20Sutskever%2C%20I.%2C%20%26%20Salakhutdinov%2C%20R.%20%282012%29.%20Improving%20neural%20networks%20by%20preventing%20co-adaptation%20of%20feature%20detectors.%20CoRR%2C%20abs%2F1207.0580."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR35">Hoiem, D., Chodpathumwan, Y., &amp; Dai, Q. (2012). Diagnosing error in object detectors. In <em class="EmphasisTypeItalic ">ECCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Hoiem%2C%20D.%2C%20Chodpathumwan%2C%20Y.%2C%20%26%20Dai%2C%20Q.%20%282012%29.%20Diagnosing%20error%20in%20object%20detectors.%20In%20ECCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR36">Howard, A. (2014). Some improvements on deep convolutional neural network based image classification. In <em class="EmphasisTypeItalic ">ICLR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Howard%2C%20A.%20%282014%29.%20Some%20improvements%20on%20deep%20convolutional%20neural%20network%20based%20image%20classification.%20In%20ICLR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR37">Huang, G. B., Ramesh, M., Berg, T., &amp; Learned-Miller, E. (2007). Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical report 07–49, University of Massachusetts, Amherst.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Huang%2C%20G.%20B.%2C%20Ramesh%2C%20M.%2C%20Berg%2C%20T.%2C%20%26%20Learned-Miller%2C%20E.%20%282007%29.%20Labeled%20faces%20in%20the%20wild%3A%20A%20database%20for%20studying%20face%20recognition%20in%20unconstrained%20environments.%20Technical%20report%2007%E2%80%9349%2C%20University%20of%20Massachusetts%2C%20Amherst."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR38">Iandola, F. N., Moskewicz, M. W., Karayev, S., Girshick, R. B., Darrell, T., &amp; Keutzer, K. (2014). Densenet: Implementing efficient convnet descriptor pyramids. CoRR.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Iandola%2C%20F.%20N.%2C%20Moskewicz%2C%20M.%20W.%2C%20Karayev%2C%20S.%2C%20Girshick%2C%20R.%20B.%2C%20Darrell%2C%20T.%2C%20%26%20Keutzer%2C%20K.%20%282014%29.%20Densenet%3A%20Implementing%20efficient%20convnet%20descriptor%20pyramids.%20CoRR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR39">Jia, Y. (2013). Caffe: An open source convolutional architecture for fast feature embedding. <span class="ExternalRef"><a target="_top" rel="noopener noreferrer" href="http://caffe.berkeleyvision.org/"><span class="RefSource">http://caffe.berkeleyvision.org/</span></a></span>.<span class="Occurrences"></span></div></li><li class="Citation"><div class="CitationContent" id="CR40">Jojic, N., Frey, B. J., &amp; Kannan, A. (2003). Epitomic analysis of appearance and shape. In <em class="EmphasisTypeItalic ">ICCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Jojic%2C%20N.%2C%20Frey%2C%20B.%20J.%2C%20%26%20Kannan%2C%20A.%20%282003%29.%20Epitomic%20analysis%20of%20appearance%20and%20shape.%20In%20ICCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR41">Kanezaki, A., Inaba, S., Ushiku, Y., Yamashita, Y., Muraoka, H., Kuniyoshi, Y., &amp; Harada, T. (2014). Hard negative classes for multiple object detection. In <em class="EmphasisTypeItalic ">ICRA</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Kanezaki%2C%20A.%2C%20Inaba%2C%20S.%2C%20Ushiku%2C%20Y.%2C%20Yamashita%2C%20Y.%2C%20Muraoka%2C%20H.%2C%20Kuniyoshi%2C%20Y.%2C%20%26%20Harada%2C%20T.%20%282014%29.%20Hard%20negative%20classes%20for%20multiple%20object%20detection.%20In%20ICRA."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR42">Khosla, A., Jayadevaprakash, N., Yao, B., &amp; Fei-Fei, L. (2011). Novel dataset for fine-grained image categorization. In <em class="EmphasisTypeItalic ">First workshop on fine-grained visual categorization, CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Khosla%2C%20A.%2C%20Jayadevaprakash%2C%20N.%2C%20Yao%2C%20B.%2C%20%26%20Fei-Fei%2C%20L.%20%282011%29.%20Novel%20dataset%20for%20fine-grained%20image%20categorization.%20In%20First%20workshop%20on%20fine-grained%20visual%20categorization%2C%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR43">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In <em class="EmphasisTypeItalic ">NIPS</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Krizhevsky%2C%20A.%2C%20Sutskever%2C%20I.%2C%20%26%20Hinton%2C%20G.%20%282012%29.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks.%20In%20NIPS."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR44">Kuettel, D., Guillaumin, M., &amp; Ferrari, V. (2012). Segmentation propagation in ImageNet. In <em class="EmphasisTypeItalic ">ECCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Kuettel%2C%20D.%2C%20Guillaumin%2C%20M.%2C%20%26%20Ferrari%2C%20V.%20%282012%29.%20Segmentation%20propagation%20in%20ImageNet.%20In%20ECCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR45">Lazebnik, S., Schmid, C., &amp; Ponce, J. (2006). Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Lazebnik%2C%20S.%2C%20Schmid%2C%20C.%2C%20%26%20Ponce%2C%20J.%20%282006%29.%20Beyond%20bags%20of%20features%3A%20Spatial%20pyramid%20matching%20for%20recognizing%20natural%20scene%20categories.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR46">Lin, M., Chen, Q., &amp; Yan, S. (2014a). Network in network. In <em class="EmphasisTypeItalic ">ICLR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Lin%2C%20M.%2C%20Chen%2C%20Q.%2C%20%26%20Yan%2C%20S.%20%282014a%29.%20Network%20in%20network.%20In%20ICLR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR47">Lin, Y., Lv, F., Cao, L., Zhu, S., Yang, M., Cour, T., Yu, K., &amp; Huang, T. (2011). Large-scale image classification: Fast feature extraction and SVM training. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Lin%2C%20Y.%2C%20Lv%2C%20F.%2C%20Cao%2C%20L.%2C%20Zhu%2C%20S.%2C%20Yang%2C%20M.%2C%20Cour%2C%20T.%2C%20Yu%2C%20K.%2C%20%26%20Huang%2C%20T.%20%282011%29.%20Large-scale%20image%20classification%3A%20Fast%20feature%20extraction%20and%20SVM%20training.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR48">Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollr, P., &amp; Zitnick, C. L. (2014b). Microsoft COCO: Common objects in context. In <em class="EmphasisTypeItalic ">ECCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Lin%2C%20T.-Y.%2C%20Maire%2C%20M.%2C%20Belongie%2C%20S.%2C%20Hays%2C%20J.%2C%20Perona%2C%20P.%2C%20Ramanan%2C%20D.%2C%20Dollr%2C%20P.%2C%20%26%20Zitnick%2C%20C.%20L.%20%282014b%29.%20Microsoft%20COCO%3A%20Common%20objects%20in%20context.%20In%20ECCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR49">Liu, C., Yuen, J., &amp; Torralba, A. (2011). Nonparametric scene parsing via label transfer. <em class="EmphasisTypeItalic ">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em class="EmphasisTypeItalic ">32</em>, 2368–2382.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1109/TPAMI.2011.131"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Nonparametric%20scene%20parsing%20via%20label%20transfer&amp;author=C.%20Liu&amp;author=J.%20Yuen&amp;author=A.%20Torralba&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;volume=32&amp;pages=2368-2382&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR50">Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. <em class="EmphasisTypeItalic ">International Journal of Computer Vision</em>, <em class="EmphasisTypeItalic ">60</em>(2), 91–110.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1023/B%3AVISI.0000029664.99615.94"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;author=DG.%20Lowe&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;volume=60&amp;issue=2&amp;pages=91-110&amp;publication_year=2004"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR51">Maji, S., &amp; Malik, J. (2009). Object detection using a max-margin hough transform. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Maji%2C%20S.%2C%20%26%20Malik%2C%20J.%20%282009%29.%20Object%20detection%20using%20a%20max-margin%20hough%20transform.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR52">Manen, S., Guillaumin, M., &amp; Van Gool, L. (2013). Prime object proposals with randomized Prim’s algorithm. In <em class="EmphasisTypeItalic ">ICCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Manen%2C%20S.%2C%20Guillaumin%2C%20M.%2C%20%26%20Van%20Gool%2C%20L.%20%282013%29.%20Prime%20object%20proposals%20with%20randomized%20Prim%E2%80%99s%20algorithm.%20In%20ICCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR53">Mensink, T., Verbeek, J., Perronnin, F., &amp; Csurka, G. (2012). Metric learning for large scale image classification: Generalizing to new classes at near-zero cost. In <em class="EmphasisTypeItalic ">ECCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Mensink%2C%20T.%2C%20Verbeek%2C%20J.%2C%20Perronnin%2C%20F.%2C%20%26%20Csurka%2C%20G.%20%282012%29.%20Metric%20learning%20for%20large%20scale%20image%20classification%3A%20Generalizing%20to%20new%20classes%20at%20near-zero%20cost.%20In%20ECCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR54">Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient estimation of word representations in vector space. In <em class="EmphasisTypeItalic ">ICLR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Mikolov%2C%20T.%2C%20Chen%2C%20K.%2C%20Corrado%2C%20G.%2C%20%26%20Dean%2C%20J.%20%282013%29.%20Efficient%20estimation%20of%20word%20representations%20in%20vector%20space.%20In%20ICLR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR55">Miller, G. A. (1995). Wordnet: A lexical database for English. <em class="EmphasisTypeItalic ">Commun. ACM</em>, <em class="EmphasisTypeItalic ">38</em>(11), 39–41.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1145/219717.219748"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Wordnet%3A%20A%20lexical%20database%20for%20English&amp;author=GA.%20Miller&amp;journal=Commun.%20ACM&amp;volume=38&amp;issue=11&amp;pages=39-41&amp;publication_year=1995"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR56">Oliva, A., &amp; Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial envelope. In <em class="EmphasisTypeItalic ">IJCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Oliva%2C%20A.%2C%20%26%20Torralba%2C%20A.%20%282001%29.%20Modeling%20the%20shape%20of%20the%20scene%3A%20A%20holistic%20representation%20of%20the%20spatial%20envelope.%20In%20IJCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR57">Ordonez, V., Deng, J., Choi, Y., Berg, A. C., &amp; Berg, T. L. (2013). From large scale image categorization to entry-level categories. In <em class="EmphasisTypeItalic ">IEEE international conference on computer vision (ICCV)</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Ordonez%2C%20V.%2C%20Deng%2C%20J.%2C%20Choi%2C%20Y.%2C%20Berg%2C%20A.%20C.%2C%20%26%20Berg%2C%20T.%20L.%20%282013%29.%20From%20large%20scale%20image%20categorization%20to%20entry-level%20categories.%20In%20IEEE%20international%20conference%20on%20computer%20vision%20%28ICCV%29."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR58">Ouyang, W., &amp; Wang, X. (2013). Joint deep learning for pedestrian detection. In <em class="EmphasisTypeItalic ">ICCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Ouyang%2C%20W.%2C%20%26%20Wang%2C%20X.%20%282013%29.%20Joint%20deep%20learning%20for%20pedestrian%20detection.%20In%20ICCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR59">Ouyang, W., Luo, P., Zeng, X., Qiu, S., Tian, Y., Li, H., Yang, S., Wang, Z., Xiong, Y., Qian, C., Zhu, Z., Wang, R., Loy, C. C., Wang, X., &amp; Tang, X. (2014). Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection. CoRR, abs/1409.3505.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Ouyang%2C%20W.%2C%20Luo%2C%20P.%2C%20Zeng%2C%20X.%2C%20Qiu%2C%20S.%2C%20Tian%2C%20Y.%2C%20Li%2C%20H.%2C%20Yang%2C%20S.%2C%20Wang%2C%20Z.%2C%20Xiong%2C%20Y.%2C%20Qian%2C%20C.%2C%20Zhu%2C%20Z.%2C%20Wang%2C%20R.%2C%20Loy%2C%20C.%20C.%2C%20Wang%2C%20X.%2C%20%26%20Tang%2C%20X.%20%282014%29.%20Deepid-net%3A%20multi-stage%20and%20deformable%20deep%20convolutional%20neural%20networks%20for%20object%20detection.%20CoRR%2C%20abs%2F1409.3505."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR60">Papandreou, G. (2014). Deep epitomic convolutional neural networks. CoRR.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Papandreou%2C%20G.%20%282014%29.%20Deep%20epitomic%20convolutional%20neural%20networks.%20CoRR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR61">Papandreou, G., Chen, L.-C., &amp; Yuille, A. L. (2014). Modeling image patches with a generic dictionary of mini-epitomes.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Papandreou%2C%20G.%2C%20Chen%2C%20L.-C.%2C%20%26%20Yuille%2C%20A.%20L.%20%282014%29.%20Modeling%20image%20patches%20with%20a%20generic%20dictionary%20of%20mini-epitomes."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR62">Perronnin, F., &amp; Dance, C. R. (2007). Fisher kernels on visual vocabularies for image categorization. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Perronnin%2C%20F.%2C%20%26%20Dance%2C%20C.%20R.%20%282007%29.%20Fisher%20kernels%20on%20visual%20vocabularies%20for%20image%20categorization.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR63">Perronnin, F., Akata, Z., Harchaoui, Z., &amp; Schmid, C. (2012). Towards good practice in large-scale learning for image classification. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Perronnin%2C%20F.%2C%20Akata%2C%20Z.%2C%20Harchaoui%2C%20Z.%2C%20%26%20Schmid%2C%20C.%20%282012%29.%20Towards%20good%20practice%20in%20large-scale%20learning%20for%20image%20classification.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR64">Perronnin, F., Sánchez, J., &amp; Mensink, T. (2010). Improving the fisher kernel for large-scale image classification. In <em class="EmphasisTypeItalic ">ECCV</em> (4).<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Perronnin%2C%20F.%2C%20S%C3%A1nchez%2C%20J.%2C%20%26%20Mensink%2C%20T.%20%282010%29.%20Improving%20the%20fisher%20kernel%20for%20large-scale%20image%20classification.%20In%20ECCV%20%284%29."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR65">Russakovsky, O., Deng, J., Huang, Z., Berg, A., &amp; Fei-Fei, L. (2013). Detecting avocados to zucchinis: What have we done, &amp; where are we going? In <em class="EmphasisTypeItalic ">ICCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Russakovsky%2C%20O.%2C%20Deng%2C%20J.%2C%20Huang%2C%20Z.%2C%20Berg%2C%20A.%2C%20%26%20Fei-Fei%2C%20L.%20%282013%29.%20Detecting%20avocados%20to%20zucchinis%3A%20What%20have%20we%20done%2C%20%26%20where%20are%20we%20going%3F%20In%20ICCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR66">Russell, B., Torralba, A., Murphy, K., &amp; Freeman, W. T. (2007). LabelMe: A database and web-based tool for image annotation. In <em class="EmphasisTypeItalic ">IJCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Russell%2C%20B.%2C%20Torralba%2C%20A.%2C%20Murphy%2C%20K.%2C%20%26%20Freeman%2C%20W.%20T.%20%282007%29.%20LabelMe%3A%20A%20database%20and%20web-based%20tool%20for%20image%20annotation.%20In%20IJCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR67">Sanchez, J., &amp; Perronnin, F. (2011). High-dim. signature compression for large-scale image classification. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Sanchez%2C%20J.%2C%20%26%20Perronnin%2C%20F.%20%282011%29.%20High-dim.%20signature%20compression%20for%20large-scale%20image%20classification.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR68">Sanchez, J., Perronnin, F., &amp; de Campos, T. (2012). Modeling spatial layout of images beyond spatial pyramids. In <em class="EmphasisTypeItalic ">PRL</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Sanchez%2C%20J.%2C%20Perronnin%2C%20F.%2C%20%26%20de%20Campos%2C%20T.%20%282012%29.%20Modeling%20spatial%20layout%20of%20images%20beyond%20spatial%20pyramids.%20In%20PRL."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR69">Scheirer, W., Kumar, N., Belhumeur, P. N., &amp; Boult, T. E. (2012). Multi-attribute spaces: Calibration for attribute fusion and similarity search. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Scheirer%2C%20W.%2C%20Kumar%2C%20N.%2C%20Belhumeur%2C%20P.%20N.%2C%20%26%20Boult%2C%20T.%20E.%20%282012%29.%20Multi-attribute%20spaces%3A%20Calibration%20for%20attribute%20fusion%20and%20similarity%20search.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR70">Schmidhuber, J. (2012). Multi-column deep neural networks for image classification. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Schmidhuber%2C%20J.%20%282012%29.%20Multi-column%20deep%20neural%20networks%20for%20image%20classification.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR71">Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., &amp; LeCun, Y. (2013). Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Sermanet%2C%20P.%2C%20Eigen%2C%20D.%2C%20Zhang%2C%20X.%2C%20Mathieu%2C%20M.%2C%20Fergus%2C%20R.%2C%20%26%20LeCun%2C%20Y.%20%282013%29.%20Overfeat%3A%20Integrated%20recognition%2C%20localization%20and%20detection%20using%20convolutional%20networks.%20CoRR%2C%20abs%2F1312.6229."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR72">Sheng, V. S., Provost, F., &amp; Ipeirotis, P. G. (2008). Get another label? Improving data quality and data mining using multiple, noisy labelers. In <em class="EmphasisTypeItalic ">SIGKDD</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Sheng%2C%20V.%20S.%2C%20Provost%2C%20F.%2C%20%26%20Ipeirotis%2C%20P.%20G.%20%282008%29.%20Get%20another%20label%3F%20Improving%20data%20quality%20and%20data%20mining%20using%20multiple%2C%20noisy%20labelers.%20In%20SIGKDD."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR73">Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Simonyan%2C%20K.%2C%20%26%20Zisserman%2C%20A.%20%282014%29.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition.%20CoRR%2C%20abs%2F1409.1556."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR74">Simonyan, K., Vedaldi, A., &amp; Zisserman, A. (2013). Deep fisher networks for large-scale image classification. In <em class="EmphasisTypeItalic ">NIPS</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Simonyan%2C%20K.%2C%20Vedaldi%2C%20A.%2C%20%26%20Zisserman%2C%20A.%20%282013%29.%20Deep%20fisher%20networks%20for%20large-scale%20image%20classification.%20In%20NIPS."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR75">Sorokin, A., &amp; Forsyth, D. (2008). Utility data annotation with Amazon Mechanical Turk. In <em class="EmphasisTypeItalic ">InterNet08</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Sorokin%2C%20A.%2C%20%26%20Forsyth%2C%20D.%20%282008%29.%20Utility%20data%20annotation%20with%20Amazon%20Mechanical%20Turk.%20In%20InterNet08."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR76">Su, H., Deng, J., &amp; Fei-Fei, L. (2012). Crowdsourcing annotations for visual object detection. In <em class="EmphasisTypeItalic ">AAAI human computation workshop</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Su%2C%20H.%2C%20Deng%2C%20J.%2C%20%26%20Fei-Fei%2C%20L.%20%282012%29.%20Crowdsourcing%20annotations%20for%20visual%20object%20detection.%20In%20AAAI%20human%20computation%20workshop."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR77">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., &amp; Rabinovich, A. (2014). Going deeper with convolutions. Technical report.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Szegedy%2C%20C.%2C%20Liu%2C%20W.%2C%20Jia%2C%20Y.%2C%20Sermanet%2C%20P.%2C%20Reed%2C%20S.%2C%20Anguelov%2C%20D.%2C%20Erhan%2C%20D.%2C%20%26%20Rabinovich%2C%20A.%20%282014%29.%20Going%20deeper%20with%20convolutions.%20Technical%20report."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR78">Tang, Y. (2013). Deep learning using support vector machines. CoRR, abs/1306.0239.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Tang%2C%20Y.%20%282013%29.%20Deep%20learning%20using%20support%20vector%20machines.%20CoRR%2C%20abs%2F1306.0239."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR79">Thorpe, S., Fize, D., Marlot, C., et al. (1996). Speed of processing in the human visual system. <em class="EmphasisTypeItalic ">Nature</em>, <em class="EmphasisTypeItalic ">381</em>(6582), 520–522.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1038/381520a0"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Speed%20of%20processing%20in%20the%20human%20visual%20system&amp;author=S.%20Thorpe&amp;author=D.%20Fize&amp;author=C.%20Marlot&amp;journal=Nature&amp;volume=381&amp;issue=6582&amp;pages=520-522&amp;publication_year=1996"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR80">Torralba, A., &amp; Efros, A. A. (2011). Unbiased look at dataset bias. In <em class="EmphasisTypeItalic ">CVPR’11</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Torralba%2C%20A.%2C%20%26%20Efros%2C%20A.%20A.%20%282011%29.%20Unbiased%20look%20at%20dataset%20bias.%20In%20CVPR%E2%80%9911."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR81">Torralba, A., Fergus, R., &amp; Freeman, W. (2008). 80 million tiny images: A large data set for nonparametric object and scene recognition. <em class="EmphasisTypeItalic ">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em class="EmphasisTypeItalic ">30</em>, 1958–1970.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1109/TPAMI.2008.128"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=80%20million%20tiny%20images%3A%20A%20large%20data%20set%20for%20nonparametric%20object%20and%20scene%20recognition&amp;author=A.%20Torralba&amp;author=R.%20Fergus&amp;author=W.%20Freeman&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;volume=30&amp;pages=1958-1970&amp;publication_year=2008"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR82">Uijlings, J., van de Sande, K., Gevers, T., &amp; Smeulders, A. (2013). Selective search for object recognition. <em class="EmphasisTypeItalic ">International Journal of Computer Vision</em>, <em class="EmphasisTypeItalic ">104</em>, 154–171.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1007/s11263-013-0620-5"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Selective%20search%20for%20object%20recognition&amp;author=J.%20Uijlings&amp;author=K.%20Sande&amp;author=T.%20Gevers&amp;author=A.%20Smeulders&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;volume=104&amp;pages=154-171&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR83">Urtasun, R., Fergus, R., Hoiem, D., Torralba, A., Geiger, A., Lenz, P., Silberman, N., Xiao, J., &amp; Fidler, S. (2013–2014). Reconstruction meets recognition challenge. <span class="ExternalRef"><a target="_top" rel="noopener noreferrer" href="http://ttic.uchicago.edu.proxy.library.cornell.edu/rurtasun/rmrc/"><span class="RefSource">http://ttic.uchicago.edu.proxy.library.cornell.edu/rurtasun/rmrc/</span></a></span>.<span class="Occurrences"></span></div></li><li class="Citation"><div class="CitationContent" id="CR84">van de Sande, K. E. A., Snoek, C. G. M., &amp; Smeulders, A. W. M. (2014). Fisher and vlad with flair. In <em class="EmphasisTypeItalic ">Proceedings of the IEEE conference on computer vision and pattern recognition</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=van%20de%20Sande%2C%20K.%20E.%20A.%2C%20Snoek%2C%20C.%20G.%20M.%2C%20%26%20Smeulders%2C%20A.%20W.%20M.%20%282014%29.%20Fisher%20and%20vlad%20with%20flair.%20In%20Proceedings%20of%20the%20IEEE%20conference%20on%20computer%20vision%20and%20pattern%20recognition."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR85">van de Sande, K. E. A., Uijlings, J. R. R., Gevers, T., &amp; Smeulders, A. W. M. (2011b). Segmentation as selective search for object recognition. In <em class="EmphasisTypeItalic ">ICCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=van%20de%20Sande%2C%20K.%20E.%20A.%2C%20Uijlings%2C%20J.%20R.%20R.%2C%20Gevers%2C%20T.%2C%20%26%20Smeulders%2C%20A.%20W.%20M.%20%282011b%29.%20Segmentation%20as%20selective%20search%20for%20object%20recognition.%20In%20ICCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR86">van de Sande, K. E. A., Gevers, T., &amp; Snoek, C. G. M. (2010). Evaluating color descriptors for object and scene recognition. <em class="EmphasisTypeItalic ">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em class="EmphasisTypeItalic ">32</em>(9), 1582–1596.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1109/TPAMI.2009.154"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Evaluating%20color%20descriptors%20for%20object%20and%20scene%20recognition&amp;author=KEA.%20Sande&amp;author=T.%20Gevers&amp;author=CGM.%20Snoek&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;volume=32&amp;issue=9&amp;pages=1582-1596&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR87">van de Sande, K. E. A., Gevers, T., &amp; Snoek, C. G. M. (2011a). Empowering visual categorization with the GPU. <em class="EmphasisTypeItalic ">IEEE Transactions on Multimedia</em>, <em class="EmphasisTypeItalic ">13</em>(1), 60–70.<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_top" rel="noopener noreferrer" href="https://doi-org.proxy.library.cornell.edu/10.1109/TMM.2010.2091400"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Empowering%20visual%20categorization%20with%20the%20GPU&amp;author=KEA.%20Sande&amp;author=T.%20Gevers&amp;author=CGM.%20Snoek&amp;journal=IEEE%20Transactions%20on%20Multimedia&amp;volume=13&amp;issue=1&amp;pages=60-70&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR88">Vittayakorn, S., &amp; Hays, J. (2011). Quality assessment for crowdsourced object annotations. In <em class="EmphasisTypeItalic ">BMVC</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Vittayakorn%2C%20S.%2C%20%26%20Hays%2C%20J.%20%282011%29.%20Quality%20assessment%20for%20crowdsourced%20object%20annotations.%20In%20BMVC."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR89">von Ahn, L., &amp; Dabbish, L. (2005). Esp: Labeling images with a computer game. In <em class="EmphasisTypeItalic ">AAAI spring symposium: Knowledge collection from volunteer contributors</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=von%20Ahn%2C%20L.%2C%20%26%20Dabbish%2C%20L.%20%282005%29.%20Esp%3A%20Labeling%20images%20with%20a%20computer%20game.%20In%20AAAI%20spring%20symposium%3A%20Knowledge%20collection%20from%20volunteer%20contributors."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR90">Vondrick, C., Patterson, D., &amp; Ramanan, D. (2012). Efficiently scaling up crowdsourced video annotation. <em class="EmphasisTypeItalic ">International Journal of Computer Vision</em>, <em class="EmphasisTypeItalic ">1010</em>, 184–204.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Efficiently%20scaling%20up%20crowdsourced%20video%20annotation&amp;author=C.%20Vondrick&amp;author=D.%20Patterson&amp;author=D.%20Ramanan&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;volume=1010&amp;pages=184-204&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR91">Wan, L., Zeiler, M., Zhang, S., LeCun, Y., &amp; Fergus, R. (2013). Regularization of neural networks using dropconnect. In <em class="EmphasisTypeItalic ">Proceedings of the international conference on machine learning (ICML’13)</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Wan%2C%20L.%2C%20Zeiler%2C%20M.%2C%20Zhang%2C%20S.%2C%20LeCun%2C%20Y.%2C%20%26%20Fergus%2C%20R.%20%282013%29.%20Regularization%20of%20neural%20networks%20using%20dropconnect.%20In%20Proceedings%20of%20the%20international%20conference%20on%20machine%20learning%20%28ICML%E2%80%9913%29."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR92">Wang, M., Xiao, T., Li, J., Hong, C., Zhang, J., &amp; Zhang, Z. (2014). Minerva: A scalable and highly efficient training platform for deep learning. In <em class="EmphasisTypeItalic ">APSys</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Wang%2C%20M.%2C%20Xiao%2C%20T.%2C%20Li%2C%20J.%2C%20Hong%2C%20C.%2C%20Zhang%2C%20J.%2C%20%26%20Zhang%2C%20Z.%20%282014%29.%20Minerva%3A%20A%20scalable%20and%20highly%20efficient%20training%20platform%20for%20deep%20learning.%20In%20APSys."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR93">Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., &amp; Gong, Y. (2010). Locality-constrained linear coding for image classification. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Wang%2C%20J.%2C%20Yang%2C%20J.%2C%20Yu%2C%20K.%2C%20Lv%2C%20F.%2C%20Huang%2C%20T.%2C%20%26%20Gong%2C%20Y.%20%282010%29.%20Locality-constrained%20linear%20coding%20for%20image%20classification.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR94">Wang, X., Yang, M., Zhu, S., &amp; Lin, Y. (2013). Regionlets for generic object detection. In <em class="EmphasisTypeItalic ">ICCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Wang%2C%20X.%2C%20Yang%2C%20M.%2C%20Zhu%2C%20S.%2C%20%26%20Lin%2C%20Y.%20%282013%29.%20Regionlets%20for%20generic%20object%20detection.%20In%20ICCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR95">Welinder, P., Branson, S., Belongie, S., &amp; Perona, P. (2010). The multidimensional wisdom of crowds. In <em class="EmphasisTypeItalic ">NIPS</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Welinder%2C%20P.%2C%20Branson%2C%20S.%2C%20Belongie%2C%20S.%2C%20%26%20Perona%2C%20P.%20%282010%29.%20The%20multidimensional%20wisdom%20of%20crowds.%20In%20NIPS."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR96">Xiao, J., Hays, J., Ehinger, K., Oliva, A., &amp; Torralba., A. (2010). SUN database: Large-scale scene recognition from Abbey to Zoo. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Xiao%2C%20J.%2C%20Hays%2C%20J.%2C%20Ehinger%2C%20K.%2C%20Oliva%2C%20A.%2C%20%26%20Torralba.%2C%20A.%20%282010%29.%20SUN%20database%3A%20Large-scale%20scene%20recognition%20from%20Abbey%20to%20Zoo.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR97">Yang, J., Yu, K., Gong, Y., &amp; Huang, T. (2009). Linear spatial pyramid matching using sparse coding for image classification. In <em class="EmphasisTypeItalic ">CVPR</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Yang%2C%20J.%2C%20Yu%2C%20K.%2C%20Gong%2C%20Y.%2C%20%26%20Huang%2C%20T.%20%282009%29.%20Linear%20spatial%20pyramid%20matching%20using%20sparse%20coding%20for%20image%20classification.%20In%20CVPR."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR98">Yao, B., Yang, X., &amp; Zhu, S.-C. (2007). <em class="EmphasisTypeItalic ">Introduction to a large scale general purpose ground truth dataset: methodology, annotation tool, and benchmarks</em>. Berlin: Springer.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com.proxy.library.cornell.edu/scholar_lookup?title=Introduction%20to%20a%20large%20scale%20general%20purpose%20ground%20truth%20dataset%3A%20methodology%2C%20annotation%20tool%2C%20and%20benchmarks&amp;author=B.%20Yao&amp;author=X.%20Yang&amp;author=S-C.%20Zhu&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR99">Zeiler, M. D., &amp; Fergus, R. (2013). Visualizing and understanding convolutional networks. CoRR, abs/1311.2901.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Zeiler%2C%20M.%20D.%2C%20%26%20Fergus%2C%20R.%20%282013%29.%20Visualizing%20and%20understanding%20convolutional%20networks.%20CoRR%2C%20abs%2F1311.2901."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR100">Zeiler, M. D., Taylor, G. W., &amp; Fergus, R. (2011). Adaptive deconvolutional networks for mid and high level feature learning. In <em class="EmphasisTypeItalic ">ICCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Zeiler%2C%20M.%20D.%2C%20Taylor%2C%20G.%20W.%2C%20%26%20Fergus%2C%20R.%20%282011%29.%20Adaptive%20deconvolutional%20networks%20for%20mid%20and%20high%20level%20feature%20learning.%20In%20ICCV."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR101">Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., &amp; Oliva, A. (2014). Learning deep features for scene recognition using places database. In <em class="EmphasisTypeItalic ">NIPS</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Zhou%2C%20B.%2C%20Lapedriza%2C%20A.%2C%20Xiao%2C%20J.%2C%20Torralba%2C%20A.%2C%20%26%20Oliva%2C%20A.%20%282014%29.%20Learning%20deep%20features%20for%20scene%20recognition%20using%20places%20database.%20In%20NIPS."><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR102">Zhou, X., Yu, K., Zhang, T., &amp; Huang, T. (2010). Image classification using super-vector coding of local image descriptors. In <em class="EmphasisTypeItalic ">ECCV</em>.<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_top" rel="noopener noreferrer" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar-google-com.proxy.library.cornell.edu/scholar?q=Zhou%2C%20X.%2C%20Yu%2C%20K.%2C%20Zhang%2C%20T.%2C%20%26%20Huang%2C%20T.%20%282010%29.%20Image%20classification%20using%20super-vector%20coding%20of%20local%20image%20descriptors.%20In%20ECCV."><span><span>Google Scholar</span></span></a></span></span></div></li></ol></div></section><section class="Section1 RenderAsSection1"><h2 class="Heading" id="copyrightInformation">Copyright information</h2><div class="ArticleCopyright content"><div class="ArticleCopyright">©&nbsp;Springer Science+Business Media New York&nbsp;2015</div></div></section><section id="authorsandaffiliations" class="Section1 RenderAsSection1"><h2 class="Heading">Authors and Affiliations</h2><div class="content authors-affiliations u-interface"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Olga&nbsp;Russakovsky</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul><span class="author-information"><span class="author-information__contact u-icon-before icon--email-before"><a href="mailto:olga@cs.stanford.edu" title="olga@cs.stanford.edu" itemprop="email" class="gtm-email-author">Email author</a></span></span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Jia&nbsp;Deng</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-2">2</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Hao&nbsp;Su</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Jonathan&nbsp;Krause</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Sanjeev&nbsp;Satheesh</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Sean&nbsp;Ma</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Zhiheng&nbsp;Huang</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Andrej&nbsp;Karpathy</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Aditya&nbsp;Khosla</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-3">3</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Michael&nbsp;Bernstein</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Alexander&nbsp;C.&nbsp;Berg</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-4">4</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Li&nbsp;Fei-Fei</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li></ul><ol class="test-affiliations"><li class="affiliation" data-test="affiliation-1" data-affiliation-highlight="affiliation-1" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">1.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">Stanford University</span><span itemprop="city" class="affiliation__city">Stanford</span><span itemprop="country" class="affiliation__country">USA</span></span></li><li class="affiliation" data-test="affiliation-2" data-affiliation-highlight="affiliation-2" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">2.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">University of Michigan</span><span itemprop="city" class="affiliation__city">Ann Arbor</span><span itemprop="country" class="affiliation__country">USA</span></span></li><li class="affiliation" data-test="affiliation-3" data-affiliation-highlight="affiliation-3" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">3.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">Massachusetts Institute of Technology</span><span itemprop="city" class="affiliation__city">Cambridge</span><span itemprop="country" class="affiliation__country">USA</span></span></li><li class="affiliation" data-test="affiliation-4" data-affiliation-highlight="affiliation-4" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">4.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">UNC Chapel Hill</span><span itemprop="city" class="affiliation__city">Chapel Hill</span><span itemprop="country" class="affiliation__country">USA</span></span></li></ol></div></section></div>
                                <aside class="content-type-about" id="AboutThisContent">
    <h2 class="Heading" id="aboutcontent">About this article</h2>
    <div class="content bibliographic-information">
                <div id="crossMark" class="crossmark">
            <a data-crossmark="10.1007%2Fs11263-015-0816-y" class="gtm-crossmark" target="_top" rel="noopener noreferrer" href="https://crossmark-crossref-org.proxy.library.cornell.edu/dialog/?doi=10.1007%2Fs11263-015-0816-y" title="Verify currency and authenticity via CrossMark">
                <span class="u-screenreader-only">CrossMark</span>
                <svg class="CrossMark" id="crossmark-icon" width="57" height="81">
                    <image width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="/springerlink-static/1756782716/images/png/crossmark.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/1756782716/images/svg/crossmark.svg"></image>
                </svg>
            </a>
        </div>

            <div class="crossmark__adjacent">
                <dl class="citation-info u-highlight-target u-mb-16" id="citeas">
    <dt class="test-cite-heading">

        Cite this article as:
    </dt>
    <dd id="citethis-text">Russakovsky, O., Deng, J., Su, H. et al. Int J Comput Vis (2015) 115: 211. https://doi-org.proxy.library.cornell.edu/10.1007/s11263-015-0816-y</dd>
</dl>
                    <ul class="bibliographic-information__list bibliographic-information__list--inline">
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title"><abbr title="Digital Object Identifier">DOI</abbr></span>
            <span class="bibliographic-information__value u-overflow-wrap" id="doi-url">https://doi-org.proxy.library.cornell.edu/10.1007/s11263-015-0816-y</span>
        </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Publisher Name</span>
                <span class="bibliographic-information__value" id="publisher-name">Springer US</span>
            </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Print ISSN</span>
                <span class="bibliographic-information__value" id="print-issn">0920-5691</span>
            </li>
            <li class="bibliographic-information__item ">
                <span class="bibliographic-information__title">Online ISSN</span>
                <span class="bibliographic-information__value" id="electronic-issn">1573-1405</span>
            </li>

        
    </ul>

                <ul class="bibliographic-information__list">
    <li class="bibliographic-information__item">
        <a id="about-journal" class="bibliographic-information__misc-links gtm-about-this" title="Visit Springer.com for information about this article's journal" href="https://www-springer-com.proxy.library.cornell.edu/journal/11263/about">About this journal</a>
    </li>
    <li class="bibliographic-information__item">
        <a id="reprintsandpermissions-link" class="u-external" target="_top" rel="noopener noreferrer" href="https://s100.copyright.com/AppDispatchServlet?publisherName=Springer&amp;orderBeanReset=true&amp;orderSource=SpringerLink&amp;author=Olga+Russakovsky%2C+Jia+Deng%2C+Hao+Su+et+al&amp;authorEmail=olga%40cs.stanford.edu&amp;issueNum=3&amp;contentID=10.1007%2Fs11263-015-0816-y&amp;openAccess=false&amp;endPage=252&amp;publicationDate=2015&amp;startPage=211&amp;volumeNum=115&amp;title=ImageNet+Large+Scale+Visual+Recognition+Challenge&amp;imprint=Springer+Science%2BBusiness+Media+New+York&amp;publication=0920-5691&amp;authorAddress=Stanford%2C+CA%2C+USA" title="Visit RightsLink for information about reusing this article">Reprints and Permissions</a>
    </li>
</ul>
            </div>
        
            
    </div>
</aside>

                                <div class="collapsible-section uptodate-recommendations gtm-recommendations">
    <h2 class="uptodate-recommendations__title collapsible-section__heading gtm-recommendations__title" id="uptodaterecommendations">Personalised recommendations</h2>
    <div class="collapsible-section__content">
        <div class="uptodate-recommendations__container">
             <link rel="uptodate-inline" href="https://link-springer-com.proxy.library.cornell.edu/springerlink-static/1756782716/css/recommendations.css">
        </div>
    </div>
</div>
                                            <div class="sticky-banner 
            u-interface u-js-screenreader-only" aria-hidden="true" data-component="SpringerLink.StickyBanner" data-namespace="hasButton">
                <div class="sticky-banner__container">
                        <div class="citations" data-component="SV.Dropdown" data-namespace="citationsSticky">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" class="gtm-cite-dropdown">How to cite?</a>
    </li>
        <li>
            <a href="https://citation-needed-springer-com.proxy.library.cornell.edu/v2/references/10.1007/s11263-015-0816-y?format=refman&amp;flavour=citation" title="Download this article's citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>

                            Papers
                        </span>
                        <span>

                            Reference Manager
                        </span>
                        <span>

                            RefWorks
                        </span>
                        <span>

                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed-springer-com.proxy.library.cornell.edu/v2/references/10.1007/s11263-015-0816-y?format=endnote&amp;flavour=citation" title="Download this article's citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>

                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed-springer-com.proxy.library.cornell.edu/v2/references/10.1007/s11263-015-0816-y?format=bibtex&amp;flavour=citation" title="Download this article's citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>

                            BibTeX
                        </span>
                        <span>

                            JabRef
                        </span>
                        <span>

                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul>
    </div>

                        <div class="share-this" data-component="SV.Dropdown" data-namespace="shareThisSticky">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Share</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="share-this__content" data-role="button-dropdown__content">
    <li>
        <a class="test-shareby-email-link gtm-shareby-email-link" href="mailto:?to=&amp;subject=Read%20this%20article%20on%20SpringerLink&amp;body=ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%0A%0Ahttps%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y" title="Share this article via email">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"></path><g transform="translate(3 5)"><rect fill="#F9F9F9" width="18" height="14" rx="2"></rect><path d="M1 2.006v9.988c0 .557.446 1.006.995 1.006h14.01c.549 0 .995-.449.995-1.006v-9.988c0-.557-.446-1.006-.995-1.006h-14.01c-.549 0-.995.449-.995 1.006zm-1 0c0-1.108.893-2.006 1.995-2.006h14.01c1.102 0 1.995.897 1.995 2.006v9.988c0 1.108-.893 2.006-1.995 2.006h-14.01c-1.102 0-1.995-.897-1.995-2.006v-9.988zM9 9l7-4v-1.443l-7 4-7-4v1.443z" fill="#666"></path></g></g></svg>
                <span>Email</span>
            </span>
        </a>
    </li>
    <li>
        <a class="test-shareby-facebook-link gtm-shareby-facebook-link" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y" target="_top" rel="noopener noreferrer" title="Share this article via Facebook">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"></path><path d="M12.717 19.091h-2.66v-6.274h-1.329v-2.162h1.329v-1.298c0-1.763.75-2.813 2.883-2.813h1.775v2.162h-1.11c-.83 0-.885.302-.885.866l-.004 1.082h2.011l-.235 2.162h-1.775v6.274z" fill="#666"></path></g></svg>
                <span>Facebook</span>
            </span>
        </a>
    </li>
    <li>
        <a class="test-shareby-twitter-link gtm-shareby-twitter-link" href="https://twitter.com/intent/tweet?text=I%27m%20reading%20this%20on%20%23springerlink&amp;url=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y" target="_top" rel="noopener noreferrer" title="Share this article via Twitter">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"></path><path d="M16.651 9.189c.485-.306.858-.792 1.033-1.371-.454.284-.957.49-1.493.601-.428-.482-1.039-.783-1.715-.783-1.298 0-2.349 1.11-2.349 2.478 0 .194.019.384.06.564-1.952-.104-3.684-1.089-4.843-2.59-.202.367-.318.793-.318 1.247 0 .859.415 1.618 1.045 2.063-.385-.013-.748-.126-1.065-.31v.03c0 1.201.809 2.203 1.886 2.43-.198.058-.405.087-.62.087-.151 0-.299-.015-.442-.044.299.984 1.166 1.702 2.195 1.721-.805.665-1.818 1.061-2.919 1.061-.19 0-.377-.011-.561-.034 1.04.703 2.275 1.113 3.602 1.113 4.323 0 6.686-3.777 6.686-7.052l-.006-.321c.459-.35.859-.786 1.173-1.283-.422.197-.875.33-1.349.39z" fill="#666"></path></g></svg>
                <span>Twitter</span>
            </span>
        </a>
    </li>
        <li>
            <a class="test-shareby-linkedin-link gtm-shareby-linkedin-link" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y&amp;title=ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge&amp;summary=The%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%20is%20a%20benchmark%20in%20object%20category%20classification%20and%20detection%20on%20hundreds%20of%20object%20categories%20and%20millions%20of%20images.%20The%20challenge%20has%20been%20run%20annually%20from%202010%20to%20present,%20attracting%20partic%E2%80%A6" target="_top" rel="noopener noreferrer" title="Share this article via LinkedIn">
                <span class="share-this__types">
                    <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"></path><path d="M6.821 10.044h2.126v7.41h-2.126v-7.41zm1.009-.927h-.015c-.77 0-1.269-.566-1.269-1.284 0-.732.514-1.287 1.299-1.287.784 0 1.267.554 1.282 1.285 0 .717-.498 1.286-1.297 1.286zm9.625 8.338h-2.411v-3.835c0-1.004-.377-1.688-1.206-1.688-.634 0-.987.462-1.151.908-.062.159-.052.382-.052.606v4.01h-2.389s.031-6.794 0-7.411h2.389v1.163c.141-.509.904-1.234 2.122-1.234 1.511 0 2.698 1.067 2.698 3.361v4.121z" fill="#666"></path></g></svg>
                    <span>LinkedIn</span>
                </span>
            </a>
        </li>
        <li>
            <a class="gtm-shareby-sharelink-link" data-test="shareable-link" href="https://link-springer-com.proxy.library.cornell.edu/sharelink/10.1007/s11263-015-0816-y" target="_top" rel="noopener noreferrer" title="Get shareable link">
                <span class="share-this__types">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="#666" d="M9 7c-2.8 0-5 2.2-5 5s2.2 5 5 5h2v-2h-2c-1.7 0-3-1.3-3-3s1.3-3 3-3h2v-2h-2zm6 0c2.8 0 5 2.2 5 5s-2.2 5-5 5h-2v-2h2c1.7 0 3-1.3 3-3s-1.3-3-3-3h-2v-2h2zm-1 4c.5 0 1 .4 1 1s-.5 1-1 1h-4c-.5 0-1-.4-1-1s.5-1 1-1h4z"></path></svg>
                    <span>Shareable link</span>
                </span>
            </a>
        </li>
</ul>
    </div>

                                    <a href="https://link-springer-com.proxy.library.cornell.edu/content/pdf/10.1007%2Fs11263-015-0816-y.pdf" target="_top" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener noreferrer">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"></path></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

                </div>
            </div>




                            </div>
                        </div>

                        <aside class="main-sidebar-right u-interface">
                            <div data-role="sticky-wrapper">
                                <div class="main-sidebar-right__content u-composite-layer" data-component="SpringerLink.StickySidebar">
                                    <div class="article-actions" id="article-actions">
                                        <h2 class="u-screenreader-only">Actions</h2>


                                        <div class="u-js-hide u-js-show-two-col">
                                            

                                                    <div class="download-article test-pdf-link">
                                                                <a href="https://link-springer-com.proxy.library.cornell.edu/content/pdf/10.1007%2Fs11263-015-0816-y.pdf" target="_top" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener noreferrer">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"></path></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

                                                    </div>


                                                <div class="citations" data-component="SV.Dropdown" data-namespace="citations">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" class="gtm-cite-dropdown">How to cite?</a>
    </li>
        <li>
            <a href="https://citation-needed-springer-com.proxy.library.cornell.edu/v2/references/10.1007/s11263-015-0816-y?format=refman&amp;flavour=citation" title="Download this article's citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>

                            Papers
                        </span>
                        <span>

                            Reference Manager
                        </span>
                        <span>

                            RefWorks
                        </span>
                        <span>

                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed-springer-com.proxy.library.cornell.edu/v2/references/10.1007/s11263-015-0816-y?format=endnote&amp;flavour=citation" title="Download this article's citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>

                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed-springer-com.proxy.library.cornell.edu/v2/references/10.1007/s11263-015-0816-y?format=bibtex&amp;flavour=citation" title="Download this article's citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>

                            BibTeX
                        </span>
                        <span>

                            JabRef
                        </span>
                        <span>

                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul>
    </div>

                                                <div class="share-this" data-component="SV.Dropdown" data-namespace="shareThis">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Share</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="share-this__content" data-role="button-dropdown__content">
    <li>
        <a class="test-shareby-email-link gtm-shareby-email-link" href="mailto:?to=&amp;subject=Read%20this%20article%20on%20SpringerLink&amp;body=ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%0A%0Ahttps%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y" title="Share this article via email">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"></path><g transform="translate(3 5)"><rect fill="#F9F9F9" width="18" height="14" rx="2"></rect><path d="M1 2.006v9.988c0 .557.446 1.006.995 1.006h14.01c.549 0 .995-.449.995-1.006v-9.988c0-.557-.446-1.006-.995-1.006h-14.01c-.549 0-.995.449-.995 1.006zm-1 0c0-1.108.893-2.006 1.995-2.006h14.01c1.102 0 1.995.897 1.995 2.006v9.988c0 1.108-.893 2.006-1.995 2.006h-14.01c-1.102 0-1.995-.897-1.995-2.006v-9.988zM9 9l7-4v-1.443l-7 4-7-4v1.443z" fill="#666"></path></g></g></svg>
                <span>Email</span>
            </span>
        </a>
    </li>
    <li>
        <a class="test-shareby-facebook-link gtm-shareby-facebook-link" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y" target="_top" rel="noopener noreferrer" title="Share this article via Facebook">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"></path><path d="M12.717 19.091h-2.66v-6.274h-1.329v-2.162h1.329v-1.298c0-1.763.75-2.813 2.883-2.813h1.775v2.162h-1.11c-.83 0-.885.302-.885.866l-.004 1.082h2.011l-.235 2.162h-1.775v6.274z" fill="#666"></path></g></svg>
                <span>Facebook</span>
            </span>
        </a>
    </li>
    <li>
        <a class="test-shareby-twitter-link gtm-shareby-twitter-link" href="https://twitter.com/intent/tweet?text=I%27m%20reading%20this%20on%20%23springerlink&amp;url=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y" target="_top" rel="noopener noreferrer" title="Share this article via Twitter">
            <span class="share-this__types">
                <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"></path><path d="M16.651 9.189c.485-.306.858-.792 1.033-1.371-.454.284-.957.49-1.493.601-.428-.482-1.039-.783-1.715-.783-1.298 0-2.349 1.11-2.349 2.478 0 .194.019.384.06.564-1.952-.104-3.684-1.089-4.843-2.59-.202.367-.318.793-.318 1.247 0 .859.415 1.618 1.045 2.063-.385-.013-.748-.126-1.065-.31v.03c0 1.201.809 2.203 1.886 2.43-.198.058-.405.087-.62.087-.151 0-.299-.015-.442-.044.299.984 1.166 1.702 2.195 1.721-.805.665-1.818 1.061-2.919 1.061-.19 0-.377-.011-.561-.034 1.04.703 2.275 1.113 3.602 1.113 4.323 0 6.686-3.777 6.686-7.052l-.006-.321c.459-.35.859-.786 1.173-1.283-.422.197-.875.33-1.349.39z" fill="#666"></path></g></svg>
                <span>Twitter</span>
            </span>
        </a>
    </li>
        <li>
            <a class="test-shareby-linkedin-link gtm-shareby-linkedin-link" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-015-0816-y&amp;title=ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge&amp;summary=The%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%20is%20a%20benchmark%20in%20object%20category%20classification%20and%20detection%20on%20hundreds%20of%20object%20categories%20and%20millions%20of%20images.%20The%20challenge%20has%20been%20run%20annually%20from%202010%20to%20present,%20attracting%20partic%E2%80%A6" target="_top" rel="noopener noreferrer" title="Share this article via LinkedIn">
                <span class="share-this__types">
                    <svg width="24" height="24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none"><path fill-opacity="0" fill="#fff" d="M0 0h24v24h-24z"></path><path d="M6.821 10.044h2.126v7.41h-2.126v-7.41zm1.009-.927h-.015c-.77 0-1.269-.566-1.269-1.284 0-.732.514-1.287 1.299-1.287.784 0 1.267.554 1.282 1.285 0 .717-.498 1.286-1.297 1.286zm9.625 8.338h-2.411v-3.835c0-1.004-.377-1.688-1.206-1.688-.634 0-.987.462-1.151.908-.062.159-.052.382-.052.606v4.01h-2.389s.031-6.794 0-7.411h2.389v1.163c.141-.509.904-1.234 2.122-1.234 1.511 0 2.698 1.067 2.698 3.361v4.121z" fill="#666"></path></g></svg>
                    <span>LinkedIn</span>
                </span>
            </a>
        </li>
        <li>
            <a class="gtm-shareby-sharelink-link" data-test="shareable-link" href="https://link-springer-com.proxy.library.cornell.edu/sharelink/10.1007/s11263-015-0816-y" target="_top" rel="noopener noreferrer" title="Get shareable link">
                <span class="share-this__types">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="#666" d="M9 7c-2.8 0-5 2.2-5 5s2.2 5 5 5h2v-2h-2c-1.7 0-3-1.3-3-3s1.3-3 3-3h2v-2h-2zm6 0c2.8 0 5 2.2 5 5s-2.2 5-5 5h-2v-2h2c1.7 0 3-1.3 3-3s-1.3-3-3-3h-2v-2h2zm-1 4c.5 0 1 .4 1 1s-.5 1-1 1h-4c-.5 0-1-.4-1-1s.5-1 1-1h4z"></path></svg>
                    <span>Shareable link</span>
                </span>
            </a>
        </li>
</ul>
    </div>

                                        </div>
                                    </div>
                                    <nav class="toc" aria-label="article contents">
    <h2 class="u-h4 u-screenreader-only">Table of contents</h2>
    <ul id="article-contents" class="article-contents" role="menu">
            <li role="menuitem">
                <a title="Article" href="#enumeration"><span class="u-overflow-ellipsis">Article</span></a>
            </li>
            <li role="menuitem">
                <a title="Abstract" href="#Abs1"><span class="u-overflow-ellipsis">Abstract</span></a>
            </li>
            <li role="menuitem">
                <a title="1 Introduction" href="#Sec1"><span class="u-overflow-ellipsis">1 Introduction</span></a>
            </li>
            <li role="menuitem">
                <a title="2 Challenge Tasks" href="#Sec4"><span class="u-overflow-ellipsis">2 Challenge Tasks</span></a>
            </li>
            <li role="menuitem">
                <a title="3 Dataset Construction at Large Scale" href="#Sec8"><span class="u-overflow-ellipsis">3 Dataset Construction at Large Scale</span></a>
            </li>
            <li role="menuitem">
                <a title="4 Evaluation at Large Scale" href="#Sec22"><span class="u-overflow-ellipsis">4 Evaluation at Large Scale</span></a>
            </li>
            <li role="menuitem">
                <a title="5 Methods" href="#Sec26"><span class="u-overflow-ellipsis">5 Methods</span></a>
            </li>
            <li role="menuitem">
                <a title="6 Results and Analysis" href="#Sec29"><span class="u-overflow-ellipsis">6 Results and Analysis</span></a>
            </li>
            <li role="menuitem">
                <a title="7 Conclusions" href="#Sec43"><span class="u-overflow-ellipsis">7 Conclusions</span></a>
            </li>
            <li role="menuitem">
                <a title="Footnotes" href="#Footnotes"><span class="u-overflow-ellipsis">Footnotes</span></a>
            </li>
            <li role="menuitem">
                <a title="Notes" href="#Notes"><span class="u-overflow-ellipsis">Notes</span></a>
            </li>
            <li role="menuitem">
                <a title="Appendix 1: ILSVRC2012-2014 Image Classification and Single-Object Localization Object Categories" href="#Sec47"><span class="u-overflow-ellipsis">Appendix 1: ILSVRC2012-2014 Image Classification and Single-Object Localization Object Categories</span></a>
            </li>
            <li role="menuitem">
                <a title="Appendix 2: Additional Single-Object Localization Dataset Statistics" href="#Sec48"><span class="u-overflow-ellipsis">Appendix 2: Additional Single-Object Localization Dataset Statistics</span></a>
            </li>
            <li role="menuitem">
                <a title="Appendix 3: Manually Curated Queries for Obtaining Object Detection Scene Images" href="#Sec49"><span class="u-overflow-ellipsis">Appendix 3: Manually Curated Queries for Obtaining Object Detection Scene Images</span></a>
            </li>
            <li role="menuitem">
                <a title="Appendix 4: Hierarchy of Questions for Full Image Annotation" href="#Sec50"><span class="u-overflow-ellipsis">Appendix 4: Hierarchy of Questions for Full Image Annotation</span></a>
            </li>
            <li role="menuitem">
                <a title="Appendix 5: Modification to Bounding Box System for Object Detection" href="#Sec51"><span class="u-overflow-ellipsis">Appendix 5: Modification to Bounding Box System for Object Detection</span></a>
            </li>
            <li role="menuitem">
                <a title="Appendix 6: Competition Protocol" href="#Sec52"><span class="u-overflow-ellipsis">Appendix 6: Competition Protocol</span></a>
            </li>
            <li role="menuitem">
                <a title="References" href="#Bib1"><span class="u-overflow-ellipsis">References</span></a>
            </li>
            <li role="menuitem">
                <a title="Copyright information" href="#copyrightInformation"><span class="u-overflow-ellipsis">Copyright information</span></a>
            </li>
            <li role="menuitem">
                <a title="Authors and Affiliations" href="#authorsandaffiliations"><span class="u-overflow-ellipsis">Authors and Affiliations</span></a>
            </li>
            <li role="menuitem">
                <a title="About this article" href="#aboutcontent"><span class="u-overflow-ellipsis">About this article</span></a>
            </li>
    </ul>
</nav>

                                </div>
                                        <div class="skyscraper-ad u-hide" data-component="SpringerLink.GoogleAds" data-namespace="skyscraper"></div>

                            </div>
                        </aside>
                    </div>
                </article>
            </main>

            <section class="banner banner--cookies-policy u-js-hide u-composite-layer" data-component="SpringerLink.CookiePolicy">
    <div class="banner__content u-interface">
        <h2 class="u-screenreader-only">Cookies</h2>
        <span class="banner__cookie-text">We use cookies to improve your experience with our site.</span>
        <a class="banner__link banner__link--cookie" href="https://link-springer-com.proxy.library.cornell.edu/termsandconditions#cookies">More information</a>

        <form class="banner__form" action="https://link-springer-com.proxy.library.cornell.edu/acceptcookies" method="POST">
            <button class="u-link-like banner__right-ui">Accept</button>
        </form>
    </div>
</section>

                <footer class="footer u-interface">
        <div class="footer__aside-wrapper">
            <div class="footer__content">
                <div class="footer__aside">
                    <p class="footer__strapline">Over 10 million scientific documents at your fingertips</p>
                                <div class="footer__edition" data-component="SpringerLink.EditionSwitcher">
                                    <h3 class="u-hide" data-role="button-dropdown__title" data-btn-text="Switch between Academic &amp; Corporate Edition">Switch Edition</h3>
                                    <ul data-role="button-dropdown__content">
                                        <li class="selected"><a href="https://link-springer-com.proxy.library.cornell.edu/siteEdition/link" id="siteedition-academic-link">Academic Edition</a></li>
                                        <li><a href="https://link-springer-com.proxy.library.cornell.edu/siteEdition/rd" id="siteedition-corporate-link">Corporate Edition</a></li>
                                    </ul>
                                </div>
                </div>
            </div>
        </div>
        <div class="footer__content">
            <ul class="footer__nav">
                <li>
                    <a href="https://link-springer-com.proxy.library.cornell.edu/">Home</a>
                </li>
                <li>
                    <a href="https://link-springer-com.proxy.library.cornell.edu/impressum">Impressum</a>
                </li>
                <li>
                    <a href="https://link-springer-com.proxy.library.cornell.edu/termsandconditions">Legal Information</a>
                </li>
                <li>
                    <a href="https://link-springer-com.proxy.library.cornell.edu/accessibility" class="gtm-footer-accessibility">Accessibility</a>
                </li>
                <li>
                    <a id="contactus-footer-link" href="https://link-springer-com.proxy.library.cornell.edu/contactus">Contact Us</a>
                </li>
            </ul>
            <a class="parent-logo" target="_top" rel="noreferrer noopener" href="https://www.springernature.com/" title="Go to Springer Nature">
                <span class="u-screenreader-only">Springer Nature</span>
                <svg width="125" height="12">
                    <image width="125" height="12" alt="Springer Nature logo" src="/springerlink-static/1756782716/images/png/springernature.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/1756782716/images/svg/springernature.svg">
                    </image>
                </svg>
            </a>

            <p class="footer__copyright">© 2017 Springer International Publishing AG. Part of <a target="_top" rel="noreferrer noopener" href="https://www.springernature.com/">Springer Nature</a>.</p>

                <p class="footer__user-access-info">
                    <span>Not logged in</span>
                    <span>CORNELL UNIVERSITY Ithaca, NY 14850 (8200595982) - Center for Research LIbraries c/o NERL (8200828607)</span>
                    <span>132.236.27.111</span>
                </p>
        </div>
    </footer>

        </div>
          <script type="text/javascript">

        (function() {
            var linkEl = document.querySelector('.js-ctm');
            if (window.matchMedia && window.matchMedia(linkEl.media)) {
                var scriptMathJax = document.createElement('script');
                scriptMathJax.async = true;
                scriptMathJax.src = '/springerlink-static/1756782716/js/mathJax.js';
                var s0 = document.getElementsByTagName('script')[0];
                s0.parentNode.insertBefore(scriptMathJax, s0);
            }
        })();
    </script>

<script type="text/javascript">

    (function() {
        var linkEl = document.querySelector('.js-ctm');
        if (window.matchMedia && window.matchMedia(linkEl.media)) {
            (function(h){h.className = h.className.replace('no-js', 'js')})(document.documentElement);
            window.SpringerLink = window.SpringerLink || {};
            window.SpringerLink.staticLocation = '/springerlink-static/1756782716';
            var scriptJquery = document.createElement('script');
            var scriptMain = document.createElement('script');
            scriptJquery.async = false;
            scriptJquery.src = window.SpringerLink.staticLocation + '/js/jquery-3.2.1.min.js';
            scriptMain.async = false;
            scriptMain.src =  window.SpringerLink.staticLocation + '/js/main.js';
            document.body.appendChild(scriptJquery);
            document.body.appendChild(scriptMain);
        }
    })();
</script><script src="jquery-3.2.1.min.js"></script><script src="main.js"></script>

<script type="text/javascript">

    window.onload = function() {
    var linkEl = document.querySelector('.js-ctm');

        if (window.matchMedia && window.matchMedia(linkEl.media) && ($('.test-bookpdf-link').length || $('.test-bookepub-link').length)) {
            reportParticipation('https://ab-reporting-live-cf-public-springer-com.proxy.library.cornell.edu', 'bunsen-bookpage-book-download', 'baseline');
            reportForMouseEvents(['.test-bookpdf-link', '.test-bookepub-link'], 'https://ab-reporting-live-cf-public-springer-com.proxy.library.cornell.edu', 'bunsen-bookpage-book-download', 'baseline');

            if ($('.gtm-pdf-link').length) {
                reportParticipation('https://ab-reporting-live-cf-public-springer-com.proxy.library.cornell.edu', 'bunsen-bookpage-chapter-download', 'baseline');
                reportForMouseEvent('.gtm-pdf-link', 'https://ab-reporting-live-cf-public-springer-com.proxy.library.cornell.edu', 'bunsen-bookpage-chapter-download', 'baseline');
            }
        }


    function viewport() {
        if (document.documentElement.clientWidth < 620) {
            size = 'small';
        }
        else if(document.documentElement.clientWidth < 1075 ) {
            size = 'medium';
        }
        else {
            size = 'wide';
        }
        return size;
    }

    function reportForMouseEvent(linkCssSelector, nolardUrl, experiment, abgroup) {
        var counter = 0;
        $('body').delegate(linkCssSelector, 'click', function() {
            if(counter == 0) {
                reportConversion(nolardUrl, experiment, abgroup);
                counter++;
            }
        });
    }

    function reportForMouseEvents(linkCssSelectors, nolardUrl, experiment, abgroup) {
        var counter = 0;
        linkCssSelectors.forEach(function(cssSelector) {
            $('body').delegate(cssSelector, 'click', function() {
                if(counter == 0) {
                    reportConversion(nolardUrl, experiment, abgroup);
                    counter++;
                }
            });
        });
    }

    function reportParticipation(nolardUrl, experiment, abgroup) {
        $.ajax({ url: nolardUrl + '/participate/' + experiment + '/' + abgroup });
    }

    function reportConversion(nolardUrl, experiment, abgroup) {
        $.ajax({ url: nolardUrl + '/convert/' + experiment + '/' + abgroup });
    }
    };
</script>

<script class="kxct" data-id="KDqyaFZ_" data-timing="async" data-version="3.0" type="text/javascript">

 window.Krux||((Krux=function(){Krux.q.push(arguments)}).q=[]);
 (function(){
   var k=document.createElement('script');k.type='text/javascript';k.async=true;
   k.src=(location.protocol==='https:'?'https:':'http:')+'//cdn.krxd.net/controltag/KDqyaFZ_.js';
   var s=document.getElementsByTagName('script')[0];s.parentNode.insertBefore(k,s);
 }());
</script>

<script>

    window.Krux||((Krux=function(){Krux.q.push(arguments);}).q=[]);
    (function(){
        function retrieve(n){
            var m, k='kx'+n;
            if (window.localStorage) {
                return window.localStorage[k] || "";
            } else if (navigator.cookieEnabled) {
                m = document.cookie.match(k+'=([^;]*)');
                return (m && unescape(m[1])) || "";
            } else {
                return '';
            }
        }
        Krux.user = retrieve('user');
        Krux.segments = retrieve('segs') && retrieve('segs').split(',') || [];
    })();
</script>

    <script type="text/javascript">

        var googletag = googletag || {};
        googletag.cmd = googletag.cmd || [];
        (function() {
            var gads = document.createElement('script');
            gads.async = true;
            gads.type = 'text/javascript';
            var useSSL = 'https:' == document.location.protocol;
            gads.src = (useSSL ? 'https:' : 'http:') +
                    '//www.googletagservices.com/tag/js/gpt.js';
            var node = document.getElementsByTagName('script')[0];
            node.parentNode.insertBefore(gads, node);
        })();
    </script>
    <script type="text/javascript" id="googletag-push">

        
            var adSlot = '270604982/springerlink/11263/article';
        

        var definedSlots = [
                {slot: [728, 90], containerName: 'doubleclick-leaderboard-ad'},
                {slot: [160, 600], containerName: 'doubleclick-ad'},
        ];

        googletag.cmd.push(function() {
                googletag.pubads().setTargeting("doi","10.1007-s11263-015-0816-y");
                googletag.pubads().setTargeting("kwrd",["Dataset","Large-scale","Benchmark","Object_recognition","Object_detection"]);
                googletag.pubads().setTargeting("pmc",["I","I22005","I21017","I22021","I2203X"]);
                googletag.pubads().setTargeting("BPID",["8200595982","8200828607"]);
                googletag.pubads().setTargeting("edition","academic");
                googletag.pubads().setTargeting("sucode","SC6");
                googletag.pubads().setTargeting("eissn","1573-1405");
                googletag.pubads().setTargeting("pissn","0920-5691");
                googletag.pubads().setTargeting("ksg",Krux.segments);
                googletag.pubads().setTargeting("kuid",Krux.uid);
                googletag.pubads().setTargeting("logged","N");
            googletag.pubads().enableSingleRequest();
            googletag.pubads().enableAsyncRendering();
            googletag.enableServices();
        });
    </script>

        
        <span id="chat-widget" class="u-hide"></span>
                    <noscript>

                <img aria-hidden="true" role="presentation" src="https://ssl-springer.met.vgwort.de/na/pw-vgzm.415900-10.1007-s11263-015-0816-y" width='1' height='1' alt='' />
            </noscript>

    

<script type="text/javascript" id="">(function(d,z){function n(){for(var a in dataLayer)if(dataLayer.hasOwnProperty(a)&&dataLayer[a]["Event Category"])return dataLayer[a]["Event Category"];return"warning: no event category"}function v(a,b,c,h){"undefined"!==typeof dataLayer?dataLayer.push({event:"Scroll To Section",eventCategory:n(),eventAction:a,eventLabel:b,eventValue:1,eventNonInteraction:h}):("undefined"!==typeof ga&&ga("send","event",n(),a,b,1,{nonInteraction:1}),"undefined"!==typeof _gaq&&_gaq.push(["_trackEvent",n(),a,b,1,!0]))}
function k(b,g,c,h){if(-1===a[c].cache.indexOf(c+"-"+h)&&document.querySelectorAll(b).length){var f="viewed"===h?document.querySelectorAll(b)[0].getBoundingClientRect().height:0,x=document.querySelectorAll(b)[0].getBoundingClientRect().top+d.pageYOffset;g>=x+f&&(v(p(c)+" "+p(h),b,c,!1),a[c].cache.push(c+"-"+h),q++)}}function r(a){a=document.querySelectorAll(a);return a.length?a[0].offsetHeight||0:0}function m(b,g,c){var f=d.pageYOffset+d.innerHeight;if(q>=w)"handleClick"===g&&d.removeEventListener(b,
t),"throttle"===g&&d.removeEventListener(b,u);else if(c)setTimeout(function(){0<r(a[c].content)&&(a[c].reached&&k(a[c].content,f,c,"reached"),a[c].viewed&&k(a[c].content,f,c,"viewed"))},10);else for(var e in a)a.hasOwnProperty(e)&&0<r(a[e].content)&&(a[e].reached&&k(a[e].content,f,e,"reached"),a[e].viewed&&k(a[e].content,f,e,"viewed"))}function y(b){b=document.querySelectorAll(a[b].content).length;return 0<b?!0:!1}function p(a){return a.charAt(0).toUpperCase()+a.slice(1)}function u(a,b){var c,f,e,
g=null,d=0,k=function(){d=new Date;g=null;e=a.apply(c,f)};return function(){var h=new Date;d||(d=h);var l=b-(h-d);c=this;f=arguments;0>=l?(clearTimeout(g),g=null,d=h,e=a.apply(c,f)):g||(g=setTimeout(k,l));return e}}function t(a,b){return function(c){a(c,b)}}var w=0,q=0,a={recommendations:{content:".gtm-recommendations iframe",clickable:".gtm-recommendations .gtm-recommendations__title",exists:!1,reached:!1,viewed:!0,size:0,cache:[]},"abstract":{content:"[id^\x3dAbs]",clickable:null,exists:!0,reached:!1,
viewed:!0,size:0,cache:[]},references:{content:".Bibliography \x3e .content, #Bib1 \x3e .content",clickable:".Bibliography \x3e .Heading, #Bib1 \x3e .Heading",exists:!0,reached:!0,viewed:!0,size:0,cache:[]},about:{content:"#AboutThisContent \x3e .content",clickable:"#AboutThisContent \x3e #aboutcontent",exists:!1,reached:!1,viewed:!0,size:0,cache:[]}};d.addEventListener("scroll",u(function(){m("scroll","throttle")},500));d.addEventListener("orientationchange",u(function(){m("orientationchange","throttle")},
500));for(var b in a)if(a.hasOwnProperty(b)&&(a[b].exists&&a[b].size++,a[b].reached&&a[b].size++,a[b].viewed&&a[b].size++,w+=a[b].size,a[b].exists&&y(b)||!a[b].exists)){a[b].exists&&-1===a[b].cache.indexOf(b+"-exists")&&(v(p(b)+" Exists",a[b].content,b,!0),a[b].cache.push(b+"-exists"),q++);if(0<r(a[b].content)){var l=d.pageYOffset+d.innerHeight;a[b].reached&&k(a[b].content,l,b,"reached");a[b].viewed&&k(a[b].content,l,b,"viewed")}a[b].clickable&&(l=document.querySelectorAll(a[b].clickable)[0],l.addEventListener("click",
t(function(a,b){m("click","handleClick",b)},b)),l.addEventListener("click",t(function(a,b){13==a.keyCode&&m("click","handleClick",b)},b)))}})(window);</script><div style="display: none; visibility: hidden;"><script type="text/javascript">window.CustomWebTrekk||document.write("\x3cscript src\x3d'/springerlink-static/1756782716/js/gtm-webtrekk.js' type\x3d'text/javascript'\x3e\x3c/script\x3e");</script><script src="gtm-webtrekk.js" type="text/javascript"></script>

<script type="text/javascript">"function"==typeof CustomWebTrekk&&(new CustomWebTrekk).sendInfo("10.1007\/s11263-015-0816-y","Article","11263 | International Journal of Computer Vision","article html download","rd_springer_com.journal.article_full_text","N","N","Y");</script>
</div></body>
</html>
